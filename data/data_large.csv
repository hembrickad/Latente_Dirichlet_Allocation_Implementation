Closed-form Marginal Likelihood in Gamma-Poisson Matrix Factorization,"  We present novel understandings of the Gamma-Poisson (GaP) model, a probabilistic matrix factorization model for count data. We show that GaP can be rewritten free of the score/activation matrix. This gives us new insights about the estimation of the topic/dictionary matrix by maximum marginal likelihood estimation. In particular, this explains the robustness of this estimator to over-specified values of the factorization rank, especially its ability to automatically prune irrelevant dictionary columns, as empirically observed in previous work. The marginalization of the activation matrix leads in turn to a new Monte Carlo Expectation-Maximization algorithm with favorable properties. "
Laboratory mid-IR spectra of equilibrated and igneous meteorites. Searching for observables of planetesimal debris,"  Meteorites contain minerals from Solar System asteroids with different properties (like size, presence of water, core formation). We provide new mid-IR transmission spectra of powdered meteorites to obtain templates of how mid-IR spectra of asteroidal debris would look like. This is essential for interpreting mid-IR spectra of past and future space observatories, like the James Webb Space Telescope. We show that the transmission spectra of wet and dry chondrites, carbonaceous and ordinary chondrites and achondrite and chondrite meteorites are distinctly different in a way one can distinguish in astronomical mid-IR spectra. The two observables that spectroscopically separate the different meteorites groups (and thus the different types of parent bodies) are the pyroxene-olivine feature strength ratio and the peak shift of the olivine spectral features due to an increase in the iron concentration of the olivine. "
Case For Static AMSDU Aggregation in WLANs,"  Frame aggregation is a mechanism by which multiple frames are combined into a single transmission unit over the air. Frames aggregated at the AMSDU level use a common CRC check to enforce integrity. For longer aggregated AMSDU frames, the packet error rate increases significantly for the same bit error rate. Hence, multiple studies have proposed doing AMSDU aggregation adaptively based on the error rate. This study evaluates if there is a \emph{practical} advantage in doing adaptive AMSDU aggregation based on the link bit error rate. Evaluations on a model show that instead of implementing a complex adaptive AMSDU frame aggregation mechanism which impact queuing and other implementation aspects, it is easier to influence packet error rate with traditional mechanisms while keeping the AMSDU aggregation logic simple. "
The $Gaia$-ESO Survey: the inner disk intermediate-age open cluster NGC 6802,"Milky Way open clusters are very diverse in terms of age, chemical composition, and kinematic properties. Intermediate-age and old open clusters are less common, and it is even harder to find them inside the solar Galactocentric radius, due to the high mortality rate and strong extinction inside this region. NGC 6802 is one of the inner disk open clusters (IOCs) observed by the $Gaia$-ESO survey (GES). This cluster is an important target for calibrating the abundances derived in the survey due to the kinematic and chemical homogeneity of the members in open clusters. Using the measurements from $Gaia$-ESO internal data release 4 (iDR4), we identify 95 main-sequence dwarfs as cluster members from the GIRAFFE target list, and eight giants as cluster members from the UVES target list. The dwarf cluster members have a median radial velocity of $13.6\pm1.9$ km s$^{-1}$, while the giant cluster members have a median radial velocity of $12.0\pm0.9$ km s$^{-1}$ and a median [Fe/H] of $0.10\pm0.02$ dex. The color-magnitude diagram of these cluster members suggests an age of $0.9\pm0.1$ Gyr, with $(m-M)_0=11.4$ and $E(B-V)=0.86$. We perform the first detailed chemical abundance analysis of NGC 6802, including 27 elemental species. To gain a more general picture about IOCs, the measurements of NGC 6802 are compared with those of other IOCs previously studied by GES, that is, NGC 4815, Trumpler 20, NGC 6705, and Berkeley 81. NGC 6802 shows similar C, N, Na, and Al abundances as other IOCs. These elements are compared with nucleosynthetic models as a function of cluster turn-off mass. The $\alpha$, iron-peak, and neutron-capture elements are also explored in a self-consistent way."
Witness-Functions versus Interpretation-Functions for Secrecy in Cryptographic Protocols: What to Choose?,"  Proving that a cryptographic protocol is correct for secrecy is a hard task. One of the strongest strategies to reach this goal is to show that it is increasing, which means that the security level of every single atomic message exchanged in the protocol, safely evaluated, never deceases. Recently, two families of functions have been proposed to measure the security level of atomic messages. The first one is the family of interpretation-functions. The second is the family of witness-functions. In this paper, we show that the witness-functions are more efficient than interpretation-functions. We give a detailed analysis of an ad-hoc protocol on which the witness-functions succeed in proving its correctness for secrecy while the interpretation-functions fail to do so. "
Pairwise Difference Estimation of High Dimensional Partially Linear Model,"This paper proposes a regularized pairwise difference approach for estimating the linear component coefficient in a partially linear model, with consistency and exact rates of convergence obtained in high dimensions under mild scaling requirements. Our analysis reveals interesting features such as (i) the bandwidth parameter automatically adapts to the model and is actually tuning-insensitive; and (ii) the procedure could even maintain fast rate of convergence for $\alpha$-Hölder class of $\alpha\leq1/2$. Simulation studies show the advantage of the proposed method, and application of our approach to a brain imaging data reveals some biological patterns which fail to be recovered using competing methods."
Dissecting the multivariate extremal index and tail dependence,"  A central issue in the theory of extreme values focuses on suitable conditions such that the well-known results for the limiting distributions of the maximum of i.i.d. sequences can be applied to stationary ones. In this context, the extremal index appears as a key parameter to capture the effect of temporal dependence on the limiting distribution of the maxima. The multivariate extremal index corresponds to a generalization of this concept to a multivariate context and affects the tail dependence structure within the marginal sequences and between them. As it is a function, the inference becomes more difficult, and it is therefore important to obtain characterizations, namely bounds based on the marginal dependence that are easier to estimate. In this work we present two decompositions that emphasize different types of information contained in the multivariate extremal index, an upper limit better than those found in the literature and we analyze its role in dependence on the limiting model of the componentwise maxima of a stationary sequence. We will illustrate the results with examples of recognized interest in applications. "
"Big Universe, Big Data: Machine Learning and Image Analysis for Astronomy","  Astrophysics and cosmology are rich with data. The advent of wide-area digital cameras on large aperture telescopes has led to ever more ambitious surveys of the sky. Data volumes of entire surveys a decade ago can now be acquired in a single night and real-time analysis is often desired. Thus, modern astronomy requires big data know-how, in particular it demands highly efficient machine learning and image analysis algorithms. But scalability is not the only challenge: Astronomy applications touch several current machine learning research questions, such as learning from biased data and dealing with label and measurement noise. We argue that this makes astronomy a great domain for computer science research, as it pushes the boundaries of data analysis. In the following, we will present this exciting application area for data scientists. We will focus on exemplary results, discuss main challenges, and highlight some recent methodological advancements in machine learning and image analysis triggered by astronomical applications. "
Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog,"  A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, all learned without any human supervision! In this paper, using a Task and Tell reference game between two agents as a testbed, we present a sequence of 'negative' results culminating in a 'positive' one -- showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge 'naturally', despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate. "
Properties and Origin of Galaxy Velocity Bias in the Illustris Simulation,"We use the hydrodynamical galaxy formation simulations from the Illustris suite to study the origin and properties of galaxy velocity bias, i.e., the difference between the velocity distributions of galaxies and dark matter inside halos. We find that galaxy velocity bias is a decreasing function of the ratio of galaxy stellar mass to host halo mass. In general, central galaxies are not at rest with respect to dark matter halos or the core of halos, with a velocity dispersion above 0.04 times that of the dark matter. The central galaxy velocity bias is found to be mostly caused by the close interactions between the central and satellite galaxies. For satellite galaxies, the velocity bias is related to their dynamical and tidal evolution history after being accreted onto the host halos. It depends on the time after the accretion and their distances from the halo centers, with massive satellites generally moving more slowly than the dark matter. The results are in broad agreements with those inferred from modeling small-scale redshift-space galaxy clustering data, and the study can help improve models of redshift-space galaxy clustering."
Computer Modeling of Halogen Bonds and Other $σ$-Hole Interactions,"In the field of noncovalent interactions a new paradigm has recently become popular. It stems from the analysis of molecular electrostatic potentials and introduces a label, which has recently attracted enormous attention. The label is {\sigma}-hole, and it was first used in connection with halogens. It initiated a renaissance of interest in halogenated compounds, and later on, when found also on other groups of atoms (chalcogens, pnicogens, tetrels and aerogens), it resulted in a new direction of research of intermolecular interactions. In this review, we summarize advances from about the last 10 years in understanding those interactions related to {\sigma}-hole. We pay particular attention to theoretical and computational techniques, which play a crucial role in the field."
Towards Universal End-to-End Affect Recognition from Multilingual Speech by ConvNets,"We propose an end-to-end affect recognition approach using a Convolutional Neural Network (CNN) that handles multiple languages, with applications to emotion and personality recognition from speech. We lay the foundation of a universal model that is trained on multiple languages at once. As affect is shared across all languages, we are able to leverage shared information between languages and improve the overall performance for each one. We obtained an average improvement of 12.8% on emotion and 10.1% on personality when compared with the same model trained on each language only. It is end-to-end because we directly take narrow-band raw waveforms as input. This allows us to accept as input audio recorded from any source and to avoid the overhead and information loss of feature extraction. It outperforms a similar CNN using spectrograms as input by 12.8% for emotion and 6.3% for personality, based on F-scores. Analysis of the network parameters and layers activation shows that the network learns and extracts significant features in the first layer, in particular pitch, energy and contour variations. Subsequent convolutional layers instead capture language-specific representations through the analysis of supra-segmental features. Our model represents an important step for the development of a fully universal affect recognizer, able to recognize additional descriptors, such as stress, and for the future implementation into affective interactive systems."
A Variational Characterization of Fluid Sloshing with Surface Tension,"  We consider the sloshing problem for an incompressible, inviscid, irrotational fluid in an open container, including effects due to surface tension on the free surface. We restrict ourselves to a constant contact angle and seek time-harmonic solutions of the linearized problem, which describes the time-evolution of the fluid due to a small initial disturbance of the surface at rest. As opposed to the zero surface tension case, where the problem reduces to a partial differential equation for the velocity potential, we obtain a coupled system for the velocity potential and the free surface displacement. We derive a new variational formulation of the coupled problem and establish the existence of solutions using the direct method from the calculus of variations. We prove a domain monotonicity result for the fundamental sloshing eigenvalue. In the limit of zero surface tension, we recover the variational formulation of the mixed Steklov-Neumann eigenvalue problem and give the first-order perturbation formula for a simple eigenvalue. "
diagnoseIT: Expertengestützte automatische Diagnose von Performance-Probleme in Enterprise-Anwendungen (Abschlussbericht),  This is the final report of the collaborative research project diagnoseIT on expert-guided automatic diagnosis of performance problems in enterprise applications. 
Statistical Inference in Political Networks Research,"  Researchers interested in statistically modeling network data have a well-established and quickly growing set of approaches from which to choose. Several of these methods have been regularly applied in research on political networks, while others have yet to permeate the field. Here, we review the most prominent methods of inferential network analysis---both for cross-sectionally and longitudinally observed networks including (temporal) exponential random graph models, latent space models, the quadratic assignment procedure, and stochastic actor oriented models. For each method, we summarize its analytic form, identify prominent published applications in political science and discuss computational considerations. We conclude with a set of guidelines for selecting a method for a given application. "
Controller-jammer game models of Denial of Service in control systems operating over packet-dropping links,"  The paper introduces a class of zero-sum games between the adversary and controller as a scenario for a `denial of service' in a networked control system. The communication link is modeled as a set of transmission regimes controlled by a strategic jammer whose intention is to wage an attack on the plant by choosing a most damaging regime-switching strategy. We demonstrate that even in the one-step case, the introduced games admit a saddle-point equilibrium, at which the jammer's optimal policy is to randomize in a region of the plant's state space, thus requiring the controller to undertake a nontrivial response which is different from what one would expect in a standard stochastic control problem over a packet dropping link. The paper derives conditions for the introduced games to have such a saddle-point equilibrium. Furthermore, we show that in more general multi-stage games, these conditions provide `greedy' jamming strategies for the adversary. "
Conservation of spin supercurrents in superconductors,"  We demonstrate that spin supercurrents are conserved upon transmission through a conventional superconductor, even in the presence of spin-dependent scattering by impurities with magnetic moments or spin-orbit coupling. This is fundamentally different from conventional spin currents, which decay in the presence of such scattering, and this has important implications for the usage of superconducting materials in spintronic hybrid structures. "
Driver Identification Using Automobile Sensor Data from a Single Turn,"As automotive electronics continue to advance, cars are becoming more and more reliant on sensors to perform everyday driving operations. These sensors are omnipresent and help the car navigate, reduce accidents, and provide comfortable rides. However, they can also be used to learn about the drivers themselves. In this paper, we propose a method to predict, from sensor data collected at a single turn, the identity of a driver out of a given set of individuals. We cast the problem in terms of time series classification, where our dataset contains sensor readings at one turn, repeated several times by multiple drivers. We build a classifier to find unique patterns in each individual's driving style, which are visible in the data even on such a short road segment. To test our approach, we analyze a new dataset collected by AUDI AG and Audi Electronics Venture, where a fleet of test vehicles was equipped with automotive data loggers storing all sensor readings on real roads. We show that turns are particularly well-suited for detecting variations across drivers, especially when compared to straightaways. We then focus on the 12 most frequently made turns in the dataset, which include rural, urban, highway on-ramps, and more, obtaining accurate identification results and learning useful insights about driver behavior in a variety of settings."
Same-different problems strain convolutional neural networks,"  The robust and efficient recognition of visual relations in images is a hallmark of biological vision. We argue that, despite recent progress in visual recognition, modern machine vision algorithms are severely limited in their ability to learn visual relations. Through controlled experiments, we demonstrate that visual-relation problems strain convolutional neural networks (CNNs). The networks eventually break altogether when rote memorization becomes impossible, as when intra-class variability exceeds network capacity. Motivated by the comparable success of biological vision, we argue that feedback mechanisms including attention and perceptual grouping may be the key computational components underlying abstract visual reasoning.\ "
Hyper-dimensional computing for a visual question-answering system that is trainable end-to-end,"  In this work we propose a system for visual question answering. Our architecture is composed of two parts, the first part creates the logical knowledge base given the image. The second part evaluates questions against the knowledge base. Differently from previous work, the knowledge base is represented using hyper-dimensional computing. This choice has the advantage that all the operations in the system, namely creating the knowledge base and evaluating the questions against it, are differentiable, thereby making the system easily trainable in an end-to-end fashion. "
The Inner Structure of Time-Dependent Signals,"This paper shows how a time series of measurements of an evolving system can be processed to create an inner time series that is unaffected by any instantaneous invertible, possibly nonlinear transformation of the measurements. An inner time series contains information that does not depend on the nature of the sensors, which the observer chose to monitor the system. Instead, it encodes information that is intrinsic to the evolution of the observed system. Because of its sensor-independence, an inner time series may produce fewer false negatives when it is used to detect events in the presence of sensor drift. Furthermore, if the observed physical system is comprised of non-interacting subsystems, its inner time series is separable; i.e., it consists of a collection of time series, each one being the inner time series of an isolated subsystem. Because of this property, an inner time series can be used to detect a specific behavior of one of the independent subsystems without using blind source separation to disentangle that subsystem from the others. The method is illustrated by applying it to: 1) an analytic example; 2) the audio waveform of one speaker; 3) video images from a moving camera; 4) mixtures of audio waveforms of two speakers."
Classical properties of the leading eigenstates of quantum dissipative systems,"  By analyzing a paradigmatic example of the theory of dissipative systems -- the classical and quantum dissipative standard map -- we are able to explain the main features of the decay to the quantum equilibrium state. The classical isoperiodic stable structures typically present in the parameter space of these kind of systems play a fundamental role. In fact, we have found that the period of stable structures that are near in this space determines the phase of the leading eigenstates of the corresponding quantum superoperator. Moreover, the eigenvectors show a strong localization on the corresponding periodic orbits (limit cycles). We show that this sort of scarring phenomenon (an established property of Hamiltonian and projectively open systems) is present in the dissipative case and it is of extreme simplicity. "
Towards Binary-Valued Gates for Robust LSTM Training,"Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. It aims to use gates to control information flow (e.g., whether to skip some information or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal. In this paper, we propose a new way for LSTM training, which pushes the output values of the gates towards 0 or 1. By doing so, we can better control the information flow: the gates are mostly open or closed, instead of in a middle state, which makes the results more interpretable. Empirical studies show that (1) Although it seems that we restrict the model capacity, there is no performance drop: we achieve better or comparable performances due to its better generalization ability; (2) The outputs of gates are not sensitive to their inputs: we can easily compress the LSTM unit in multiple ways, e.g., low-rank approximation and low-precision approximation. The compressed models are even better than the baseline models without compression."
GUB Covers and Power-Indexed formulations for Wireless Network Design,"We propose a pure 0-1 formulation for the wireless network design problem, i.e. the problem of configuring a set of transmitters to provide service coverage to a set of receivers. In contrast with classical mixed integer formulations, where power emissions are represented by continuous variables, we consider only a finite set of powers values. This has two major advantages: it better fits the usual practice and eliminates the sources of numerical problems which heavily affect continuous models. A crucial ingredient of our approach is an effective basic formulation for the single knapsack problem representing the coverage condition of a receiver. This formulation is based on the GUB cover inequalities introduced by Wolsey (1990) and its core is an extension of the exact formulation of the GUB knapsack polytope with two GUB constraints. This special case corresponds to the very common practical situation where only one major interferer is present. We assess the effectiveness of our formulation by comprehensive computational results over realistic instances of two typical technologies, namely WiMAX and DVB-T."
Orbital contributions to the electron g-factor in semiconductor nanowires,"Recent experiments on Majorana fermions in semiconductor nanowires [Albrecht et al., Nat. 531, 206 (2016)] revealed a surprisingly large electronic Landé g-factor, several times larger than the bulk value - contrary to the expectation that confinement reduces the g-factor. Here we assess the role of orbital contributions to the electron g-factor in nanowires and quantum dots. We show that an LS coupling in higher subbands leads to an enhancement of the g-factor of an order of magnitude or more for small effective mass semiconductors. We validate our theoretical finding with simulations of InAs and InSb, showing that the effect persists even if cylindrical symmetry is broken. A huge anisotropy of the enhanced g-factors under magnetic field rotation allows for a straightforward experimental test of this theory."
Comprehensive routing strategy on multilayer networks,"  Designing an efficient routing strategy is of great importance to alleviate traffic congestion in multilayer networks. In this work, we design an effective routing strategy for multilayer networks by comprehensively considering the roles of nodes' local structures in micro-level, as well as the macro-level differences in transmission speeds between different layers. Both numerical and analytical results indicate that our proposed routing strategy can reasonably redistribute the traffic load of the low speed layer to the high speed layer, and thus the traffic capacity of multilayer networks are significantly enhanced compared with the monolayer low speed networks. There is an optimal combination of macro- and micro-level control parameters at which can remarkably alleviate the congestion and thus maximize the traffic capacity for a given multilayer network. Moreover, we find that increasing the size and the average degree of the high speed layer can enhance the traffic capacity of multilayer networks more effectively. We finally verify that real-world network topology does not invalidate the results. The theoretical predictions agree well with the numerical simulations. "
Long short-term memory and learning-to-learn in networks of spiking neurons,"Recurrent networks of spiking neurons (RSNNs) underlie the astounding computing and learning capabilities of the brain. But computing and learning capabilities of RSNN models have remained poor, at least in comparison with artificial neural networks (ANNs). We address two possible reasons for that. One is that RSNNs in the brain are not randomly connected or designed according to simple rules, and they do not start learning as a tabula rasa network. Rather, RSNNs in the brain were optimized for their tasks through evolution, development, and prior experience. Details of these optimization processes are largely unknown. But their functional contribution can be approximated through powerful optimization methods, such as backpropagation through time (BPTT). A second major mismatch between RSNNs in the brain and models is that the latter only show a small fraction of the dynamics of neurons and synapses in the brain. We include neurons in our RSNN model that reproduce one prominent dynamical process of biological neurons that takes place at the behaviourally relevant time scale of seconds: neuronal adaptation. We denote these networks as LSNNs because of their Long short-term memory. The inclusion of adapting neurons drastically increases the computing and learning capability of RSNNs if they are trained and configured by deep learning (BPTT combined with a rewiring algorithm that optimizes the network architecture). In fact, the computational performance of these RSNNs approaches for the first time that of LSTM networks. In addition RSNNs with adapting neurons can acquire abstract knowledge from prior learning in a Learning-to-Learn (L2L) scheme, and transfer that knowledge in order to learn new but related tasks from very few examples. We demonstrate this for supervised learning and reinforcement learning."
Detecting Molecular Rotational Dynamics Complementing the Low-Frequency Terahertz Vibrations in a Zirconium-Based Metal-Organic Framework,"We show clear experimental evidence of co-operative terahertz (THz) dynamics observed below 3 THz (~100 cm-1), for a low-symmetry Zr-based metal-organic framework (MOF) structure, termed MIL-140A [ZrO(O2C-C6H4-CO2)]. Utilizing a combination of high-resolution inelastic neutron scattering and synchrotron radiation far-infrared spectroscopy, we measured low-energy vibrations originating from the hindered rotations of organic linkers, whose energy barriers and detailed dynamics have been elucidated via ab initio density functional theory (DFT) calculations. For completeness, we obtained Raman spectra and characterized the alterations to the complex pore architecture caused by the THz rotations. We discovered an array of soft modes with trampoline-like motions, which could potentially be the source of anomalous mechanical phenomena, such as negative linear compressibility and negative thermal expansion. Our results also demonstrate coordinated shear dynamics (~2.5 THz), a mechanism which we have shown to destabilize MOF crystals, in the exact crystallographic direction of the minimum shear modulus (Gmin)."
A southern-sky total intensity source catalogue at 2.3 GHz from S-band Polarisation All-Sky Survey data,"The S-band Polarisation All-Sky Survey (S-PASS) has observed the entire southern sky using the 64-metre Parkes radio telescope at 2.3GHz with an effective bandwidth of 184MHz. The surveyed sky area covers all declinations $\delta\leq 0^\circ$. To analyse compact sources the survey data have been re-processed to produce a set of 107 Stokes $I$ maps with 10.75arcmin resolution and the large scale emission contribution filtered out. In this paper we use these Stokes $I$ images to create a total intensity southern-sky extragalactic source catalogue at 2.3GHz. The source catalogue contains 23,389 sources and covers a sky area of 16,600deg$^2$, excluding the Galactic plane for latitudes $|b|<10^\circ$. Approximately 8% of catalogued sources are resolved. S-PASS source positions are typically accurate to within 35arcsec. At a flux density of 225mJy the S-PASS source catalogue is more than 95% complete, and $\sim$94% of S-PASS sources brighter than 500mJy beam$^{-1}$ have a counterpart at lower frequencies."
Mechanomyography based closed-loop Functional Electrical Stimulation cycling system,"Functional Electrical Stimulation (FES) systems are successful in restoring motor function and supporting paralyzed users. Commercially available FES products are open loop, meaning that the system is unable to adapt to changing conditions with the user and their muscles which results in muscle fatigue and poor stimulation protocols. This is because it is difficult to close the loop between stimulation and monitoring of muscle contraction using adaptive stimulation. FES causes electrical artefacts which make it challenging to monitor muscle contractions with traditional methods such as electromyography (EMG). We look to overcome this limitation by combining FES with novel mechanomyographic (MMG) sensors to be able to monitor muscle activity during stimulation in real time. To provide a meaningful task we built an FES cycling rig with a software interface that enabled us to perform adaptive recording and stimulation, and then combine this with sensors to record forces applied to the pedals using force sensitive resistors (FSRs), crank angle position using a magnetic incremental encoder and inputs from the user using switches and a potentiometer. We illustrated this with a closed-loop stimulation algorithm that used the inputs from the sensors to control the output of a programmable RehaStim 1 FES stimulator (Hasomed) in real-time. This recumbent bicycle rig was used as a testing platform for FES cycling. The algorithm was designed to respond to a change in requested speed (RPM) from the user and change the stimulation power (% of maximum current mA) until this speed was achieved and then maintain it."
QT2S: A System for Monitoring Road Traffic via Fine Grounding of Tweets,"Social media platforms provide continuous access to user generated content that enables real-time monitoring of user behavior and of events. The geographical dimension of such user behavior and events has recently caught a lot of attention in several domains: mobility, humanitarian, or infrastructural. While resolving the location of a user can be straightforward, depending on the affordances of their device and/or of the application they are using, in most cases, locating a user demands a larger effort, such as exploiting textual features. On Twitter for instance, only 2% of all tweets are geo-referenced. In this paper, we present a system for zoomed-in grounding (below city level) for short messages (e.g., tweets). The system combines different natural language processing and machine learning techniques to increase the number of geo-grounded tweets, which is essential to many applications such as disaster response and real-time traffic monitoring."
A Benchmark for Dose Finding Studies with Continuous Outcomes,"An important tool to evaluate the performance of any design is an optimal benchmark proposed by O'Quigley and others (2002, Biostatistics 3(1), 51-56) that provides an upper bound on the performance of a design under a given scenario. The original benchmark can be applied to dose finding studies with a binary endpoint only. However, there is a growing interest in dose finding studies involving continuous outcomes, but no benchmark for such studies has been developed. We show that the original benchmark and its extension by Cheung (2014, Biometrics 70(2), 389-397), when looked at from a different perspective, can be generalised to various settings with several discrete and continuous outcomes. We illustrate and compare the benchmark performance in the setting of a Phase I clinical trial with continuous toxicity endpoint and in the setting of a Phase I/II clinical trial with continuous efficacy outcome. We show that the proposed benchmark provides an accurate upper bound for model-based dose finding methods and serves as a powerful tool for evaluating designs."
Wavefront retrieval through random pupil plane phase probes: Gerchberg-Saxton approach,"A pupil plane wavefront reconstruction procedure is proposed based on analysis of a sequence of focal plane images corresponding to a sequence of random pupil plane phase probes. The developed method provides the unique nontrivial solution of wavefront retrieval problem and shows global convergence to this solution demonstrated using a Gerchberg-Saxton implementation. The method is general and can be used in any optical system that includes deformable mirrors for active/adaptive wavefront correction. The presented numerical simulation and lab experimental results show low noise sensitivity, high reliability and robustness of the proposed approach for high quality optical wavefront restoration. Laboratory experiments have shown $\lambda$/14 rms accuracy in retrieval of a poked DM actuator fiducial pattern with spatial resolution of 20-30$~\mu$m that is comparable with accuracy of direct high-resolution interferometric measurements."
Uplink Non-Orthogonal Multiple Access for 5G Wireless Networks,"  Orthogonal Frequency Division Multiple Access (OFDMA) as well as other orthogonal multiple access techniques fail to achieve the system capacity limit in the uplink due to the exclusivity in resource allocation. This issue is more prominent when fairness among the users is considered in the system. Current Non-Orthogonal Multiple Access (NOMA) techniques introduce redundancy by coding/spreading to facilitate the users' signals separation at the receiver, which degrade the system spectral efficiency. Hence, in order to achieve higher capacity, more efficient NOMA schemes need to be developed. In this paper, we propose a NOMA scheme for uplink that removes the resource allocation exclusivity and allows more than one user to share the same subcarrier without any coding/spreading redundancy. Joint processing is implemented at the receiver to detect the users' signals. However, to control the receiver complexity, an upper limit on the number of users per subcarrier needs to be imposed. In addition, a novel subcarrier and power allocation algorithm is proposed for the new NOMA scheme that maximizes the users' sum-rate. The link-level performance evaluation has shown that the proposed scheme achieves bit error rate close to the single-user case. Numerical results show that the proposed NOMA scheme can significantly improve the system performance in terms of spectral efficiency and fairness comparing to OFDMA. "
Epsilon-approximations and epsilon-nets,  The use of random samples to approximate properties of geometric configurations has been an influential idea for both combinatorial and algorithmic purposes. This chapter considers two related notions---$\epsilon$-approximations and $\epsilon$-nets---that capture the most important quantitative properties that one would expect from a random sample with respect to an underlying geometric configuration. 
A continuous framework for fairness,"  Increasingly, discrimination by algorithms is perceived as a societal and legal problem. As a response, a number of criteria for implementing algorithmic fairness in machine learning have been developed in the literature. This paper proposes the Continuous Fairness Algorithm (CFA$\theta$) which enables a continuous interpolation between different fairness definitions. More specifically, we make three main contributions to the existing literature. First, our approach allows the decision maker to continuously vary between concepts of individual and group fairness. As a consequence, the algorithm enables the decision maker to adopt intermediate ""worldviews"" on the degree of discrimination encoded in algorithmic processes, adding nuance to the extreme cases of ""we're all equal"" (WAE) and ""what you see is what you get"" (WYSIWYG) proposed so far in the literature. Second, we use optimal transport theory, and specifically the concept of the barycenter, to maximize decision maker utility under the chosen fairness constraints. Third, the algorithm is able to handle cases of intersectionality, i.e., of multi-dimensional discrimination of certain groups on grounds of several criteria. We discuss three main examples (college admissions; credit application; insurance contracts) and map out the policy implications of our approach. The explicit formalization of the trade-off between individual and group fairness allows this post-processing approach to be tailored to different situational contexts in which one or the other fairness criterion may take precedence. "
Counting Subwords Occurrences in Base-b Expansions,"We count the number of distinct (scattered) subwords occurring in the base-b expansion of the non-negative integers. More precisely, we consider the sequence $(S_b(n))_{n\ge 0}$ counting the number of positive entries on each row of a generalization of the Pascal triangle to binomial coefficients of base-$b$ expansions. By using a convenient tree structure, we provide recurrence relations for $(S_b(n))_{n\ge 0}$ leading to the $b$-regularity of the latter sequence. Then we deduce the asymptotics of the summatory function of the sequence $(S_b(n))_{n\ge 0}$."
Tensor Renormalization Group with Randomized Singular Value Decomposition,"  An algorithm of the tensor renormalization group is proposed based on a randomized algorithm for singular value decomposition. Our algorithm is applicable to a broad range of two-dimensional classical models. In the case of a square lattice, its computational complexity and memory usage are proportional to the fifth and the third power of the bond dimension, respectively, whereas those of the conventional implementation are of the sixth and the fourth power. The oversampling parameter larger than the bond dimension is sufficient to reproduce the same result as full singular value decomposition even at the critical point of the two-dimensional Ising model. "
Cohomology of symplectic groups and Meyer's signature theorem,"Meyer showed that the signature of a closed oriented surface bundle over a surface is a multiple of $4$, and can be computed using an element of $H^2(\mathsf{Sp}(2g, \mathbb{Z}),\mathbb{Z})$. Denoting by $1 \to \mathbb{Z} \to \widetilde{\mathsf{Sp}(2g,\mathbb{Z})} \to \mathsf{Sp}(2g,\mathbb{Z}) \to 1$ the pullback of the universal cover of $\mathsf{ Sp}(2g,\mathbb{R})$, Deligne proved that every finite index subgroup of $\widetilde{\mathsf {Sp}(2g, \mathbb{Z})}$ contains $2\mathbb{Z}$. As a consequence, a class in the second cohomology of any finite quotient of $\mathsf{Sp}(2g, \mathbb{Z})$ can at most enable us to compute the signature of a surface bundle modulo $8$. We show that this is in fact possible and investigate the smallest quotient of $\mathsf{Sp}(2g, \mathbb{Z})$ that contains this information. This quotient $\mathfrak{H}$ is a non-split extension of $\mathsf {Sp}(2g,2)$ by an elementary abelian group of order $2^{2g+1}$. There is a central extension $1\to \mathbb{Z}/2\to\tilde{\mathfrak{H}}\to\mathfrak{H}\to 1$, and $\tilde{\mathfrak{H}}$ appears as a quotient of the metaplectic double cover $\mathsf{Mp}(2g,\mathbb{Z})=\widetilde{\mathsf{Sp}(2g,\mathbb{Z})}/2\mathbb{Z}$. It is an extension of $\mathsf{Sp}(2g,2)$ by an almost extraspecial group of order $2^{2g+2}$, and has a faithful irreducible complex representation of dimension $2^g$. Provided $g\ge 4$, $\widetilde{\mathfrak{H}}$ is the universal central extension of $\mathfrak{H}$. Putting all this together, we provide a recipe for computing the signature modulo $8$, and indicate some consequences."
The Newman--Shapiro problem,"We give a negative answer to the Newman--Shapiro problem on weighted approximation for entire functions formulated in 1966 and motivated by the theory of operators on the Fock space. There exists a function in the Fock space such that its exponential multiples do not approximate some entire multiples in the space. Furthermore, we establish several positive results under different restrictions on the function in question."
randUTV: A blocked randomized algorithm for computing a rank-revealing UTV factorization,"  This manuscript describes the randomized algorithm randUTV for computing a so called UTV factorization efficiently. Given a matrix $A$, the algorithm computes a factorization $A = UTV^{*}$, where $U$ and $V$ have orthonormal columns, and $T$ is triangular (either upper or lower, whichever is preferred). The algorithm randUTV is developed primarily to be a fast and easily parallelized alternative to algorithms for computing the Singular Value Decomposition (SVD). randUTV provides accuracy very close to that of the SVD for problems such as low-rank approximation, solving ill-conditioned linear systems, determining bases for various subspaces associated with the matrix, etc. Moreover, randUTV produces highly accurate approximations to the singular values of $A$. Unlike the SVD, the randomized algorithm proposed builds a UTV factorization in an incremental, single-stage, and non-iterative way, making it possible to halt the factorization process once a specified tolerance has been met. Numerical experiments comparing the accuracy and speed of randUTV to the SVD are presented. These experiments demonstrate that in comparison to column pivoted QR, which is another factorization that is often used as a relatively economic alternative to the SVD, randUTV compares favorably in terms of speed while providing far higher accuracy. "
"Continuity of nonlinear eigenvalues in $CD(K,\infty)$ spaces with respect to measured Gromov-Hausdorff convergence","  In this note we prove in the nonlinear setting of $CD(K,\infty)$ spaces the stability of the Krasnoselskii spectrum of the Laplace operator $-\Delta$ under measured Gromov-Hausdorff convergence, under an additional compactness assumption satisfied, for instance, by sequences of $CD^*(K,N)$ metric measure spaces with uniformly bounded diameter. Additionally, we show that every element $\lambda$ in the Krasnoselskii spectrum is indeed an eigenvalue, namely there exists a nontrivial $u$ satisfying the eigenvalue equation $- \Delta u = \lambda u$. "
Modeling WiFi Traffic for White Space Prediction in Wireless Sensor Networks,"Cross Technology Interference (CTI) is a prevalent phenomenon in the 2.4 GHz unlicensed spectrum causing packet losses and increased channel contention. In particular, WiFi interference is a severe problem for low-power wireless networks as its presence causes a significant degradation of the overall performance. In this paper, we propose a proactive approach based on WiFi interference modeling for accurately predicting transmission opportunities for low-power wireless networks. We leverage statistical analysis of real-world WiFi traces to learn aggregated traffic characteristics in terms of Inter-Arrival Time (IAT) that, once captured into a specific 2nd order Markov Modulated Poisson Process (MMPP(2)) model, enable accurate estimation of interference. We further use a hidden Markov model (HMM) for channel occupancy prediction. We evaluated the performance of i) the MMPP(2) traffic model w.r.t. real-world traces and an existing Pareto model for accurately characterizing the WiFi traffic and, ii) compared the HMM based white space prediction to random channel access. We report encouraging results for using interference modeling for white space prediction."
Dyson models under renormalization and in weak fields,"We consider one-dimensional long-range spin models (usually called Dyson models), consisting of Ising ferromagnets with slowly decaying long-range pair potentials of the form $\frac{1}{|i-j|^{\alpha}}$ mainly focusing on the range of slow decays $1 < \alpha \leq 2$. We describe two recent results, one about renormalization and one about the effect of external fields at low temperature. The first result states that a decimated long-range Gibbs measure in one dimension becomes non-Gibbsian, in the same vein as comparable results in higher dimensions for short-range models. The second result addresses the behaviour of such models under inhomogeneous fields, in particular external fields which decay to zero polynomially as $(|i|+1)^{- \gamma}$. We study how the critical decay power of the field, $\gamma$, for which the phase transition persists and the decay power $\alpha$ of the Dyson model compare, extending recent results for short-range models on lattices and on trees. We also briefly point out some analogies between these results."
Coupling geometry on binary bipartite networks: hypotheses testing on pattern geometry and nestedness,"  Upon a matrix representation of a binary bipartite network, via the permutation invariance, a coupling geometry is computed to approximate the minimum energy macrostate of a network's system. Such a macrostate is supposed to constitute the intrinsic structures of the system, so that the coupling geometry should be taken as information contents, or even the nonparametric minimum sufficient statistics of the network data. Then pertinent null and alternative hypotheses, such as nestedness, are to be formulated according to the macrostate. That is, any efficient testing statistic needs to be a function of this coupling geometry. These conceptual architectures and mechanisms are by and large still missing in community ecology literature, and rendered misconceptions prevalent in this research area. Here the algorithmically computed coupling geometry is shown consisting of deterministic multiscale block patterns, which are framed by two marginal ultrametric trees on row and column axes, and stochastic uniform randomness within each block found on the finest scale. Functionally a series of increasingly larger ensembles of matrix mimicries is derived by conforming to the multiscale block configurations. Here matrix mimicking is meant to be subject to constraints of row and column sums sequences. Based on such a series of ensembles, a profile of distributions becomes a natural device for checking the validity of testing statistics or structural indexes. An energy based index is used for testing whether network data indeed contains structural geometry. A new version block-based nestedness index is also proposed. Its validity is checked and compared with the existing ones. A computing paradigm, called Data Mechanics, and its application on one real data network are illustrated throughout the developments and discussions in this paper. "
"MUSE-inspired view of the quasar Q2059-360, its Lyman alpha blob, and its neighborhood","The radio-quiet quasar Q2059-360 at redshift $z=3.08$ is known to be close to a small Lyman $\alpha$ blob (LAB) and to be absorbed by a proximate damped Ly$\alpha$ (PDLA) system. Here, we present the Multi Unit Spectroscopic Explorer (MUSE) integral field spectroscopy follow-up of this quasi-stellar object (QSO). Our primary goal is to characterize this LAB in detail by mapping it both spatially and spectrally using the Ly$\alpha$ line, and by looking for high-ionization lines to constrain the emission mechanism. Combining the high sensitivity of the MUSE integral field spectrograph mounted on the Yepun telescope at ESO-VLT with the natural coronagraph provided by the PDLA, we map the LAB down to the QSO position, after robust subtraction of QSO light in the spectral domain. In addition to confirming earlier results for the small bright component of the LAB, we unveil a faint filamentary emission protruding to the south over about 80 pkpc (physical kpc); this results in a total size of about 120 pkpc. We derive the velocity field of the LAB (assuming no transfer effects) and map the Ly$\alpha$ line width. Upper limits are set to the flux of the N V $\lambda 1238-1242$, C IV $\lambda 1548-1551$, He II $\lambda 1640$, and C III] $\lambda 1548-1551$ lines. We have discovered two probable Ly$\alpha$ emitters at the same redshift as the LAB and at projected distances of 265 kpc and 207 kpc from the QSO; their Ly$\alpha$ luminosities might well be enhanced by the QSO radiation. We also find an emission line galaxy at $z=0.33$ near the line of sight to the QSO. This LAB shares the same general characteristics as the 17 others surrounding radio-quiet QSOs presented previously. However, there are indications that it may be centered on the PDLA galaxy rather than on the QSO."
Community Detection on Euclidean Random Graphs,"  We study the problem of community detection (CD) on Euclidean random geometric graphs where each vertex has two latent variables: a binary community label and a $\mathbb{R}^d$ valued location label which forms the support of a Poisson point process of intensity $\lambda$. A random graph is then drawn with edge probabilities dependent on both the community and location labels. In contrast to the stochastic block model (SBM) that has no location labels, the resulting random graph contains many more short loops due to the geometric embedding. We consider the recovery of the community labels, partial and exact, using the random graph and the location labels. We establish phase transitions for both sparse and logarithmic degree regimes, and provide bounds on the location of the thresholds, conjectured to be tight in the case of exact recovery. We also show that the threshold of the distinguishability problem, i.e., the testing between our model and the null model without community labels exhibits no phase-transition and in particular, does not match the weak recovery threshold (in contrast to the SBM). "
Load balancing with heterogeneous schedulers,"Load balancing is a common approach in web server farms or inventory routing problems. An important issue in such systems is to determine the server to which an incoming request should be routed to optimize a given performance criteria. In this paper, we assume the server's scheduling disciplines to be heterogeneous. More precisely, a server implements a scheduling discipline which belongs to the class of limited processor sharing (LPS-$d$) scheduling disciplines. Under LPS-$d$, up to $d$ jobs can be served simultaneously, and hence, includes as special cases First Come First Served ($d=1$) and Processor Sharing ($d=\infty$). In order to obtain efficient heuristics, we model the above load-balancing framework as a multi-armed restless bandit problem. Using the relaxation technique, as first developed in the seminal work of Whittle, we derive Whittle's index policy for general cost functions and obtain a closed-form expression for Whittle's index in terms of the steady-state distribution. Through numerical computations, we investigate the performance of Whittle's index with two different performance criteria: linear cost criterion and a cost criterion that depends on the first and second moment of the throughput. Our results show that \emph{(i)} the structure of Whittle's index policy can strongly depend on the scheduling discipline implemented in the server, i.e., on $d$, and that \emph{(ii)} Whittle's index policy significantly outperforms standard dispatching rules such as Join the Shortest Queue (JSQ), Join the Shortest Expected Workload (JSEW), and Random Server allocation (RSA)."
An Information Theoretic Framework for Active De-anonymization in Social Networks Based on Group Memberships,"  In this paper, a new mathematical formulation for the problem of de-anonymizing social network users by actively querying their membership in social network groups is introduced. In this formulation, the attacker has access to a noisy observation of the group membership of each user in the social network. When an unidentified victim visits a malicious website, the attacker uses browser history sniffing to make queries regarding the victim's social media activity. Particularly, it can make polar queries regarding the victim's group memberships and the victim's identity. The attacker receives noisy responses to her queries. The goal is to de-anonymize the victim with the minimum number of queries. Starting with a rigorous mathematical model for this active de-anonymization problem, an upper bound on the attacker's expected query cost is derived, and new attack algorithms are proposed which achieve this bound. These algorithms vary in computational cost and performance. The results suggest that prior heuristic approaches to this problem provide sub-optimal solutions. "
Geobiodynamics and Roegenian Economic Systems,"  This mathematical essay brings together ideas from Economics, Geobiodynamics and Thermodynamics. Its purpose is to obtain real models of complex evolutionary systems. More specifically, the essay defines Roegenian Economy and links Geobiodynamics and Roegenian Economy. In this context, we discuss the isomorphism between the concepts and techniques of Thermodynamics and Economics. Then we describe a Roegenian economic system like a Carnot group. After we analyse the phase equilibrium for two heterogeneous economic systems. The European Union Economics appears like Cartesian product of Roegenian economic systems and its Balance is analysed in details. A Section at the end describes the ""economic black holes"" as small parts of a a global economic system in which national income is so great that it causes others poor enrichment. These ideas can be used to improve our knowledge and understanding of the nature of development and evolution of thermodynamic-economic systems. "
The effects of the overshooting of the convective core on main-sequence turnoffs of young- and intermediate-age star clusters,"Recent investigations have shown that the extended main-sequence turnoffs (eMSTOs) are a common feature of intermediate-age star clusters in the Magellanic Clouds. The eMSTOs are also found in the color-magnitude diagram (CMD) of young-age star clusters. The origin of the eMSTOs is still an open question. Moreover, asteroseismology shows that the value of the overshooting parameter $\delta_{\rm ov}$ of the convective core is not fixed for the stars with an approximatelly equal mass. Thus the MSTO of star clusters may be affected by the overshooting of the convective core (OVCC). We calculated the effects of the OVCC with different $\delta_{\rm ov}$ on the MSTO of young- and intermediate-age star clusters. \textbf{If $\delta_{\rm ov}$ varies between stars in a cluster,} the observed eMSTOs of young- and intermediate-age star clusters can be explained well by the effects. The equivalent age spreads of MSTO caused by the OVCC are related to the age of star clusters and are in good agreement with observed results of many clusters. Moreover, the observed eMSTOs of NGC 1856 are reproduced by the coeval populations with different $\delta_{\rm ov}$. The eMSTOs of star clusters may be relevant to the effects of the OVCC. The effects of the OVCC \textbf{are similar to that of rotation in some respects. But the effects cannot result in a significant split of main sequence of young star clusters at $m_{U}\lesssim 21$.} The presence of a rapid rotation can make the split of main sequence of young star clusters more significant."
First eigenvalue estimates of Dirichlet-to-Neumann operators on graphs,"Following Escobar [Esc97] and Jammes [Jam15], we introduce two types of isoperimetric constants and give lower bound estimates for the first nontrivial eigenvalues of Dirichlet-to-Neumann operators on finite graphs with boundary respectively."
Air-burst Generated Tsunamis,"  This paper examines the questions of whether smaller asteroids that burst in the air over water can generate tsunamis that could pose a threat to distant locations. Such air burst-generated tsunamis are qualitatively different than the more frequently studied earthquake-generated tsunamis, and differ as well from impact asteroids. Numerical simulations are presented using the shallow water equations in several settings, demonstrating very little tsunami threat from this scenario. A model problem with an explicit solution that demonstrates and explains the same phenomena found in the computations is analyzed. We discuss the question of whether compressibility and dispersion are important effects that should be included, and show results from a more sophisticated model problem using the linearized Euler equations that begins to addresses this. "
A new design principle of robust onion-like networks self-organized in growth,"  Today's economy, production activity, and our life are sustained by social and technological network infrastructures, while new threats of network attacks by destructing loops have been found recently in network science. We inversely take into account the weakness, and propose a new design principle for incrementally growing robust networks. The networks are self-organized by enhancing interwoven long loops. In particular, we consider the range-limited approximation of linking by intermediations in a few hops, and show the strong robustness in the growth without degrading efficiency of paths. Moreover, we demonstrate that the tolerance of connectivity is reformable even from extremely vulnerable real networks according to our proposed growing process with some investment. These results may indicate a prospective direction to the future growth of our network infrastructures. "
Surrogate Losses for Online Learning of Stepsizes in Stochastic Non-Convex Optimization,"  Stochastic Gradient Descent (SGD) has played a central role in machine learning. However, it requires a carefully hand-picked stepsize for fast convergence, which is notoriously tedious and time-consuming to tune. Over the last several years, a plethora of adaptive gradient-based algorithms have emerged to ameliorate this problem. They have proved efficient in reducing the labor of tuning in practice, but many of them lack theoretic guarantees even in the convex setting. In this paper, we propose new surrogate losses to cast the problem of learning the optimal stepsizes for the stochastic optimization of a non-convex smooth objective function onto an online convex optimization problem. This allows the use of no-regret online algorithms to compute optimal stepsizes on the fly. In turn, this results in a SGD algorithm with self-tuned stepsizes that guarantees convergence rates that are automatically adaptive to the level of noise. "
On Categorical Time Series Models With Covariates,"  We study the problem of stationarity and ergodicity for autoregressive multinomial logistic time series models which possibly include a latent process and are defined by a GARCH-type recursive equation. We improve considerably upon the existing results related to stationarity and ergodicity conditions of such models. Proofs are based on theory developed for chains with complete connections. This approach is based on a useful coupling technique which is utilized for studying ergodicity of more general finite-state stochastic processes. Such processes generalize finite-state Markov chains by assuming infinite order models of past values. For finite order Markov chains, we also discuss ergodicity properties when some strongly exogenous covariates are considered in the dynamics of the process. "
Layered Coding for Energy Harvesting Communication Without CSIT,"  Due to stringent constraints on resources, it may be infeasible to acquire the current channel state information at the transmitter in energy harvesting communication systems. In this paper, we optimize an energy harvesting transmitter, communicating over a slow fading channel, using layered coding. The transmitter has access to the channel statistics, but does not know the exact channel state. In layered coding, the codewords are first designed for each of the channel states at different rates, and then the codewords are either time-multiplexed or superimposed before the transmission, leading to two transmission strategies. The receiver then decodes the information adaptively based on the realized channel state. The transmitter is equipped with a finite-capacity battery having non-zero internal resistance. In each of the transmission strategies, we first formulate and study an average rate maximization problem with non-causal knowledge of the harvested power variations. Further, assuming statistical knowledge and causal information of the harvested power variations, we propose a sub-optimal algorithm, and compare with the stochastic dynamic programming based solution and a greedy policy. "
The automorphisms of Petit's algebras,"Let $\sigma$ be an automorphism of a field $K$ with fixed field $F$. We study the automorphisms of nonassociative unital algebras which are canonical generalizations of the associative quotient algebras $K[t;\sigma]/fK[t;\sigma]$ obtained when the twisted polynomial $f\in K[t;\sigma]$ is invariant, and were first defined by Petit. We compute all their automorphisms if $\sigma$ commutes with all automorphisms in ${\rm Aut}_F(K)$ and $n\geq m-1$, where $n$ is the order of $\sigma$ and $m$ the degree of $f$,and obtain partial results for $n<m-1$. In the case where $K/F$ is a finite Galois field extension, we obtain more detailed information on the structure of the automorphism groups of these nonassociative unital algebras over $F$. We also briefly investigate when two such algebras are isomorphic."
A convolutional autoencoder approach for mining features in cellular electron cryo-tomograms and weakly supervised coarse segmentation,"Cellular electron cryo-tomography enables the 3D visualization of cellular organization in the near-native state and at submolecular resolution. However, the contents of cellular tomograms are often complex, making it difficult to automatically isolate different in situ cellular components. In this paper, we propose a convolutional autoencoder-based unsupervised approach to provide a coarse grouping of 3D small subvolumes extracted from tomograms. We demonstrate that the autoencoder can be used for efficient and coarse characterization of features of macromolecular complexes and surfaces, such as membranes. In addition, the autoencoder can be used to detect non-cellular features related to sample preparation and data collection, such as carbon edges from the grid and tomogram boundaries. The autoencoder is also able to detect patterns that may indicate spatial interactions between cellular components. Furthermore, we demonstrate that our autoencoder can be used for weakly supervised semantic segmentation of cellular components, requiring a very small amount of manual annotation."
Probabilistic Rule Realization and Selection,"  Abstraction and realization are bilateral processes that are key in deriving intelligence and creativity. In many domains, the two processes are approached through rules: high-level principles that reveal invariances within similar yet diverse examples. Under a probabilistic setting for discrete input spaces, we focus on the rule realization problem which generates input sample distributions that follow the given rules. More ambitiously, we go beyond a mechanical realization that takes whatever is given, but instead ask for proactively selecting reasonable rules to realize. This goal is demanding in practice, since the initial rule set may not always be consistent and thus intelligent compromises are needed. We formulate both rule realization and selection as two strongly connected components within a single and symmetric bi-convex problem, and derive an efficient algorithm that works at large scale. Taking music compositional rules as the main example throughout the paper, we demonstrate our model's efficiency in not only music realization (composition) but also music interpretation and understanding (analysis). "
Asymptotic and numerical analysis of a porous medium model for transpiration-driven sap flow in trees,"We develop a 3D porous medium model for sap flow within a tree stem, which consists of a nonlinear parabolic partial differential equation with a suitable transpiration source term. Using an asymptotic analysis, we derive approximate series solutions for the liquid saturation and sap velocity for a general class of coefficient functions. Several important non-dimensional parameters are identified that can be used to characterize various flow regimes. We investigate the relative importance of stem aspect ratio versus anisotropy in the sapwood hydraulic conductivity, and how these two effects impact the radial and vertical components of sap velocity. The analytical results are validated by means of a second-order finite volume discretization of the governing equations, and comparisons are drawn to experimental results on Norway spruce trees."
Risk Estimators for Choosing Regularization Parameters in Ill-Posed Problems - Properties and Limitations,"  This paper discusses the properties of certain risk estimators recently proposed to choose regularization parameters in ill-posed problems. A simple approach is Stein's unbiased risk estimator (SURE), which estimates the risk in the data space, while a recent modification (GSURE) estimates the risk in the space of the unknown variable. It seems intuitive that the latter is more appropriate for ill-posed problems, since the properties in the data space do not tell much about the quality of the reconstruction. We provide theoretical studies of both estimators for linear Tikhonov regularization in a finite dimensional setting and estimate the quality of the risk estimators, which also leads to asymptotic convergence results as the dimension of the problem tends to infinity. Unlike previous papers, who studied image processing problems with a very low degree of ill-posedness, we are interested in the behavior of the risk estimators for increasing ill-posedness. Interestingly, our theoretical results indicate that the quality of the GSURE risk can deteriorate asymptotically for ill-posed problems, which is confirmed by a detailed numerical study. The latter shows that in many cases the GSURE estimator leads to extremely small regularization parameters, which obviously cannot stabilize the reconstruction. Similar but less severe issues with respect to robustness also appear for the SURE estimator, which in comparison to the rather conservative discrepancy principle leads to the conclusion that regularization parameter choice based on unbiased risk estimation is not a reliable procedure for ill-posed problems. A similar numerical study for sparsity regularization demonstrates that the same issue appears in nonlinear variational regularization approaches. "
Analyzing Effects of Seasonal Variations in Wind Generation and Load on Voltage Profiles,"This paper presents a methodology for building daily profiles of wind generation and load for different seasons to assess their impacts on voltage violations. The measurement-based wind models showed very high accuracy when validated against several years of actual wind power data. System load modeling was carried out by analyzing the seasonal trends that occur in residential, commercial, and industrial loads. When the proposed approach was implemented on the IEEE 118-bus system, it could identify violations in bus voltage profiles that the season-independent model could not capture. The results of the proposed approach are expected to provide better visualization of the problems that seasonal variations in wind power and load might cause to the electric power grid."
Imitation Learning from Imperfect Demonstration,"Imitation learning (IL) aims to learn an optimal policy from demonstrations. However, such demonstrations are often imperfect since collecting optimal ones is costly. To effectively learn from imperfect demonstrations, we propose a novel approach that utilizes confidence scores, which describe the quality of demonstrations. More specifically, we propose two confidence-based IL methods, namely two-step importance weighting IL (2IWIL) and generative adversarial IL with imperfect demonstration and confidence (IC-GAIL). We show that confidence scores given only to a small portion of sub-optimal demonstrations significantly improve the performance of IL both theoretically and empirically."
Some Bounds on Binary LCD Codes,"A linear code with a complementary dual (or LCD code) is defined to be a linear code $C$ whose dual code $C^{\perp}$ satisfies $C \cap C^{\perp}$= $\left\{ \mathbf{0}\right\} $. Let $LCD{[}n,k{]}$ denote the maximum of possible values of $d$ among $[n,k,d]$ binary LCD codes. We give exact values of $LCD{[}n,k{]}$ for $1 \le k \le n \le 12$. We also show that $LCD[n,n-i]=2$ for any $i\geq2$ and $n\geq2^{i}$. Furthermore, we show that $LCD[n,k]\leq LCD[n,k-1]$ for $k$ odd and $LCD[n,k]\leq LCD[n,k-2]$ for $k$ even."
Control-Oriented Learning on the Fly,"  This paper focuses on developing a strategy for control of systems whose dynamics are almost entirely unknown. This situation arises naturally in a scenario where a system undergoes a critical failure. In that case, it is imperative to retain the ability to satisfy basic control objectives in order to avert an imminent catastrophe. A prime example of such an objective is the reach-avoid problem, where a system needs to move to a certain state in a constrained state space. To deal with limitations on our knowledge of system dynamics, we develop a theory of myopic control. The primary goal of myopic control is to, at any given time, optimize the current direction of the system trajectory, given solely the information obtained about the system until that time. We propose an algorithm that uses small perturbations in the control effort to learn local dynamics while simultaneously ensuring that the system moves in a direction that appears to be nearly optimal, and provide hard bounds for its suboptimality. We additionally verify the usefulness of the algorithm on a simulation of a damaged aircraft seeking to avoid a crash, as well as on an example of a Van der Pol oscillator. "
Multi Agent Driven Data Mining For Knowledge Discovery in Cloud Computing,"  Today, huge amount of data is available on the web. Now there is a need to convert that data in knowledge which can be useful for different purposes. This paper depicts the use of data mining process, OLAP with the combination of multi agent system to find the knowledge from data in cloud computing. For this, I am also trying to explain one case study of online shopping of one Bakery Shop. May be we can increase the sale of items by using the model, which I am trying to represent. "
Matrix Completion from $O(n)$ Samples in Linear Time,"We consider the problem of reconstructing a rank-$k$ $n \times n$ matrix $M$ from a sampling of its entries. Under a certain incoherence assumption on $M$ and for the case when both the rank and the condition number of $M$ are bounded, it was shown in \cite{CandesRecht2009, CandesTao2010, keshavan2010, Recht2011, Jain2012, Hardt2014} that $M$ can be recovered exactly or approximately (depending on some trade-off between accuracy and computational complexity) using $O(n \, \text{poly}(\log n))$ samples in super-linear time $O(n^{a} \, \text{poly}(\log n))$ for some constant $a \geq 1$. In this paper, we propose a new matrix completion algorithm using a novel sampling scheme based on a union of independent sparse random regular bipartite graphs. We show that under the same conditions w.h.p. our algorithm recovers an $\epsilon$-approximation of $M$ in terms of the Frobenius norm using $O(n \log^2(1/\epsilon))$ samples and in linear time $O(n \log^2(1/\epsilon))$. This provides the best known bounds both on the sample complexity and computational complexity for reconstructing (approximately) an unknown low-rank matrix. The novelty of our algorithm is two new steps of thresholding singular values and rescaling singular vectors in the application of the ""vanilla"" alternating minimization algorithm. The structure of sparse random regular graphs is used heavily for controlling the impact of these regularization steps."
"Ingestion, Indexing and Retrieval of High-Velocity Multidimensional Sensor Data on a Single Node","  Multidimensional data are becoming more prevalent, partly due to the rise of the Internet of Things (IoT), and with that the need to ingest and analyze data streams at rates higher than before. Some industrial IoT applications require ingesting millions of records per second, while processing queries on recently ingested and historical data. Unfortunately, existing database systems suited to multidimensional data exhibit low per-node ingestion performance, and even if they can scale horizontally in distributed settings, they require large number of nodes to meet such ingest demands. For this reason, in this paper we evaluate a single-node multidimensional data store for high-velocity sensor data. Its design centers around a two-level indexing structure, wherein the global index is an in-memory R*-tree and the local indices are serialized kd-trees. This study is confined to records with numerical indexing fields and range queries, and covers ingest throughput, query response time, and storage footprint. We show that the adopted design streamlines data ingestion and offers ingress rates two orders of magnitude higher than those of Percona Server, SQLite, and Druid. Our prototype also reports query response times comparable to or better than those of Percona Server and Druid, and compares favorably in terms of storage footprint. In addition, we evaluate a kd-tree partitioning based scheme for grouping incoming streamed data records. Compared to a random scheme, this scheme produces less overlap between groups of streamed records, but contrary to what we expected, such reduced overlap does not translate into better query performance. By contrast, the local indices prove much more beneficial to query performance. We believe the experience reported in this paper is valuable to practitioners and researchers alike interested in building database systems for high-velocity multidimensional data. "
Electron and thermal transport via Variable Range Hopping in MoSe$_{2}$ single crystals,"Bulk single crystal Molybdenum diselenide has been studied for its electronic and thermal transport properties. We perform resistivity measurements with current in-plane (CIP) and current perpendicular to plane (CPP) as a function of temperature. The CIP measurements exhibit metal to semiconductor transition at $\simeq 31$ K. In the semiconducting phase ($T > 31$ K), the transport is best explained by variable range hopping (VRH) model. Large magnitude of resistivity in CPP mode indicates strong structural anisotropy. Seebeck coefficient as a function of temperature measured in the range $90 - 300$ K, also agrees well with the VRH model. The room temperature Seebeck coefficient is found to be $139$ $\mu$V/K. VRH fittings of the resistivity and Seebeck coefficient data indicate high degree of localization."
Ball in double hoop: demonstration model for numerical optimal control,"  Ball and hoop system is a well-known model for the education of linear control systems. In this paper, we have a look at this system from another perspective and show that it is also suitable for demonstration of more advanced control techniques. In contrast to the standard use, we describe the dynamics of the system at full length; in addition to the mode where the ball rolls on the (outer) hoop we also consider the mode where the ball drops out of the hoop and enters a free-fall mode. Furthermore, we add another (inner) hoop in the center upon which the ball can land from the free-fall mode. This constitutes another mode of the hybrid description of the system. We present two challenging tasks for this model and show how they can be solved by trajectory generation and stabilization. We also describe how such a model can be built and experimentally verify the validity of our approach solving the proposed tasks. "
Forecasting market states,"  We propose a novel methodology to define, analyse and forecast market states. In our approach market states are identified by a reference sparse precision matrix and a vector of expectation values. In our procedure each multivariate observation is associated to a given market state accordingly to a penalized likelihood maximization. The procedure is made computationally very efficient and can be used with a large number of assets. We demonstrate that this procedure successfully classifies different states of the markets in an unsupervised manner. In particular, we describe an experiment with one hundred log-returns and two states in which the methodology automatically associates one state to periods with average positive returns and the other state to periods with average negative returns, therefore discovering spontaneously the common classification of `bull' and `bear' markets. In another experiment, with again one hundred log-returns and two states, we demonstrate that this procedure can be efficiently used to forecast off-sample future market states with significant prediction accuracy. This methodology opens the way to a range of applications in risk management and trading strategies where the correlation structure plays a central role. "
Planetary Ring Dynamics -- The Streamline Formalism -- 2. Theory of Narrow Rings and Sharp Edges,"The present material covers the features of large scale ring dynamics in perturbed flows that were not addressed in part 1 (astro-ph/1606.00759); this includes an extensive coverage of all kinds of ring modes dynamics (except density waves which have been covered in part 1), the origin of ring eccentricities and mode amplitudes, and the issue of ring/gap confinement. This still leaves aside a number of important dynamical issues relating to the ring small scale structure, most notably the dynamics of self-gravitational wakes, of local viscous overstabilities and of ballistic transport processes. As this material is designed to be self-contained, there is some 30% overlap with part 1. This work constitutes a preprint of Chapter 11 of the forthcoming Cambridge University book on rings (Planetary Ring Systems, Matt Tiscareno and Carl Murray, eds)."
Parallel Streaming Wasserstein Barycenters,"  Efficiently aggregating data from different sources is a challenging problem, particularly when samples from each source are distributed differently. These differences can be inherent to the inference task or present for other reasons: sensors in a sensor network may be placed far apart, affecting their individual measurements. Conversely, it is computationally advantageous to split Bayesian inference tasks across subsets of data, but data need not be identically distributed across subsets. One principled way to fuse probability distributions is via the lens of optimal transport: the Wasserstein barycenter is a single distribution that summarizes a collection of input measures while respecting their geometry. However, computing the barycenter scales poorly and requires discretization of all input distributions and the barycenter itself. Improving on this situation, we present a scalable, communication-efficient, parallel algorithm for computing the Wasserstein barycenter of arbitrary distributions. Our algorithm can operate directly on continuous input distributions and is optimized for streaming data. Our method is even robust to nonstationary input distributions and produces a barycenter estimate that tracks the input measures over time. The algorithm is semi-discrete, needing to discretize only the barycenter estimate. To the best of our knowledge, we also provide the first bounds on the quality of the approximate barycenter as the discretization becomes finer. Finally, we demonstrate the practical effectiveness of our method, both in tracking moving distributions on a sphere, as well as in a large-scale Bayesian inference task. "
Lecar's visual comparison method to assess the randomness of Bode's law: an answer,The usual main objection against any attempt in finding a physical cause for the planet distance distribution is based on the assumption that similar distance distribution could be obtained by sequences of random numbers. This assumption was stated by Lecar in an old paper (1973). We show here how this assumption is incorrect and how his visual comparison method is inappropriate.
GoDP: Globally optimized dual pathway system for facial landmark localization in-the-wild,"Facial landmark localization is a fundamental module for pose-invariant face recognition. The most common approach for facial landmark detection is cascaded regression, which is composed of two steps: feature extraction and facial shape regression. Recent methods employ deep convolutional networks to extract robust features for each step, while the whole system could be regarded as a deep cascaded regression architecture. In this work, instead of employing a deep regression network, a Globally Optimized Dual-Pathway (GoDP) deep architecture is proposed to identify the target pixels through solving a cascaded pixel labeling problem without resorting to high-level inference models or complex stacked architecture. The proposed end-to-end system relies on distance-aware softmax functions and dual-pathway proposal-refinement architecture. Results show that it outperforms the state-of-the-art cascaded regression-based methods on multiple in-the-wild face alignment databases. The model achieves 1.84 normalized mean error (NME) on the AFLW database, which outperforms 3DDFA by 61.8%. Experiments on face identification demonstrate that GoDP, coupled with DPM-headhunter, is able to improve rank-1 identification rate by 44.2% compared to Dlib toolbox on a challenging database."
"A function with support of finite measure and ""small"" spectrum",  We construct a function on the real line supported on a set of finite measure whose spectrum has density zero. 
Cell growth rate dictates the onset of glass to fluid-like transition and long time super-diffusion in an evolving cell colony,"Collective migration dominates many phenomena, from cell movement in living systems to abiotic self-propelling particles. Focusing on the early stages of tumor evolution, we enunciate the principles involved in cell dynamics and highlight their implications in understanding similar behavior in seemingly unrelated soft glassy materials and possibly chemokine-induced migration of CD8$^{+}$ T cells. We performed simulations of tumor invasion using a minimal three dimensional model, accounting for cell elasticity and adhesive cell-cell interactions as well as cell birth and death to establish that cell growth rate-dependent tumor expansion results in the emergence of distinct topological niches. Cells at the periphery move with higher velocity perpendicular to the tumor boundary, while motion of interior cells is slower and isotropic. The mean square displacement, $\Delta(t)$, of cells exhibits glassy behavior at times comparable to the cell cycle time, while exhibiting super-diffusive behavior, $\Delta (t) \approx t^{\alpha}$ ($\alpha > 1$), at longer times. We derive the value of $\alpha \approx 1.33$ using a field theoretic approach based on stochastic quantization. In the process we establish the universality of super-diffusion in a class of seemingly unrelated non-equilibrium systems. Super diffusion at long times arises only if there is an imbalance between cell birth and death rates. Our findings for the collective migration, which also suggests that tumor evolution occurs in a polarized manner, are in quantitative agreement with {\it in vitro} experiments. Although set in the context of tumor invasion the findings should also hold in describing collective motion in growing cells and in active systems where creation and annihilation of particles play a role."
Distributed Average Tracking for Lipschitz-Type Nonlinear Dynamical Systems,"  In this paper, a distributed average tracking problem is studied for Lipschitz-type nonlinear dynamical systems. The objective is to design distributed average tracking algorithms for locally interactive agents to track the average of multiple reference signals. Here, in both the agents' and the reference signals' dynamics, there is a nonlinear term satisfying the Lipschitz-type condition. Three types of distributed average tracking algorithms are designed. First, based on state-dependent-gain designing approaches, a robust distributed average tracking algorithm is developed to solve distributed average tracking problems without requiring the same initial condition. Second, by using a gain adaption scheme, an adaptive distributed average tracking algorithm is proposed in this paper to remove the requirement that the Lipschitz constant is known for agents. Third, to reduce chattering and make the algorithms easier to implement, a continuous distributed average tracking algorithm based on a time-varying boundary layer is further designed as a continuous approximation of the previous discontinuous distributed average tracking algorithms. "
On the Möbius Function and Topology of General Pattern Posets,"  We introduce a formal definition of a pattern poset which encompasses several previously studied posets in the literature. Using this definition we present some general results on the Möbius function and topology of such pattern posets. We prove our results using a poset fibration based on the embeddings of the poset, where embeddings are representations of occurrences. We show that the Möbius function of these posets is intrinsically linked to the number of embeddings, and in particular to so called normal embeddings. We present results on when topological properties such as Cohen-Macaulayness and shellability are preserved by this fibration. Furthermore, we apply these results to some pattern posets and derive alternative proofs of existing results, such as Björner's results on subword order. "
A Potapov-type approach to a truncated matricial Stieltjes-type power moment problem,  The paper gives a parametrization of the solution set of a matricial Stieltjes-type truncated power moment problem in the non-degenerate and degenerate cases. The key role plays the solution of the corresponding system of Potapov's fundamental matrix inequalities. 
The scaling limit of the KPZ equation in space dimension 3 and higher,"We study in the present article the Kardar-Parisi-Zhang (KPZ) equation $$ \partial_t h(t,x)=\nu\Delta h(t,x)+\lambda |\nabla h(t,x)|^2 +\sqrt{D}\, \eta(t,x), \qquad (t,x)\in\mathbb{R}_+\times\mathbb{R}^d $$ in $d\ge 3$ dimensions in the perturbative regime, i.e. for $\lambda>0$ small enough and a smooth, bounded, integrable initial condition $h_0=h(t=0,\cdot)$. The forcing term $\eta$ in the right-hand side is a regularized space-time white noise. The exponential of $h$ -- its so-called Cole-Hopf transform -- is known to satisfy a linear PDE with multiplicative noise. We prove a large-scale diffusive limit for the solution, in particular a time-integrated heat-kernel behavior for the covariance in a parabolic scaling. The proof is based on a rigorous implementation of K. Wilson's renormalization group scheme. A double cluster/momentum-decoupling expansion allows for perturbative estimates of the bare resolvent of the Cole-Hopf linear PDE in the small-field region where the noise is not too large, following the broad lines of Iagolnitzer-Magnen. Standard large deviation estimates for $\eta$ make it possible to extend the above estimates to the large-field region. Finally, we show, by resumming all the by-products of the expansion, that the solution $h$ may be written in the large-scale limit (after a suitable Galilei transformation) as a small perturbation of the solution of the underlying linear Edwards-Wilkinson model ($\lambda=0$) with renormalized coefficients $\nu_{eff}=\nu+O(\lambda^2),D_{eff}=D+O(\lambda^2)$."
Beyond Free Riding: Quality of Indicators for Assessing Participation in Information Sharing for Threat Intelligence,"  Threat intelligence sharing has become a growing concept, whereby entities can exchange patterns of threats with each other, in the form of indicators, to a community of trust for threat analysis and incident response. However, sharing threat-related information have posed various risks to an organization that pertains to its security, privacy, and competitiveness. Given the coinciding benefits and risks of threat information sharing, some entities have adopted an elusive behavior of ""free-riding"" so that they can acquire the benefits of sharing without contributing much to the community. So far, understanding the effectiveness of sharing has been viewed from the perspective of the amount of information exchanged as opposed to its quality. In this paper, we introduce the notion of quality of indicators (\qoi) for the assessment of the level of contribution by participants in information sharing for threat intelligence. We exemplify this notion through various metrics, including correctness, relevance, utility, and uniqueness of indicators. In order to realize the notion of \qoi, we conducted an empirical study and taken a benchmark approach to define quality metrics, then we obtained a reference dataset and utilized tools from the machine learning literature for quality assessment. We compared these results against a model that only considers the volume of information as a metric for contribution, and unveiled various interesting observations, including the ability to spot low quality contributions that are synonym to free riding in threat information sharing. "
On the Design of LQR Kernels for Efficient Controller Learning,"  Finding optimal feedback controllers for nonlinear dynamic systems from data is hard. Recently, Bayesian optimization (BO) has been proposed as a powerful framework for direct controller tuning from experimental trials. For selecting the next query point and finding the global optimum, BO relies on a probabilistic description of the latent objective function, typically a Gaussian process (GP). As is shown herein, GPs with a common kernel choice can, however, lead to poor learning outcomes on standard quadratic control problems. For a first-order system, we construct two kernels that specifically leverage the structure of the well-known Linear Quadratic Regulator (LQR), yet retain the flexibility of Bayesian nonparametric learning. Simulations of uncertain linear and nonlinear systems demonstrate that the LQR kernels yield superior learning performance. "
How does propaganda influence the opinion dynamics of a population ?,"  The evolution of opinions in a population of individuals who constantly interact with a common source of user-generated content (i.e. the internet) and are also subject to propaganda is analyzed using computer simulations. The model is based on the bounded confidence approach. In the absence of propaganda, computer simulations show that the online population as a whole is either fragmented, polarized or in perfect harmony on a certain issue or ideology depending on the uncertainty of individuals in accepting opinions not closer to theirs. On applying the model to simulate radicalization, a proportion of the online population, subject to extremist propaganda radicalize depending on their pre-conceived opinions and opinion uncertainty. It is found that an optimal counter propaganda that prevents radicalization is not necessarily centrist. "
Explicit Solution for Constrained Stochastic Linear-Quadratic Control with Multiplicative Noise,"  We study in this paper a class of constrained linear-quadratic (LQ) optimal control problem formulations for the scalar-state stochastic system with multiplicative noise, which has various applications, especially in the financial risk management. The linear constraint on both the control and state variables considered in our model destroys the elegant structure of the conventional LQ formulation and has blocked the derivation of an explicit control policy so far in the literature. We successfully derive in this paper the analytical control policy for such a class of problems by utilizing the state separation property induced from its structure. We reveal that the optimal control policy is a piece-wise affine function of the state and can be computed off-line efficiently by solving two coupled Riccati equations. Under some mild conditions, we also obtain the stationary control policy for infinite time horizon. We demonstrate the implementation of our method via some illustrative examples and show how to calibrate our model to solve dynamic constrained portfolio optimization problems. "
Viewpoint Selection for Photographing Architectures,"This paper studies the problem of how to choose good viewpoints for taking photographs of architectures. We achieve this by learning from professional photographs of world famous landmarks that are available on the Internet. Unlike previous efforts devoted to photo quality assessment which mainly rely on 2D image features, we show in this paper combining 2D image features extracted from images with 3D geometric features computed on the 3D models can result in more reliable evaluation of viewpoint quality. Specifically, we collect a set of photographs for each of 15 world famous architectures as well as their 3D models from the Internet. Viewpoint recovery for images is carried out through an image-model registration process, after which a newly proposed viewpoint clustering strategy is exploited to validate users' viewpoint preferences when photographing landmarks. Finally, we extract a number of 2D and 3D features for each image based on multiple visual and geometric cues and perform viewpoint recommendation by learning from both 2D and 3D features using a specifically designed SVM-2K multi-view learner, achieving superior performance over using solely 2D or 3D features. We show the effectiveness of the proposed approach through extensive experiments. The experiments also demonstrate that our system can be used to recommend viewpoints for rendering textured 3D models of buildings for the use of architectural design, in addition to viewpoint evaluation of photographs and recommendation of viewpoints for photographing architectures in practice."
Geophysical tests for habitability in ice-covered ocean worlds,"Geophysical measurements can reveal the structure of icy ocean worlds and cycling of volatiles. The associated density, temperature, sound speed, and electrical conductivity of such worlds thus characterizes their habitability. To explore the variability and correlation of these parameters, and to provide tools for planning and data analyses, we develop 1-D calculations of internal structure, which use available constraints on the thermodynamics of aqueous MgSO$_4$, NaCl (as seawater), and NH$_3$, water ices, and silicate content. Limits in available thermodynamic data narrow the parameter space that can be explored: insufficient coverage in pressure, temperature, and composition for end-member salinities of MgSO$_4$ and NaCl, and for relevant water ices; and a dearth of suitable data for aqueous mixtures of Na-Mg-Cl-SO$_4$-NH$_3$. For Europa, ocean compositions that are oxidized and dominated by MgSO$_4$, vs reduced (NaCl), illustrate these gaps, but also show the potential for diagnostic and measurable combinations of geophysical parameters. The low-density rocky core of Enceladus may comprise hydrated minerals, or anydrous minerals with high porosity comparable to Earth's upper mantle. Titan's ocean must be dense, but not necessarily saline, as previously noted, and may have little or no high-pressure ice at its base. Ganymede's silicious interior is deepest among all known ocean worlds, and may contain multiple phases of high-pressure ice, which will become buoyant if the ocean is sufficiently salty. Callisto's likely near-eutectic ocean cannot be adequately modeled using available data. Callisto may also lack high-pressure ices, but this cannot be confirmed due to uncertainty in its moment of inertia."
A fast speed planning algorithm for robotic manipulators,"  We consider the speed planning problem for a robotic manipulator. In particular, we present an algorithm for finding the time-optimal speed law along an assigned path that satisfies velocity and acceleration constraints and respects the maximum forces and torques allowed by the actuators. The addressed optimization problem is a finite dimensional reformulation of the continuous-time speed optimization problem, obtained by discretizing the speed profile with N points. The proposed algorithm has linear complexity with respect to N and to the number of degrees of freedom. Such complexity is the best possible for this problem. Numerical tests show that the proposed algorithm is significantly faster than algorithms already existing in literature. "
Linearly convergent stochastic heavy ball method for minimizing generalization error,"  In this work we establish the first linear convergence result for the stochastic heavy ball method. The method performs SGD steps with a fixed stepsize, amended by a heavy ball momentum term. In the analysis, we focus on minimizing the expected loss and not on finite-sum minimization, which is typically a much harder problem. While in the analysis we constrain ourselves to quadratic loss, the overall objective is not necessarily strongly convex. "
Applying the Spacecraft with a Solar Sail to Form the Climate on a Mars Base,"  This article is devoted to research the application of the spacecraft with a solar sail to support the certain climatic conditions in an area of the Mars surface. Authors propose principles of functioning of the spacecraft, intended to create a light and thermal light spot in a predetermined area of the Martian surface. The mathematical motion model in such condition of the solar sail's orientation is considered and used for motion simulation session. Moreover, the analysis of this motion is performed. Thus, were obtained parameters of the stationary orbit of the spacecraft with a solar sail and were given recommendations for further applying spacecrafts to reflect the sunlight on a planet's surface. "
Data processing pipeline for Herschel HIFI,"{Context}. The HIFI instrument on the Herschel Space Observatory performed over 9100 astronomical observations, almost 900 of which were calibration observations in the course of the nearly four-year Herschel mission. The data from each observation had to be converted from raw telemetry into calibrated products and were included in the Herschel Science Archive. {Aims}. The HIFI pipeline was designed to provide robust conversion from raw telemetry into calibrated data throughout all phases of the HIFI missions. Pre-launch laboratory testing was supported as were routine mission operations. {Methods}. A modular software design allowed components to be easily added, removed, amended and/or extended as the understanding of the HIFI data developed during and after mission operations. {Results}. The HIFI pipeline processed data from all HIFI observing modes within the Herschel automated processing environment as well as within an interactive environment. The same software can be used by the general astronomical community to reprocess any standard HIFI observation. The pipeline also recorded the consistency of processing results and provided automated quality reports. Many pipeline modules were in use since the HIFI pre-launch instrument level testing. {Conclusions}. Processing in steps facilitated data analysis to discover and address instrument artefacts and uncertainties. The availability of the same pipeline components from pre-launch throughout the mission made for well-understood, tested, and stable processing. A smooth transition from one phase to the next significantly enhanced processing reliability and robustness."
Regret Analysis for Continuous Dueling Bandit,"  The dueling bandit is a learning framework wherein the feedback information in the learning process is restricted to a noisy comparison between a pair of actions. In this research, we address a dueling bandit problem based on a cost function over a continuous space. We propose a stochastic mirror descent algorithm and show that the algorithm achieves an $O(\sqrt{T\log T})$-regret bound under strong convexity and smoothness assumptions for the cost function. Subsequently, we clarify the equivalence between regret minimization in dueling bandit and convex optimization for the cost function. Moreover, when considering a lower bound in convex optimization, our algorithm is shown to achieve the optimal convergence rate in convex optimization and the optimal regret in dueling bandit except for a logarithmic factor. "
X-ray induced deuterium enrichment on N-rich organics in protoplanetary disks: an experimental investigation using synchrotron light,"The deuterium enrichment of organics in the interstellar medium, protoplanetary disks and meteorites has been proposed to be the result of ionizing radiation. The goal of this study is to quantify the effects of soft X-rays (0.1 - 2 keV), a component of stellar radiation fields illuminating protoplanetary disks, on the refractory organics present in the disks. We prepared tholins, nitrogen-rich complex organics, via plasma deposition and used synchrotron radiation to simulate X-ray fluences in protoplanetary disks. Controlled irradiation experiments at 0.5 and 1.3 keV were performed at the SEXTANTS beam line of the SOLEIL synchrotron, and were followed by ex-situ infrared, Raman and isotopic diagnostics. Infrared spectroscopy revealed the loss of singly-bonded groups (N-H, C-H and R-N$\equiv$C) and the formation of sp$^3$ carbon defects. Raman analysis revealed the introduction of defects and structural amorphization. Finally, tholins were measured via secondary ion mass spectrometry (SIMS), revealing that significant D-enrichment is induced by X-ray irradiation. Our results are compared to previous experimental studies involving the thermal degradation and electron irradiation of organics. The penetration depth of soft X-rays in $\mu$m-sized tholins leads to volume rather than surface modifications: lower energy X-rays (0.5 keV) induce a larger D-enrichment than 1.3 keV X-rays, reaching a plateau for doses larger than 5 $\times$ 10$^{27}$ eV cm$^{-3}$. Our work provides experimental evidence of a new non-thermal pathway to deuterium fractionation of organic matter."
Parisian ruin of Brownian motion risk model over an infinite-time horizon,"Let $B(t), t\in \mathbb{R}$ be a standard Brownian motion. In this paper, we derive the exact asymptotics of the probability of Parisian ruin on infinite time horizon for the following risk process \begin{align}\label{Rudef} R_u^{\delta}(t)=e^{\delta t}\left(u+c\int^{t}_{0}e^{-\delta v}d v-\sigma\int_{0}^{t}e^{-\delta v}d B(v)\right),\quad t\geq0, \end{align} where $u\geq 0$ is the initial reserve, $\delta\geq0$ is the force of interest, $c>0$ is the rate of premium and $\sigma>0$ is a volatility factor. Further, we show the asymptotics of the Parisian ruin time of this risk process."
"Exogeneity tests, incomplete models, weak identification and non-Gaussian distributions: invariance and finite-sample distributional theory","We study the distribution of Durbin-Wu-Hausman (DWH) and Revankar-Hartley (RH) tests for exogeneity from a finite-sample viewpoint, under the null and alternative hypotheses. We consider linear structural models with possibly non-Gaussian errors, where structural parameters may not be identified and where reduced forms can be incompletely specified (or nonparametric). On level control, we characterize the null distributions of all the test statistics. Through conditioning and invariance arguments, we show that these distributions do not involve nuisance parameters. In particular, this applies to several test statistics for which no finite-sample distributional theory is yet available, such as the standard statistic proposed by Hausman (1978). The distributions of the test statistics may be non-standard -- so corrections to usual asymptotic critical values are needed -- but the characterizations are sufficiently explicit to yield finite-sample (Monte-Carlo) tests of the exogeneity hypothesis. The procedures so obtained are robust to weak identification, missing instruments or misspecified reduced forms, and can easily be adapted to allow for parametric non-Gaussian error distributions. We give a general invariance result (block triangular invariance) for exogeneity test statistics. This property yields a convenient exogeneity canonical form and a parsimonious reduction of the parameters on which power depends. In the extreme case where no structural parameter is identified, the distributions under the alternative hypothesis and the null hypothesis are identical, so the power function is flat, for all the exogeneity statistics. However, as soon as identification does not fail completely, this phenomenon typically disappears."
Pair formation of hard core bosons in flat band systems,"  Hard core bosons in a large class of one or two dimensional flat band systems have an upper critical density, below which the ground states can be described completely. At the critical density, the ground states are Wigner crystals. If one adds a particle to the system at the critical density, the ground state and the low lying multi particle states of the system can be described as a Wigner crystal with an additional pair of particles. The energy band for the pair is separated from the rest of the multi-particle spectrum. The proofs use a Gerschgorin type of argument for block diagonally dominant matrices. In certain one-dimensional or tree-like structures one can show that the pair is localised, for example in the chequerboard chain. For this one-dimensional system with periodic boundary condition the energy band for the pair is flat, the pair is localised. "
"A Note on the Relationship Between Conditional and Unconditional Independence, and its Extensions for Markov Kernels","  Two known results on the relationship between conditional and unconditional independence are obtained as a consequence of the main result of this paper, a theorem that uses independence of Markov kernels to obtain a minimal condition which added to conditional independence implies independence. Some counterexamples and representation results are provided to clarify the concepts introduced and the propositions of the statement of the main theorem. Moreover, conditional independence and the mentioned results are extended to the framework of Markov kernels. "
The Effect of Temperature on Amdahl Law in 3D Multicore Era,"This work studies the influence of temperature on performance and scalability of 3D Chip Multiprocessors (CMP) from Amdahl law perspective. We find that 3D CMP may reach its thermal limit before reaching its maximum power. We show that a high level of parallelism may lead to high peak temperatures even in small scale 3D CMPs, thus limiting 3D CMP scalability and calling for different, in-memory computing architectures."
Video Object Segmentation using Supervoxel-Based Gerrymandering,"  Pixels operate locally. Superpixels have some potential to collect information across many pixels; supervoxels have more potential by implicitly operating across time. In this paper, we explore this well established notion thoroughly analyzing how supervoxels can be used in place of and in conjunction with other means of aggregating information across space-time. Focusing on the problem of strictly unsupervised video object segmentation, we devise a method called supervoxel gerrymandering that links masks of foregroundness and backgroundness via local and non-local consensus measures. We pose and answer a series of critical questions about the ability of supervoxels to adequately sway local voting; the questions regard type and scale of supervoxels as well as local versus non-local consensus, and the questions are posed in a general way so as to impact the broader knowledge of the use of supervoxels in video understanding. We work with the DAVIS dataset and find that our analysis yields an unsupervised method that outperforms all other known unsupervised methods and even many supervised ones. "
The Absent-Minded Driver Problem Redux,"  This paper reconsiders the problem of the absent-minded driver who must choose between alternatives with different payoff with imperfect recall and varying degrees of knowledge of the system. The classical absent-minded driver problem represents the case with limited information and it has bearing on the general area of communication and learning, social choice, mechanism design, auctions, theories of knowledge, belief, and rational agency. Within the framework of extensive games, this problem has applications to many artificial intelligence scenarios. It is obvious that the performance of the agent improves as information available increases. It is shown that a non-uniform assignment strategy for successive choices does better than a fixed probability strategy. We consider both classical and quantum approaches to the problem. We argue that the superior performance of quantum decisions with access to entanglement cannot be fairly compared to a classical algorithm. If the cognitive systems of agents are taken to have access to quantum resources, or have a quantum mechanical basis, then that can be leveraged into superior performance. "
Real-time Acceleration-continuous Path-constrained Trajectory Planning With Built-in Tradability Between Cruise and Time-optimal Motions,"  In this paper, a novel real-time acceleration-continuous path-constrained trajectory planning algorithm is proposed with an appealing built-in tradability mechanism between cruise motion and time-optimal motion. Different from existing approaches, the proposed approach smoothens time-optimal trajectories with bang-bang input structures to generate acceleration-continuous trajectories while preserving the completeness property. More importantly, a novel built-in tradability mechanism is proposed and embedded into the trajectory planning framework, so that the proportion of the cruise motion and time-optimal motion can be flexibly adjusted by changing a user-specified functional parameter. Thus, the user can easily apply the trajectory planning algorithm for various tasks with different requirements on motion efficiency and cruise proportion. Moreover, it is shown that feasible trajectories are computed more quickly than optimal trajectories. Rigorous mathematical analysis and proofs are provided for these aforementioned results. Comparative simulation and experimental results on omnidirectional wheeled mobile robots demonstrate the capability of the proposed algorithm in terms of flexible tunning between cruise and time-optimal motions, as well as higher computational efficiency. "
Multi-locus data distinguishes between population growth and multiple merger coalescents,"  We introduce a low dimensional function of the site frequency spectrum that is tailor-made for distinguishing coalescent models with multiple mergers from Kingman coalescent models with population growth, and use this function to construct a hypothesis test between these model classes. The null and alternative sampling distributions of the statistic are intractable, but its low dimensionality renders them amenable to Monte Carlo estimation. We construct kernel density estimates of the sampling distributions based on simulated data, and show that the resulting hypothesis test dramatically improves on the statistical power of a current state-of-the-art method. A key reason for this improvement is the use of multi-locus data, in particular averaging observed site frequency spectra across unlinked loci to reduce sampling variance. We also demonstrate the robustness of our method to nuisance and tuning parameters. Finally we show that the same kernel density estimates can be used to conduct parameter estimation, and argue that our method is readily generalisable for applications in model selection, parameter inference and experimental design. "
Ramsey interferometry of Rydberg ensembles inside microwave cavities,"We study ensembles of Rydberg atoms in a confined electromagnetic environment such as provided by a microwave cavity. The competition between standard free space Ising type and cavity-mediated interactions leads to the emergence of different regimes where the particle-particle couplings range from the typical van der Waals $r^{-6}$ behavior to $r^{-3}$ and to $r$-independence. We apply a Ramsey spectroscopic technique to map the two-body interactions into a characteristic signal such as intensity and contrast decay curves. As opposed to previous treatments requiring high-densities for considerable contrast and phase decay, the cavity scenario can exhibit similar behavior at much lower densities."
Impurity self-energy in the strongly-correlated Bose systems,We proposed the non-perturbative scheme for calculation of the impurity spectrum in the Bose system at zero temperature. The method is based on the path-integral formulation and describes an impurity as a zero-density ideal Fermi gas interacting with Bose system for which the action is written in terms of density fluctuations. On the example of the $^3$He atom immersed in the liquid helium-4 a good consistency with experimental data and results of Monte Carlo simulations is shown.
$k$-step correction for mixed integer linear programming: a new approach for instrumental variable quantile regressions and related problems,"This paper proposes a new framework for estimating instrumental variable (IV) quantile models. The first part of our proposal can be cast as a mixed integer linear program (MILP), which allows us to capitalize on recent progress in mixed integer optimization. The computational advantage of the proposed method makes it an attractive alternative to existing estimators in the presence of multiple endogenous regressors. This is a situation that arises naturally when one endogenous variable is interacted with several other variables in a regression equation. In our simulations, the proposed method using MILP with a random starting point can reliably estimate regressions for a sample size of 500 with 20 endogenous variables in 5 seconds. Theoretical results for early termination of MILP are also provided. The second part of our proposal is a $k$-step correction framework, which is proved to be able to convert any point within a small but fixed neighborhood of the true parameter value into an estimate that is asymptotically equivalent to GMM. Our result does not require the initial estimate to be consistent and only $2\log n$ iterations are needed. Since the $k$-step correction does not require any optimization, applying the $k$-step correction to MILP estimate provides a computationally attractive way of obtaining efficient estimators. When dealing with very large data sets, we can run the MILP algorithm on only a small subsample and our theoretical results guarantee that the resulting estimator from the $k$-step correction is equivalent to computing GMM on the full sample. As a result, we can handle massive datasets of millions of observations within seconds. As an empirical illustration, we examine the heterogeneous treatment effect of Job Training Partnership Act (JTPA) using a regression with 13 interaction terms of the treatment variable."
An MCMC Algorithm for Estimating the Reduced RUM,"The RRUM is a model that is frequently seen in language assessment studies. The objective of this research is to advance an MCMC algorithm for the Bayesian RRUM. The algorithm starts with estimating correlated attributes. Using a saturated model and a binary decimal conversion, the algorithm transforms possible attribute patterns to a Multinomial distribution. Along with the likelihood of an attribute pattern, a Dirichlet distribution is used as the prior to sample from the posterior. The Dirichlet distribution is constructed using Gamma distributions. Correlated attributes of examinees are generated using the inverse transform sampling. Model parameters are estimated using the Metropolis within Gibbs sampler sequentially. Two simulation studies are conducted to evaluate the performance of the algorithm. The first simulation uses a complete and balanced Q-matrix that measures 5 attributes. Comprised of 28 items and 9 attributes, the Q-matrix for the second simulation is incomplete and imbalanced. The empirical study uses the ECPE data obtained from the CDM R package. Parameter estimates from the MCMC algorithm and from the CDM R package are presented and compared. The algorithm developed in this research is implemented in R."
Stretching p-wave molecules by transverse confinements,"We revisit the confinement-induced p-wave resonance in quasi-one-dimensional (quasi-1D) atomic gases and study the induced molecules near resonance. We derive the reduced 1D interaction parameters and show that they can well predict the binding energy of shallow molecules in quasi-1D system. Importantly, these shallow molecules are found to be much more spatially extended compared to those in three dimensions (3D) without transverse confinement. Our results strongly indicate that a p-wave interacting atomic gas can be much more stable in quasi-1D near the induced p-wave resonance, where most weight of the molecule lies outside the short-range regime and thus the atom loss could be suppressed."
A Conceptual Framework for Supporting a Rapid Design of Web Applications for Data Analysis of Electrical Quality Assurance Data for the LHC,"  The Large Hadron Collider (LHC) is one of the most complex machines ever build. It is composed of many components which constitute a large system. The tunnel and the accelerator is just one of a very critical fraction of the whole LHC infrastructure. Hardware comissioning as one of the critical processes before running the LHC is implemented during the Long Shutdown (LS) states of the macine, where Electrical Quality Assurance (ELQA) is one of its key components. Here a huge data is collected when implementing various ELQA electrical tests. In this paper we present a conceptual framework for supporting a rapid design of web applications for ELQA data analysis. We show a framework's main components, their possible integration with other systems and machine learning algorithms and a simple use case of prototyping an application for Electrical Quality Assurance of the LHC. "
The Parallel Meaning Bank: Towards a Multilingual Corpus of Translations Annotated with Compositional Meaning Representations,"The Parallel Meaning Bank is a corpus of translations annotated with shared, formal meaning representations comprising over 11 million words divided over four languages (English, German, Italian, and Dutch). Our approach is based on cross-lingual projection: automatically produced (and manually corrected) semantic annotations for English sentences are mapped onto their word-aligned translations, assuming that the translations are meaning-preserving. The semantic annotation consists of five main steps: (i) segmentation of the text in sentences and lexical items; (ii) syntactic parsing with Combinatory Categorial Grammar; (iii) universal semantic tagging; (iv) symbolization; and (v) compositional semantic analysis based on Discourse Representation Theory. These steps are performed using statistical models trained in a semi-supervised manner. The employed annotation models are all language-neutral. Our first results are promising."
A nonparametric copula approach to conditional Value-at-Risk,"  Value-at-Risk and its conditional allegory, which takes into account the available information about the economic environment, form the centrepiece of the Basel framework for the evaluation of market risk in the banking sector. In this paper, a new nonparametric framework for estimating this conditional Value-at-Risk is presented. A nonparametric approach is particularly pertinent as the traditionally used parametric distributions have been shown to be insufficiently robust and flexible in most of the equity-return data sets observed in practice. The method extracts the quantile of the conditional distribution of interest, whose estimation is based on a novel estimator of the density of the copula describing the dynamic dependence observed in the series of returns. Real-world back-testing analyses demonstrate the potential of the approach, whose performance may be superior to its industry counterparts. "
Towards High-quality Visualization of Superfluid Vortices,"  Superfluidity is a special state of matter exhibiting macroscopic quantum phenomena and acting like a fluid with zero viscosity. In such a state, superfluid vortices exist as phase singularities of the model equation with unique distributions. This paper presents novel techniques to aid the visual understanding of superfluid vortices based on the state-of-the-art non-linear Klein-Gordon equation, which evolves a complex scalar field, giving rise to special vortex lattice/ring structures with dynamic vortex formation, reconnection, and Kelvin waves, etc. By formulating a numerical model with theoretical physicists in superfluid research, we obtain high-quality superfluid flow data sets without noise-like waves, suitable for vortex visualization. By further exploring superfluid vortex properties, we develop a new vortex identification and visualization method: a novel mechanism with velocity circulation to overcome phase singularity and an orthogonal-plane strategy to avoid ambiguity. Hence, our visualizations can help reveal various superfluid vortex structures and enable domain experts for related visual analysis, such as the steady vortex lattice/ring structures, dynamic vortex string interactions with reconnections and energy radiations, where the famous Kelvin waves and decaying vortex tangle were clearly observed. These visualizations have assisted physicists to verify the superfluid model, and further explore its dynamic behavior more intuitively. "
Nonrepetitive edge-colorings of trees,"A repetition is a sequence of symbols in which the first half is the same as the second half. An edge-coloring of a graph is repetition-free or nonrepetitive if there is no path with a color pattern that is a repetition. The minimum number of colors so that a graph has a nonrepetitive edge-coloring is called its Thue edge-chromatic number. We improve on the best known general upper bound of $4\Delta-4$ for the Thue edge-chromatic number of trees of maximum degree $\Delta$ due to Alon, Grytczuk, Ha{\l}uszczak and Riordan (2002) by providing a simple nonrepetitive edge-coloring with $3\Delta-2$ colors."
On the new wave behavior to the longitudinal wave quation in a magneto-electro-elastic circular rod,"With the aid of the symbolic computations software; Wolfram Mathematica 9, the powerful sine-Gordon expansion method is used in examining the analytical solution of the longitudinal wave equation in a magneto-electro-elastic circular rod. Sine-Gordon expansion method is based on the well-known sine-Gordon equation and a wave transformation. The longitudinal wave equation is an equation that arises in mathematical physics with dispersion caused by the transverse Poisson's effect in a magneto-electro-elastic circular rod. We successfully get some solutions with the complex, trigonometric and hyperbolic function structure. We present the numerical simulations of all the obtained solutions by choosing appropriate values of the parameters. We give the physical meanings of some of the obtained analytical solutions which significantly explain some practical physical problems."
Formulation of Deep Reinforcement Learning Architecture Toward Autonomous Driving for On-Ramp Merge,"  Multiple automakers have in development or in production automated driving systems (ADS) that offer freeway-pilot functions. This type of ADS is typically limited to restricted-access freeways only, that is, the transition from manual to automated modes takes place only after the ramp merging process is completed manually. One major challenge to extend the automation to ramp merging is that the automated vehicle needs to incorporate and optimize long-term objectives (e.g. successful and smooth merge) when near-term actions must be safely executed. Moreover, the merging process involves interactions with other vehicles whose behaviors are sometimes hard to predict but may influence the merging vehicle optimal actions. To tackle such a complicated control problem, we propose to apply Deep Reinforcement Learning (DRL) techniques for finding an optimal driving policy by maximizing the long-term reward in an interactive environment. Specifically, we apply a Long Short-Term Memory (LSTM) architecture to model the interactive environment, from which an internal state containing historical driving information is conveyed to a Deep Q-Network (DQN). The DQN is used to approximate the Q-function, which takes the internal state as input and generates Q-values as output for action selection. With this DRL architecture, the historical impact of interactive environment on the long-term reward can be captured and taken into account for deciding the optimal control policy. The proposed architecture has the potential to be extended and applied to other autonomous driving scenarios such as driving through a complex intersection or changing lanes under varying traffic flow conditions. "
CDS Rate Construction Methods by Machine Learning Techniques,"Regulators require financial institutions to estimate counterparty default risks from liquid CDS quotes for the valuation and risk management of OTC derivatives. However, the vast majority of counterparties do not have liquid CDS quotes and need proxy CDS rates. Existing methods cannot account for counterparty-specific default risks; we propose to construct proxy CDS rates by associating to illiquid counterparty liquid CDS Proxy based on Machine Learning Techniques. After testing 156 classifiers from 8 most popular classifier families, we found that some classifiers achieve highly satisfactory accuracy rates. Furthermore, we have rank-ordered the performances and investigated performance variations amongst and within the 8 classifier families. This paper is, to the best of our knowledge, the first systematic study of CDS Proxy construction by Machine Learning techniques, and the first systematic classifier comparison study based entirely on financial market data. Its findings both confirm and contrast existing classifier performance literature. Given the typically highly correlated nature of financial data, we investigated the impact of correlation on classifier performance. The techniques used in this paper should be of interest for financial institutions seeking a CDS Proxy method, and can serve for proxy construction for other financial variables. Some directions for future research are indicated."
Eigentriads and Eigenprogressions on the Tonnetz,"  We introduce a new multidimensional representation, named eigenprogression transform, that characterizes some essential patterns of Western tonal harmony while being equivariant to time shifts and pitch transpositions. This representation is deep, multiscale, and convolutional in the piano-roll domain, yet incurs no prior training, and is thus suited to both supervised and unsupervised MIR tasks. The eigenprogression transform combines ideas from the spiral scattering transform, spectral graph theory, and wavelet shrinkage denoising. We report state-of-the-art results on a task of supervised composer recognition (Haydn vs. Mozart) from polyphonic music pieces in MIDI format. "
A generalization of the Log Lindley distribution -- its properties and applications,"An extension of the two-parameter Log-Lindley distribution of Gomez et al. (2014) with support in (0, 1) is proposed. Its important properties like cumulative distribution function, moments, survival function, hazard rate function, Shannon entropy, stochastic n ordering and convexity (concavity) conditions are derived. An application in distorted premium principal is outlined and parameter estimation by method of maximum likelihood is also presented. We also consider use of a re-parameterized form of the proposed distribution in regression modeling for bounded responses by considering a modeling of real life data in comparison with beta regression and log-Lindley regression models."
A Flux Conserving Meshfree Method for Conservation Laws,"  Lack of conservation has been the biggest drawback in meshfree generalized finite difference methods (GFDMs). In this paper, we present a novel modification of classical meshfree GFDMs to include local balances which produce an approximate conservation of numerical fluxes. This numerical flux conservation is done within the usual moving least squares framework. Unlike Finite Volume Methods, it is based on locally defined control cells, rather than a globally defined mesh. We present the application of this method to an advection diffusion equation and the incompressible Navier - Stokes equations. Our simulations show that the introduction of flux conservation significantly reduces the errors in conservation in meshfree GFDMs. "
Anomaly detection in the dynamics of web and social networks,"  In this work, we propose a new, fast and scalable method for anomaly detection in large time-evolving graphs. It may be a static graph with dynamic node attributes (e.g. time-series), or a graph evolving in time, such as a temporal network. We define an anomaly as a localized increase in temporal activity in a cluster of nodes. The algorithm is unsupervised. It is able to detect and track anomalous activity in a dynamic network despite the noise from multiple interfering sources. We use the Hopfield network model of memory to combine the graph and time information. We show that anomalies can be spotted with a good precision using a memory network. The presented approach is scalable and we provide a distributed implementation of the algorithm. To demonstrate its efficiency, we apply it to two datasets: Enron Email dataset and Wikipedia page views. We show that the anomalous spikes are triggered by the real-world events that impact the network dynamics. Besides, the structure of the clusters and the analysis of the time evolution associated with the detected events reveals interesting facts on how humans interact, exchange and search for information, opening the door to new quantitative studies on collective and social behavior on large and dynamic datasets. "
Local coefficients and the Herbert Formula,"  We discuss a generalisation of the Herbert formula for double points, when the normal bundle of an immersion admits an additional structure, and an application. "
Emission line ratios of Fe III as astrophysical plasma diagnostics,"Recent state-of-the-art calculations of A-values and electron impact excitation rates for Fe III are used in conjunction with the Cloudy modeling code to derive emission line intensity ratios for optical transitions among the fine-structure levels of the 3d$^6$ configuration. A comparison of these with high resolution, high signal-to-noise spectra of gaseous nebulae reveals that previous discrepancies found between theory and observation are not fully resolved by the latest atomic data. Blending is ruled out as a likely cause of the discrepancies, because temperature- and density-independent ratios (arising from lines with common upper levels) match well with those predicted by theory. For a typical nebular plasma with electron temperature $T_{\rm e} = 9000$ K and electron density $\rm N_{e}=10^4 \, cm^{-3}$, cascading of electrons from the levels $\rm ^3G_5$, $\rm ^3G_4$ and $\rm ^3G_3$ plays an important role in determining the populations of lower levels, such as $\rm ^3F_4$, which provide the density diagnostic emission lines of Fe III, such as $\rm ^5D_4$ - $\rm ^3F_4$ at 4658 \AA. Hence further work on the A-values for these transitions is recommended, ideally including measurements if possible. However, some Fe III ratios do provide reliable $N_{\rm e}$-diagnostics, such as 4986/4658. The Fe III cooling function calculated with Cloudy using the most recent atomic data is found to be significantly greater at $T_e$ $\simeq$ 30000 K than predicted with the existing Cloudy model. This is due to the presence of additional emission lines with the new data, particularly in the 1000--4000 \AA\ wavelength region."
"Warped cones, (non-)rigidity, and piecewise properties, with a joint appendix with Dawid Kielak","  We prove that if a quasi-isometry of warped cones is induced by a map between the base spaces of the cones, the actions must be conjugate by this map. The converse is false in general, conjugacy of actions is not sufficient for quasi-isometry of the respective warped cones. For a general quasi-isometry of warped cones, using the asymptotically faithful covering constructed in a previous work with Jianchao Wu, we deduce that the two groups are quasi-isometric after taking Cartesian products with suitable powers of the integers. Secondly, we characterise geometric properties of a group (coarse embeddability into Banach spaces, asymptotic dimension, property A) by properties of the warped cone over an action of this group. These results apply to arbitrary asymptotically faithful coverings, in particular to box spaces. As an application, we calculate the asymptotic dimension of a warped cone, improve bounds by Szabó, Wu, and Zacharias and by Bartels on the amenability dimension of actions of virtually nilpotent groups, and give a partial answer to a question of Willett about dynamic asymptotic dimension. In the appendix, we justify optimality of the aforementioned result on general quasi-isometries by showing that quasi-isometric warped cones need not come from quasi-isometric groups, contrary to the case of box spaces. "
Enhancing MapReduce Fault Recovery Through Binocular Speculation,"  MapReduce speculation plays an important role in finding potential task stragglers and failures. But a tacit dichotomy exists in MapReduce due to its inherent two-phase (map and reduce) management scheme in which map tasks and reduce tasks have distinctly different execution behaviors, yet reduce tasks are dependent on the results of map tasks. We reveal that speculation policies for fault handling in MapReduce do not recognize this dichotomy between map and reduce tasks, which leads to an issue of speculation myopia for MapReduce fault recovery. These issues cause significant performance degradation upon network and node failures. To address the speculation myopia caused by MapReduce dichotomy, we introduce a new scheme called binocular speculation to help MapReduce increase its assessment scope for speculation. As part of the scheme, we also design three component techniques including neighborhood glance, collective speculation and speculative rollback. Our evaluation shows that, with these techniques, binocular speculation can increase the coordination of map and reduce phases, and enhance the efficiency of MapReduce fault recovery. "
Generalized classes of continuous symmetries in two-mode Dicke models,"As recently realized experimentally [Léonard et al., Nature 543, 87 (2017)], one can engineer models with continuous symmetries by coupling two cavity modes to trapped atoms, via a Raman pumping geometry. Considering specifically cases where internal states of the atoms couple to the cavity, we show an extended range of parameters for which continuous symmetry breaking can occur, and we classify the distinct steady states and time-dependent states that arise for different points in this extended parameter regime."
The strong Prikry property,"  I isolate a combinatorial property of a poset $\mathbb{P}$ that I call the strong Prikry property, which implies the existence of an ultrafilter on the complete Boolean algebra $\mathbb{B}$ of $\mathbb{P}$ such that one inclusion of the Boolean ultrapower version of the so-called \Bukovsky-Dehornoy phenomenon holds with respect to $\mathbb{B}$ and $U$. I show that in all cases that were previously studied, and for which it was shown that they come with a canonical iterated ultrapower construction whose limit can be described as a single Boolean ultrapower, the posets in question satisfy this property: Prikry forcing, Magidor forcing and generalized Prikry forcing. "
Accelerated Optimization in the PDE Framework: Formulations for the Manifold of Diffeomorphisms,"  We consider the problem of optimization of cost functionals on the infinite-dimensional manifold of diffeomorphisms. We present a new class of optimization methods, valid for any optimization problem setup on the space of diffeomorphisms by generalizing Nesterov accelerated optimization to the manifold of diffeomorphisms. While our framework is general for infinite dimensional manifolds, we specifically treat the case of diffeomorphisms, motivated by optical flow problems in computer vision. This is accomplished by building on a recent variational approach to a general class of accelerated optimization methods by Wibisono, Wilson and Jordan, which applies in finite dimensions. We generalize that approach to infinite dimensional manifolds. We derive the surprisingly simple continuum evolution equations, which are partial differential equations, for accelerated gradient descent, and relate it to simple mechanical principles from fluid mechanics. Our approach has natural connections to the optimal mass transport problem. This is because one can think of our approach as an evolution of an infinite number of particles endowed with mass (represented with a mass density) that moves in an energy landscape. The mass evolves with the optimization variable, and endows the particles with dynamics. This is different than the finite dimensional case where only a single particle moves and hence the dynamics does not depend on the mass. We derive the theory, compute the PDEs for accelerated optimization, and illustrate the behavior of these new accelerated optimization schemes. "
Learning Generalizable Robot Skills from Demonstrations in Cluttered Environments,"Learning from Demonstration (LfD) is a popular approach to endowing robots with skills without having to program them by hand. Typically, LfD relies on human demonstrations in clutter-free environments. This prevents the demonstrations from being affected by irrelevant objects, whose influence can obfuscate the true intention of the human or the constraints of the desired skill. However, it is unrealistic to assume that the robot's environment can always be restructured to remove clutter when capturing human demonstrations. To contend with this problem, we develop an importance weighted batch and incremental skill learning approach, building on a recent inference-based technique for skill representation and reproduction. Our approach reduces unwanted environmental influences on the learned skill, while still capturing the salient human behavior. We provide both batch and incremental versions of our approach and validate our algorithms on a 7-DOF JACO2 manipulator with reaching and placing skills."
Women are slightly more cooperative than men (in one-shot Prisoner's dilemma games played online),"Differences between men and women have intrigued generations of social scientists, who have found that the two sexes behave differently in settings requiring competition, risk taking, altruism, honesty, as well as many others. Yet, little is known about whether there are gender differences in cooperative behavior. Previous evidence is mixed and inconclusive. Here I shed light on this topic by analyzing the totality of studies that my research group has conducted since 2013. This is a dataset of 10,951 observations coming from 7,322 men and women living in the US, recruited through Amazon Mechanical Turk, and who passed four comprehension questions to make sure they understand the cooperation problem (a one-shot prisoner's dilemma). The analysis demonstrates that women are more cooperative than men. The effect size is small (about 4 percentage points, and this might explain why previous studies failed to detect it) but highly significant (p<.0001)."
Relaxation of Radiation-Driven Two-Level Systems Interacting with a Bose-Einstein Condensate Bath,"  We develop a microscopic theory for the relaxation dynamics of an optically pumped two-level system (TLS) coupled to a bath of weakly interacting Bose gas. Using Keldysh formalism and diagrammatic perturbation theory, expressions for the relaxation times of the TLS Rabi oscillations are derived when the boson bath is in the normal state and the Bose-Einstein condensate (BEC) state. We apply our general theory to consider an irradiated quantum dot coupled with a boson bath consisting of a two-dimensional dipolar exciton gas. When the bath is in the BEC regime, relaxation of the Rabi oscillations is due to both condensate and non-condensate fractions of the bath bosons for weak TLS-light coupling and dominantly due to the non-condensate fraction for strong TLS-light coupling. Our theory also shows that a phase transition of the bath from the normal to the BEC state strongly influences the relaxation rate of the TLS Rabi oscillations. The TLS relaxation rate is approximately independent of the pump field frequency and monotonically dependent on the field strength when the bath is in the low-temperature regime of the normal phase. Phase transition of the dipolar exciton gas leads to a non-monotonic dependence of the TLS relaxation rate on both the pump field frequency and field strength, providing a characteristic signature for the detection of BEC phase transition of the coupled dipolar exciton gas. "
On the approximation of convex bodies by ellipses with respect to the symmetric difference metric,"Given a centrally symmetric convex body $K \subset \mathbb{R}^d$ and a positive number $\lambda$, we consider, among all ellipsoids $E \subset \mathbb{R}^d$ of volume $\lambda$, those that best approximate $K$ with respect to the symmetric difference metric, or equivalently that maximize the volume of $E\cap K$: these are the maximal intersection (MI) ellipsoids introduced by Artstein-Avidan and Katzin. The question of uniqueness of MI ellipsoids (under the obviously necessary assumption that $\lambda$ is between the volumes of the John and the Loewner ellipsoids of $K$) is open in general. We provide a positive answer to this question in dimension $d=2$. Therefore we obtain a continuous $1$-parameter family of ellipses interpolating between the John and the Loewner ellipses of $K$. In order to prove uniqueness, we show that the area $I_K(E)$ of the intersection $K \cap E$ is a strictly quasiconcave function of the ellipse $E$, with respect to the natural affine structure on the set of ellipses of area $\lambda$. The proof relies on smoothening $K$, putting it in general position, and obtaining uniform estimates for certain derivatives of the function $I_K(.)$. Finally, we provide a characterization of maximal intersection positions, that is, the situation where the MI ellipse of $K$ is the unit disk, under the assumption that the two boundaries are transverse."
POLAMI: Polarimetric Monitoring of Active Galactic Nuclei at Millimetre Wavelengths. II. Widespread circular polarisation,"We analyse the circular polarisation data accumulated in the first 7 years of the POLAMI project introduced in an accompanying paper (Agudo et al.). In the 3mm wavelength band, we acquired more than 2600 observations, and all but one of our 37 sample sources were detected, most of them several times. For most sources, the observed distribution of the degree of circular polarisation is broader than that of unpolarised calibrators, indicating that weak (<0.5%) circular polarisation is present most of the time. Our detection rate and the maximum degree of polarisation found, 2.0%, are comparable to previous surveys, all made at much longer wavelengths. We argue that the process generating circular polarisation must not be strongly wavelength dependent, and we propose that the widespread presence of circular polarisation in our short wavelength sample dominated by blazars is mostly due to Faraday conversion of the linearly polarised synchrotron radiation in the helical magnetic field of the jet. Circular polarisation is variable, most notably on time scales comparable to or shorter than our median sampling interval <1 month. Longer time scales of about one year are occasionally detected, but severely limited by the weakness of the signal. At variance with some longer wavelength investigations we find that the sign of circular polarisation changes in most sources, while only 7 sources, including 3 already known, have a strong preference for one sign. The degrees of circular and linear polarisation do not show any systematic correlation. We do find however one particular event where the two polarisation degrees vary in synchronism during a time span of 0.9 years. The paper also describes a novel method for calibrating the sign of circular polarisation observations."
Single Classifier-based Passive System for Source Printer Classification using Local Texture Features,"An important aspect of examining printed documents for potential forgeries and copyright infringement is the identification of source printer as it can be helpful for ascertaining the leak and detecting forged documents. This paper proposes a system for classification of source printer from scanned images of printed documents using all the printed letters simultaneously. This system uses local texture patterns based features and a single classifier for classifying all the printed letters. Letters are extracted from scanned images using connected component analysis followed by morphological filtering without the need of using an OCR. Each letter is sub-divided into a flat region and an edge region, and local tetra patterns are estimated separately for these two regions. A strategically constructed pooling technique is used to extract the final feature vectors. The proposed method has been tested on both a publicly available dataset of 10 printers and a new dataset of 18 printers scanned at a resolution of 600 dpi as well as 300 dpi printed in four different fonts. The results indicate shape independence property in the proposed method as using a single classifier it outperforms existing handcrafted feature-based methods and needs much smaller number of training pages by using all the printed letters."
Heat spreader with parallel microchannel configurations employing nanofluids for near active cooling of MEMS,"  While parallel microchannel based cooling systems have been around for quite a period of time, employing the same and incorporating them for near active cooling of microelectronic devices is yet to be implemented and the implications of the same on thermal mitigation to be understood. The present article focusses on a specific design of the PMCS such that it can be implemented at ease on the heat spreader of a modern microprocessor to obtain near active cooling. Extensive experimental and numerical studies have been carried out to comprehend the same and three different flow configurations of PMCS have been adopted for the present investigations. Additional to focussing on the thermofluidics due to flow configuration, nanofluids have also been employed to achieve the desired essentials of mitigation of overshoot temperatures and improving uniformity of cooling. Two modelling methods, Discrete Phase Modelling and Effective Property Modelling have been employed for numerical study to model nanofluids as working fluid in micro flow paths and the DPM predictions have been observed to match accurately with experiments. To quantify the thermal performance of PMCS, an appropriate Figure of Merit has been proposed. From the FoM It has been perceived that the Z configuration employing nanofluid is the best suitable solutions for uniform thermal loads to achieve uniform cooling as well as reducing maximum temperature produced with in the device. The present results are very promising and viable approach for futuristic thermal mitigation of microprocessor systems. "
A review of asymptotic theory of estimating functions,"  Asymptotic statistical theory for estimating functions is reviewed in a generality suitable for stochastic processes. Conditions concerning existence of a consistent estimator, uniqueness, rate of convergence, and the asymptotic distribution are treated separately. Our conditions are not minimal, but can be verified for many interesting stochastic process models. Several examples illustrate the wide applicability of the theory and why the generality is needed. "
Multiview Deep Learning for Predicting Twitter Users' Location,"  The problem of predicting the location of users on large social networks like Twitter has emerged from real-life applications such as social unrest detection and online marketing. Twitter user geolocation is a difficult and active research topic with a vast literature. Most of the proposed methods follow either a content-based or a network-based approach. The former exploits user-generated content while the latter utilizes the connection or interaction between Twitter users. In this paper, we introduce a novel method combining the strength of both approaches. Concretely, we propose a multi-entry neural network architecture named MENET leveraging the advances in deep learning and multiview learning. The generalizability of MENET enables the integration of multiple data representations. In the context of Twitter user geolocation, we realize MENET with textual, network, and metadata features. Considering the natural distribution of Twitter users across the concerned geographical area, we subdivide the surface of the earth into multi-scale cells and train MENET with the labels of the cells. We show that our method outperforms the state of the art by a large margin on three benchmark datasets. "
"Designing the Optimal Bit: Balancing Energetic Cost, Speed and Reliability","  We consider the technologically relevant costs of operating a reliable bit that can be erased rapidly. We find that both erasing and reliability times are non-monotonic in the underlying friction, leading to a trade-off between erasing speed and bit reliability. Fast erasure is possible at the expense of low reliability at moderate friction, and high reliability comes at the expense of slow erasure in the underdamped and overdamped limits. Within a given class of bit parameters and control strategies, we define ""optimal"" designs of bits that meet the desired reliability and erasing time requirements with the lowest operational work cost. We find that optimal designs always saturate the bound on the erasing time requirement, but can exceed the required reliability time if critically damped. The non-trivial geometry of the reliability and erasing time-scales allows us to exclude large regions of parameter space as sub-optimal. We find that optimal designs are either critically damped or close to critical damping under the erasing procedure. "
Economic Factors of Vulnerability Trade and Exploitation,"  Cybercrime markets support the development and diffusion of new attack technologies, vulnerability exploits, and malware. Whereas the revenue streams of cyber attackers have been studied multiple times in the literature, no quantitative account currently exists on the economics of attack acquisition and deployment. Yet, this understanding is critical to characterize the production of (traded) exploits, the economy that drives it, and its effects on the overall attack scenario. In this paper we provide an empirical investigation of the economics of vulnerability exploitation, and the effects of market factors on likelihood of exploit. Our data is collected first-handedly from a prominent Russian cybercrime market where the trading of the most active attack tools reported by the security industry happens. Our findings reveal that exploits in the underground are priced similarly or above vulnerabilities in legitimate bug-hunting programs, and that the refresh cycle of exploits is slower than currently often assumed. On the other hand, cybercriminals are becoming faster at introducing selected vulnerabilities, and the market is in clear expansion both in terms of players, traded exploits, and exploit pricing. We then evaluate the effects of these market variables on likelihood of attack realization, and find strong evidence of the correlation between market activity and exploit deployment. We discuss implications on vulnerability metrics, economics, and exploit measurement. "
Complete classification of generalized crossing changes between GOF-knots,"  We show that the monodromy for a genus one, fibered knot can have at most two monodromy equivalence classes of once-unclean arcs. We use this to classify all monodromies of genus one, fibered knots that possess once-unclean arcs, all manifolds containing genus one fibered knots with generalized crossing changes resulting in another genus one fibered knot, and all generalized crossing changes between two genus one, fibered knots. "
Strain Mode of General Flow: Characterization and Implications for Flow Pattern Structures,"  Understanding the mixing capability of mixing devices based on their geometric shape is an important issue both for predicting mixing processes and for designing new mixers. The flow patterns in mixers are directly connected with the modes of the local strain rate, which is generally a combination of elongational flow and planar shear flow. We develop a measure to characterize the modes of the strain rate for general flow occurring in mixers. The spatial distribution of the volumetric strain rate (or non-planar strain rate) in connection with the flow pattern plays an essential role in understanding distributive mixing. With our measure, flows with different types of screw elements in a twin-screw extruder are numerically analyzed. The difference in flow pattern structure between conveying screws and kneading disks is successfully characterized by the distribution of the volumetric strain rate. The results suggest that the distribution of the strain rate mode offers an essential and convenient way for characterization of the relation between flow pattern structure and the mixer geometry. "
$\mathbb{Z}^2$-algebras as noncommutative blow-ups,"The goal of this note is to first prove that for a well behaved $\mathbb{Z}^2$-algebra $R$, the category $QGr(R) := Gr(R)/Tors(R)$ is equivalent to $QGr(R_\Delta)$ where $R_\Delta$ is a diagonal-like sub-$\mathbb{Z}$-algebra of $R$. Afterwards we use this result to prove that the $\mathbb{Z}^2$-algebras as introduced in [arXiv:1607.08383] are QGr-equivalent to a diagonal-like sub-$\mathbb{Z}$-algebra which is a simultaneous noncommutative blow-up of a quadratic and a cubic Sklyanin algebra. As such we link the noncommutative birational transformation and the associated $\mathbb{Z}^2$-algebras as appearing in the work of Van den Bergh and Presotto with the noncommutative blowups appearing in the work of Rogalski, Sierra and Stafford."
Deep Convolutional Neural Networks for Anomaly Event Classification on Distributed Systems,"The increasing popularity of server usage has brought a plenty of anomaly log events, which have threatened a vast collection of machines. Recognizing and categorizing the anomalous events thereby is a much salient work for our systems, especially the ones generate the massive amount of data and harness it for technology value creation and business development. To assist in focusing on the classification and the prediction of anomaly events, and gaining critical insights from system event records, we propose a novel log preprocessing method which is very effective to filter abundant information and retain critical characteristics. Additionally, a competitive approach for automated classification of anomalous events detected from the distributed system logs with the state-of-the-art deep (Convolutional Neural Network) architectures is proposed in this paper. We measure a series of deep CNN algorithms with varied hyper-parameter combinations by using standard evaluation metrics, the results of our study reveals the advantages and potential capabilities of the proposed deep CNN models for anomaly event classification tasks on real-world systems. The optimal classification precision of our approach is 98.14%, which surpasses the popular traditional machine learning methods."
Group-sparse block PCA and explained variance,"  The paper addresses the simultneous determination of goup-sparse loadings by block optimization, and the correlated problem of defining explained variance for a set of non orthogonal components. We give in both cases a comprehensive mathematical presentation of the problem, which leads to propose i) a new formulation/algorithm for group-sparse block PCA and ii) a framework for the definition of explained variance with the analysis of five definitions. The numerical results i) confirm the superiority of block optimization over deflation for the determination of group-sparse loadings, and the importance of group information when available, and ii) show that ranking of algorithms according to explained variance is essentially independant of the definition of explained variance. These results lead to propose a new optimal variance as the definition of choice for explained variance. "
Multi-Frequency Phase Synchronization,"  We propose a novel formulation for phase synchronization -- the statistical problem of jointly estimating alignment angles from noisy pairwise comparisons -- as a nonconvex optimization problem that enforces consistency among the pairwise comparisons in multiple frequency channels. Inspired by harmonic retrieval in signal processing, we develop a simple yet efficient two-stage algorithm that leverages the multi-frequency information. We demonstrate in theory and practice that the proposed algorithm significantly outperforms state-of-the-art phase synchronization algorithms, at a mild computational costs incurred by using the extra frequency channels. We also extend our algorithmic framework to general synchronization problems over compact Lie groups. "
A Bennequin-type inequality and combinatorial bounds,"  In this paper we provide a new Bennequin-type inequality for the Rasmussen- Beliakova-Wehrli invariant, featuring the numerical transverse braid invariants (the c-invariants) introduced by the author. From the Bennequin type-inequality, and a combinatorial bound on the value of the c-invariants, we deduce a new computable bound on the Rasmussen invariant. "
Thermal field theory of bosonic gases with finite-range effective interaction,"  We study a dilute and ultracold Bose gas of interacting atoms by using an effective field theory which takes account finite-range effects of the inter-atomic potential. Within the formalism of functional integration from the grand canonical partition function we derive beyond-mean-field analytical results which depend on both scattering length and effective range of the interaction. In particular, we calculate the equation of state of the bosonic system as a function of these interaction parameters both at zero and finite temperature including one-loop Gaussian fluctuation. In the case of zero-range effective interaction we explicitly show that, due to quantum fluctuations, the bosonic system is thermodynamically stable only for very small values of the gas parameter. We find that a positive effective range above a critical threshold is necessary to remove the thermodynamical instability of the uniform configuration. Remarkably, also for relatively large values of the gas parameter, our finite-range results are in quite good agreement with recent zero-temperature Monte Carlo calculations obtained with hard-sphere bosons. "
LtFi: Cross-technology Communication for RRM between LTE-U and IEEE 802.11,"Cross-technology communication (CTC) was proposed in recent literature as a way to exploit the opportunities of collaboration between heterogeneous wireless technologies. This paper presents LtFi, a system which enables to set-up a CTC between nodes of co-located LTE-U and WiFi networks. LtFi follows a two-step approach: using the air-interface LTE-U BSs are broadcasting connection and identification data to adjacent WiFi nodes, which is used to create a bi-directional control channel over the wired Internet. This way LtFi enables the development of advanced cross-technology interference and radio resource management schemes between heterogeneous WiFi and LTE-U networks. LtFi is of low complexity and fully compliant with LTE-U technology and works on WiFi side with COTS hardware. It was prototypically implemented and evaluated. Experimental results reveal that LtFi is able to reliably decoded the data transmitted over the LtFi air-interface in a crowded wireless environment at even very low LTE-U receive power levels of -92dBm. Moreover, results from system-level simulations show that LtFi is able to accurately estimate the set of interfering LTE-U BSs in a typical LTE-U multi-cell environment."
Multi-step Off-policy Learning Without Importance Sampling Ratios,"  To estimate the value functions of policies from exploratory data, most model-free off-policy algorithms rely on importance sampling, where the use of importance sampling ratios often leads to estimates with severe variance. It is thus desirable to learn off-policy without using the ratios. However, such an algorithm does not exist for multi-step learning with function approximation. In this paper, we introduce the first such algorithm based on temporal-difference (TD) learning updates. We show that an explicit use of importance sampling ratios can be eliminated by varying the amount of bootstrapping in TD updates in an action-dependent manner. Our new algorithm achieves stability using a two-timescale gradient-based TD update. A prior algorithm based on lookup table representation called Tree Backup can also be retrieved using action-dependent bootstrapping, becoming a special case of our algorithm. In two challenging off-policy tasks, we demonstrate that our algorithm is stable, effectively avoids the large variance issue, and can perform substantially better than its state-of-the-art counterpart. "
Atomic Ferris wheel beams,"  We study the generation of atom vortex beams in the case where an atomic wave-packet, moving in free space, is diffracted from a properly tailored light mask with a spiral transverse profile. We show how such a diffraction scheme could lead to the production of an atomic Ferris wheel beam. "
On the Limits of Learning Representations with Label-Based Supervision,"  Advances in neural network based classifiers have transformed automatic feature learning from a pipe dream of stronger AI to a routine and expected property of practical systems. Since the emergence of AlexNet every winning submission of the ImageNet challenge has employed end-to-end representation learning, and due to the utility of good representations for transfer learning, representation learning has become as an important and distinct task from supervised learning. At present, this distinction is inconsequential, as supervised methods are state-of-the-art in learning transferable representations. But recent work has shown that generative models can also be powerful agents of representation learning. Will the representations learned from these generative methods ever rival the quality of those from their supervised competitors? In this work, we argue in the affirmative, that from an information theoretic perspective, generative models have greater potential for representation learning. Based on several experimentally validated assumptions, we show that supervised learning is upper bounded in its capacity for representation learning in ways that certain generative models, such as Generative Adversarial Networks (GANs) are not. We hope that our analysis will provide a rigorous motivation for further exploration of generative representation learning. "
Coset space construction and inverse Higgs phenomenon for the conformal group,"  It is shown that conformally invariant theories can be obtained within the framework of the coset space construction. The corresponding technique is applicable for the construction of representations of the unbroken conformal group, as well as of a spontaneously broken one. A special role of the ""Nambu-Goldstone fields"" for special conformal transformations is clarified - they ensure self-consistency of a theory by guaranteeing that discrete symmetries are indeed symmetries of the theory. A generalization of the developed construction to a special class of symmetry groups with a non-linear realization of its discrete elements is given. Based on these results, the usage of the inverse Higgs constraints for the conformal group undergoing spontaneous symmetry breaking is questioned. "
"A spatial predictive model for Malaria resurgence in central Greece integrating entomological, environmental and Social data","Malaria constitutes an important cause of human mortality. After 2009 Greece experienced a resurgence of malaria. Here, we develop a modelbased framework that integrates entomological, geographical, social and environmental evidence in order to guide the mosquito control efforts and apply this framework to data from an entomological survey study conducted in Central Greece. Our results indicate that malaria transmission risk in Greece is potentially substantial. In addition, specific districts such as seaside, lakeside and rice field regions appear to represent potential malaria hotspots in Central Greece. We found that appropriate maps depicting the basic reproduction number, R0 , are useful tools for informing policy makers on the risk of malaria resurgence and can serve as a guide to inform recommendations regarding control measures."
Large-scale analysis of user exposure to online advertising in Facebook,"Online advertising is the major source of income for a large portion of Internet Services. There exists a body of literature aiming at optimizing ads engagement, understanding the privacy and ethical implications of online advertising, etc. However, to the best of our knowledge, no previous work analyses at large scale the exposure of real users to online advertising. This paper performs a comprehensive analysis of the exposure of users to ads and advertisers using a dataset including more than 7M ads from 140K unique advertisers delivered to more than 5K users that was collected between October 2016 and May 2018. The study focuses on Facebook, which is the second largest advertising platform only to Google in terms of revenue, and accounts for more than 2.2B monthly active users. Our analysis reveals that Facebook users are exposed (in median) to 70 ads per week, which come from 12 advertisers. Ads represent between 10% and 15% of all the information received in users' newsfeed. A small increment of 1% in the portion of ads in the newsfeed could roughly represent a revenue increase of 8.17M USD per week for Facebook. Finally, we also reveal that Facebook users are overprofiled since in the best case only 22.76% of the interests Facebook assigns to users for advertising purpose are actually related to the ads those users receive."
Concept Formation and Dynamics of Repeated Inference in Deep Generative Models,"  Deep generative models are reported to be useful in broad applications including image generation. Repeated inference between data space and latent space in these models can denoise cluttered images and improve the quality of inferred results. However, previous studies only qualitatively evaluated image outputs in data space, and the mechanism behind the inference has not been investigated. The purpose of the current study is to numerically analyze changes in activity patterns of neurons in the latent space of a deep generative model called a ""variational auto-encoder"" (VAE). What kinds of inference dynamics the VAE demonstrates when noise is added to the input data are identified. The VAE embeds a dataset with clear cluster structures in the latent space and the center of each cluster of multiple correlated data points (memories) is referred as the concept. Our study demonstrated that transient dynamics of inference first approaches a concept, and then moves close to a memory. Moreover, the VAE revealed that the inference dynamics approaches a more abstract concept to the extent that the uncertainty of input data increases due to noise. It was demonstrated that by increasing the number of the latent variables, the trend of the inference dynamics to approach a concept can be enhanced, and the generalization ability of the VAE can be improved. "
Lp-estimates for the square root of elliptic systems with mixed boundary conditions,"This article focuses on Lp-estimates for the square root of elliptic systems of second order in divergence form on a bounded domain. We treat complex bounded measurable coefficients and allow for mixed Dirichlet/Neumann boundary conditions on domains beyond the Lipschitz class. If there is an associated bounded semigroup on Lp0 , then we prove that the square root extends for all p $\in$ (p0, 2) to an isomorphism between a closed subspace of W1p carrying the boundary conditions and Lp. This result is sharp and extrapolates to exponents slightly above 2. As a byproduct, we obtain an optimal p-interval for the bounded H$\infty$-calculus on Lp. Estimates depend holomorphically on the coefficients, thereby making them applicable to questions of non-autonomous maximal regularity and optimal control. For completeness we also provide a short summary on the Kato square root problem in L2 for systems with lower order terms in our setting."
Ensemble of Thermostatically Controlled Loads: Statistical Physics Approach,"  Thermostatically Controlled Loads (TCL), e.g. air-conditioners and heaters, are by far the most wide-spread consumers of electricity. Normally the devices are calibrated to provide the so-called bang-bang control of temperature -- changing from on to off, and vice versa, depending on temperature. Aggregation of a large group of similar devices into a statistical ensemble is considered, where the devices operate following the same dynamics subject to stochastic perturbations and randomized, Poisson on/off switching policy. We analyze, using theoretical and computational tools of statistical physics, how the ensemble relaxes to a stationary distribution and establish relation between the relaxation and statistics of the probability flux, associated with devices' cycling in the mixed (discrete, switch on/off, and continuous, temperature) phase space. This allowed us to derive and analyze spectrum of the non-equilibrium (detailed balance broken) statistical system and uncover how switching policy affects oscillatory trend and speed of the relaxation. Relaxation of the ensemble is of a practical interest because it describes how the ensemble recovers from significant perturbations, e.g. forceful temporary switching off aimed at utilizing flexibility of the ensemble in providing ""demand response"" services relieving consumption temporarily to balance larger power grid. We discuss how the statistical analysis can guide further development of the emerging demand response technology. "
Automatic Document Image Binarization using Bayesian Optimization,"  Document image binarization is often a challenging task due to various forms of degradation. Although there exist several binarization techniques in literature, the binarized image is typically sensitive to control parameter settings of the employed technique. This paper presents an automatic document image binarization algorithm to segment the text from heavily degraded document images. The proposed technique uses a two band-pass filtering approach for background noise removal, and Bayesian optimization for automatic hyperparameter selection for optimal results. The effectiveness of the proposed binarization technique is empirically demonstrated on the Document Image Binarization Competition (DIBCO) and the Handwritten Document Image Binarization Competition (H-DIBCO) datasets. "
Vibrationally resolved electronic spectra including vibrational pre-excitation: Theory and application to VIPER spectroscopy,"Vibrationally resolved electronic absorption spectra including the effect of vibrational pre-excitation are computed in order to interpret and predict vibronic transitions that are probed in the Vibrationally Promoted Electronic Resonance (VIPER) experiment [L. J. G. W. van Wilderen et al., Angew. Chem. Int. Ed. 53, 2667 (2014)]. To this end, we employ time-independent and time-dependent methods based on the evaluation of Franck-Condon overlap integrals and Fourier transformation of time-domain wavepacket autocorrelation functions, respectively. The time-independent approach uses a generalized version of the FCclasses method [F. Santoro et al., J. Chem. Phys. 126, 084509 (2007)]. In the time-dependent approach, autocorrelation functions are obtained by wavepacket propagation and by evaluation of analytic expressions, within the harmonic approximation including Duschinsky rotation effects. For several medium-sized polyatomic systems, it is shown that selective pre-excitation of particular vibrational modes leads to a red-shift of the low-frequency edge of the electronic absorption spectrum, which is a prerequisite for the VIPER experiment. This effect is typically most pronounced upon excitation of ring distortion modes within an aromatic pi-system. Theoretical predictions as to which modes show the strongest VIPER effect are found to be in excellent agreement with experiment."
Modular Multi-Objective Deep Reinforcement Learning with Decision Values,"In this work we present a method for using Deep Q-Networks (DQNs) in multi-objective environments. Deep Q-Networks provide remarkable performance in single objective problems learning from high-level visual state representations. However, in many scenarios (e.g in robotics, games), the agent needs to pursue multiple objectives simultaneously. We propose an architecture in which separate DQNs are used to control the agent's behaviour with respect to particular objectives. In this architecture we introduce decision values to improve the scalarization of multiple DQNs into a single action. Our architecture enables the decomposition of the agent's behaviour into controllable and replaceable sub-behaviours learned by distinct modules. Moreover, it allows to change the priorities of particular objectives post-learning, while preserving the overall performance of the agent. To evaluate our solution we used a game-like simulator in which an agent - provided with high-level visual input - pursues multiple objectives in a 2D world."
Coupled conditional backward sampling particle filter,"  Unbiased estimation for hidden Markov models has been recently proposed by Jacob et al (to appear), using a coupling of two conditional particle filters (CPFs). Unbiased estimation has many advantages, such as enabling the construction of asymptotically exact confidence intervals and straightforward parallelisation. In this work we propose a new coupling of two CPFs, for unbiased estimation, that uses backward sampling steps, which is an important efficiency enhancing technique in particle filtering. We show that this coupled conditional backward sampling particle filter (CCBPF) algorithm has better stability properties, in the sense that with fixed number of particles, the coupling time in terms of iterations increases only linearly with respect to the time horizon under a general (strong mixing) condition. In contrast, current coupled CPFs require the particle number to increase with the horizon length. An important corollary of our results is a new quantitative bound for the convergence rate of the popular backward sampling conditional particle filter. Previous theoretical results have not been able to demonstrate the improvement brought by backward sampling to the CPF, whereas we provide rates showing that backward sampling ensures that the CPF can remain effective with a fixed number of particles independent of the time horizon. "
A Bayesian Evidence Synthesis Approach to Estimate Disease Prevalence in Hard-To-Reach Populations: Hepatitis C in New York City,"Existing methods to estimate the prevalence of chronic hepatitis C (HCV) in New York City (NYC) are limited in scope and fail to assess hard-to-reach subpopulations with highest risk such as injecting drug users (IDUs). To address these limitations, we employ a Bayesian multi-parameter evidence synthesis model to systematically combine multiple sources of data, account for bias in certain data sources, and provide unbiased HCV prevalence estimates with associated uncertainty. Our approach improves on previous estimates by explicitly accounting for injecting drug use and including data from high-risk subpopulations such as the incarcerated, and is more inclusive, utilizing ten NYC data sources. In addition, we derive two new equations to allow age at first injecting drug use data for former and current IDUs to be incorporated into the Bayesian evidence synthesis, a first for this type of model. Our estimated overall HCV prevalence as of 2012 among NYC adults aged 20-59 years is 2.78% (95% CI 2.61-2.94%), which represents between 124,900 and 140,000 chronic HCV cases. These estimates suggest that HCV prevalence in NYC is higher than previously indicated from household surveys (2.2%) and the surveillance system (2.37%), and that HCV transmission is increasing among young injecting adults in NYC. An ancillary benefit from our results is an estimate of current IDUs aged 20-59 in NYC: 0.58% or 27,600 individuals."
"Galaxies in the Illustris simulation as seen by the Sloan Digital Sky Survey - I: Bulge+disc decompositions, methods, and biases","We present an image-based method for comparing the structural properties of galaxies produced in hydrodynamical simulations to real galaxies in the Sloan Digital Sky Survey. The key feature of our work is the introduction of extensive observational realism, such as object crowding, noise and viewing angle, to the synthetic images of simulated galaxies, so that they can be fairly compared to real galaxy catalogs. We apply our methodology to the dust-free synthetic image catalog of galaxies from the Illustris simulation at $z=0$, which are then fit with bulge+disc models to obtain morphological parameters. In this first paper in a series, we detail our methods, quantify observational biases, and present publicly available bulge+disc decomposition catalogs. We find that our bulge+disc decompositions are largely robust to the observational biases that affect decompositions of real galaxies. However, we identify a significant population of galaxies (roughly 30\% of the full sample) in Illustris that are prone to internal segmentation, leading to systematically reduced flux estimates by up to a factor of 6, smaller half-light radii by up to a factor of $\sim$ 2, and generally erroneous bulge-to-total fractions of (B/T)=0."
PubTree: A Hierarchical Search Tool for the MEDLINE Database,"Keeping track of the ever-increasing body of scientific literature is an escalating challenge. We present PubTree a hierarchical search tool that efficiently searches the PubMed/MEDLINE dataset based upon a decision tree constructed using >26 million abstracts. The tool is implemented as a webpage, where users are asked a series of eighteen questions to locate pertinent articles. The implementation of this hierarchical search tool highlights issues endemic with document retrieval. However, the construction of this tree indicates that with future developments hierarchical search could become an effective tool (or adjunct) in the mining of biological literature."
Dimensional crossover and incipient quantum size effects in superconducting niobium nanofilms,"Superconducting and normal state properties of sputtered Niobium nanofilms have been systematically investigated, as a function of film thickness in a d=9-90 nm range, on different substrates. The width of the superconducting-to-normal transition for all films remained in few tens of mK, thus remarkably narrow, confirming their high quality. We found that the superconducting critical current density exhibits a pronounced maximum, three times larger than its bulk value, for film thickness around 25 nm, marking the 3D-to-2D crossover. The extracted magnetic penetration depth shows a sizeable enhancement for the thinnest films, aside the usual demagnetization effects. Additional amplification effects of the superconducting properties have been obtained in the case of sapphire substrates or squeezing the lateral size of the nanofilms. For thickness close to 20 nm we also measured a doubled perpendicular critical magnetic field compared to its saturation value for d>33 nm, indicating shortening of the correlation length and the formation of small Cooper pairs in the condensate. Our data analysis evidences an exciting interplay between quantum-size and proximity effects together with strong-coupling effects and importance of disorder in the thinnest films, locating the ones with optimally enhanced critical properties close to the BCS-BEC crossover regime."
Phase Diagram of Restricted Boltzmann Machines and Generalised Hopfield Networks with Arbitrary Priors,"  Restricted Boltzmann Machines are described by the Gibbs measure of a bipartite spin glass, which in turn corresponds to the one of a generalised Hopfield network. This equivalence allows us to characterise the state of these systems in terms of retrieval capabilities, both at low and high load. We study the paramagnetic-spin glass and the spin glass-retrieval phase transitions, as the pattern (i.e. weight) distribution and spin (i.e. unit) priors vary smoothly from Gaussian real variables to Boolean discrete variables. Our analysis shows that the presence of a retrieval phase is robust and not peculiar to the standard Hopfield model with Boolean patterns. The retrieval region is larger when the pattern entries and retrieval units get more peaked and, conversely, when the hidden units acquire a broader prior and therefore have a stronger response to high fields. Moreover, at low load retrieval always exists below some critical temperature, for every pattern distribution ranging from the Boolean to the Gaussian case. "
Causal nearest neighbor rules for optimal treatment regimes,"  The estimation of optimal treatment regimes is of considerable interest to precision medicine. In this work, we propose a causal $k$-nearest neighbor method to estimate the optimal treatment regime. The method roots in the framework of causal inference, and estimates the causal treatment effects within the nearest neighborhood. Although the method is simple, it possesses nice theoretical properties. We show that the causal $k$-nearest neighbor regime is universally consistent. That is, the causal $k$-nearest neighbor regime will eventually learn the optimal treatment regime as the sample size increases. We also establish its convergence rate. However, the causal $k$-nearest neighbor regime may suffer from the curse of dimensionality, i.e. performance deteriorates as dimensionality increases. To alleviate this problem, we develop an adaptive causal $k$-nearest neighbor method to perform metric selection and variable selection simultaneously. The performance of the proposed methods is illustrated in simulation studies and in an analysis of a chronic depression clinical trial. "
Definitions of solutions to the IBVP for multiD scalar balance laws,"  We consider four definitions of solution to the initial-boundary value problem for a scalar balance laws in several space dimensions. These definitions are generalised to the same most general framework and then compared. The first aim of this paper is to detail differences and analogies among them. We focus then on the ways the boundary conditions are fulfilled according to each definition, providing also connections among these various modes. The main result is the proof of the equivalence among the presented definitions of solution. "
Post-selection estimation and testing following aggregated association tests,"  The practice of pooling several individual test statistics to form aggregate tests is common in many statistical application where individual tests may be underpowered. While selection by aggregate tests can serve to increase power, the selection process invalidates the individual test-statistics, making it difficult to identify the ones that drive the signal in follow-up inference. Here, we develop a general approach for valid inference following selection by aggregate testing. We present novel powerful post-selection tests for the individual null hypotheses which are exact for the normal model and asymptotically justified otherwise. Our approach relies on the ability to characterize the distribution of the individual test statistics after conditioning on the event of selection. We provide efficient algorithms for estimation of the post-selection maximum-likelihood estimates and suggest confidence intervals which rely on a novel switching regime for good coverage guarantees. We validate our methods via comprehensive simulation studies and apply them to data from the Dallas Heart Study, demonstrating that single variant association discovery following selection by an aggregated test is indeed possible in practice. "
GANs for Medical Image Analysis,"  Generative Adversarial Networks (GANs) and their extensions have carved open many exciting ways to tackle well known and challenging medical image analysis problems such as medical image de-noising, reconstruction, segmentation, data simulation, detection or classification. Furthermore, their ability to synthesize images at unprecedented levels of realism also gives hope that the chronic scarcity of labeled data in the medical field can be resolved with the help of these generative models. In this review paper, a broad overview of recent literature on GANs for medical applications is given, the shortcomings and opportunities of the proposed methods are thoroughly discussed and potential future work is elaborated. We review the most relevant papers published until the submission date. For quick access, important details such as the underlying method, datasets and performance are tabulated. An interactive visualization categorizes all papers to keep the review alive. "
NFL Injuries Before and After the 2011 Collective Bargaining Agreement (CBA),"The National Football League's (NFL) 2011 collective bargaining agreement (CBA) with its players placed a number of contact and quantity limitations on practices and workouts. Some coaches and others have expressed a concern that this has led to poor conditioning and a subsequent increase in injuries. We sought to assess whether the 2011 CBA's practice restrictions affected the number of overall, conditioning-dependent, and/or non-conditioning-dependent injuries in the NFL or the number of games missed due to those injuries. The study population was player-seasons from 2007-2016. We included regular season, non-illness, non-head, game-loss injuries. Injuries were identified using a database from Football Outsiders. The primary outcomes were overall, conditioning-dependent and non-conditioning-dependent injury counts by season. We examined time trends in injury counts before (2007-2010) and after (2011-2016) the CBA using a Poisson interrupted time series model. The number of game-loss regular season, non-head, non-illness injuries grew from 701 in 2007 to 804 in 2016 (15% increase). The number of regular season weeks missed exhibited a similar increase. Conditioning-dependent injuries increased from 197 in 2007 to 271 in 2011 (38% rise), but were lower and remained relatively unchanged at 220-240 injuries per season thereafter. Non-conditioning injuries decreased by 37% in the first three years of the new CBA before returning to historic levels in 2014-2016. Poisson models for all, conditioning-dependent, and non-conditioning-dependent game-loss injury counts did not show statistically significant or meaningful detrimental changes associated with the CBA. We did not observe an increase in injuries following the 2011 CBA. Other concurrent injury-related rule and regulation changes limit specific causal inferences about the practice restrictions, however."
"Structured and Unstructured Outlier Identification for Robust PCA: A Non iterative, Parameter free Algorithm","  Robust PCA, the problem of PCA in the presence of outliers has been extensively investigated in the last few years. Here we focus on Robust PCA in the outlier model where each column of the data matrix is either an inlier or an outlier. Most of the existing methods for this model assumes either the knowledge of the dimension of the lower dimensional subspace or the fraction of outliers in the system. However in many applications knowledge of these parameters is not available. Motivated by this we propose a parameter free outlier identification method for robust PCA which a) does not require the knowledge of outlier fraction, b) does not require the knowledge of the dimension of the underlying subspace, c) is computationally simple and fast d) can handle structured and unstructured outliers. Further, analytical guarantees are derived for outlier identification and the performance of the algorithm is compared with the existing state of the art methods in both real and synthetic data for various outlier structures. "
Foveated Video Streaming for Cloud Gaming,"Good user experience with interactive cloud-based multimedia applications, such as cloud gaming and cloud-based VR, requires low end-to-end latency and large amounts of downstream network bandwidth at the same time. In this paper, we present a foveated video streaming system for cloud gaming. The system adapts video stream quality by adjusting the encoding parameters on the fly to match the player's gaze position. We conduct measurements with a prototype that we developed for a cloud gaming system in conjunction with eye tracker hardware. Evaluation results suggest that such foveated streaming can reduce bandwidth requirements by even more than 50% depending on parametrization of the foveated video coding and that it is feasible from the latency perspective."
Data-driven Analysis of Complex Networks and their Model-generated Counterparts,"Data-driven analysis of complex networks has been in the focus of research for decades. An important question is to discover the relation between various network characteristics in real-world networks and how these relationships vary across network domains. A related research question is to study how well the network models can capture the observed relations between the graph metrics. In this paper, we apply statistical and machine learning techniques to answer the aforementioned questions. We study 400 real-world networks along with 2400 networks generated by five frequently used network models with previously fitted parameters to make the generated graphs as similar to the real network as possible. We find that the correlation profiles of the structural measures significantly differ across network domains and the domain can be efficiently determined using a small selection of graph metrics. The goodness-of-fit of the network models and the best performing models themselves highly depend on the domains. Using machine learning techniques, it turned out to be relatively easy to decide if a network is real or model-generated. We also investigate what structural properties make it possible to achieve a good accuracy, i.e. what features the network models cannot capture."
A Short Note on Collecting Dependently Typed Values,"  Within dependently typed languages, such as Idris, types can depend on values. This dependency, however, can limit the collection of items in standard containers: all elements must have the same type, and as such their types must contain the same values. We present two dependently typed data structures for collecting dependent types: \texttt{DList} and \texttt{PList}. Use of these new data structures allow for the creation of single succinct inductive ADT whose constructions were previously verbose and split across many data structures. "
Dynamic Bridge-Finding in $\tilde{O}(\log ^2 n)$ Amortized Time,"We present a deterministic fully-dynamic data structure for maintaining information about the bridges in a graph. We support updates in $\tilde{O}((\log n)^2)$ amortized time, and can find a bridge in the component of any given vertex, or a bridge separating any two given vertices, in $O(\log n / \log \log n)$ worst case time. Our bounds match the current best for bounds for deterministic fully-dynamic connectivity up to $\log\log n$ factors. The previous best dynamic bridge finding was an $\tilde{O}((\log n)^3)$ amortized time algorithm by Thorup [STOC2000], which was a bittrick-based improvement on the $O((\log n)^4)$ amortized time algorithm by Holm et al.[STOC98, JACM2001]. Our approach is based on a different and purely combinatorial improvement of the algorithm of Holm et al., which by itself gives a new combinatorial $\tilde{O}((\log n)^3)$ amortized time algorithm. Combining it with Thorup's bittrick, we get down to the claimed $\tilde{O}((\log n)^2)$ amortized time. Essentially the same new trick can be applied to the biconnectivity data structure from [STOC98, JACM2001], improving the amortized update time to $\tilde{O}((\log n)^3)$. We also offer improvements in space. We describe a general trick which applies to both of our new algorithms, and to the old ones, to get down to linear space, where the previous best use $O(m + n\log n\log\log n)$. Finally, we show how to obtain $O(\log n/\log \log n)$ query time, matching the optimal trade-off between update and query time. Our result yields an improved running time for deciding whether a unique perfect matching exists in a static graph."
Decomposition of Uncertainty in Bayesian Deep Learning for Efficient and Risk-sensitive Learning,"  Bayesian neural networks with latent variables are scalable and flexible probabilistic models: They account for uncertainty in the estimation of the network weights and, by making use of latent variables, can capture complex noise patterns in the data. We show how to extract and decompose uncertainty into epistemic and aleatoric components for decision-making purposes. This allows us to successfully identify informative points for active learning of functions with heteroscedastic and bimodal noise. Using the decomposition we further define a novel risk-sensitive criterion for reinforcement learning to identify policies that balance expected cost, model-bias and noise aversion. "
Standard errors for regression on relational data with exchangeable errors,"  Relational arrays represent interactions or associations between pairs of actors, often in varied contexts or over time. Such data appear as, for example, trade flows between countries, financial transactions between individuals, contact frequencies between school children in classrooms, and dynamic protein-protein interactions. This paper proposes and evaluates a new class of parameter standard errors for models that represent elements of a relational array as a linear function of observable covariates. Uncertainty estimates for regression coefficients must account for both heterogeneity across actors and dependence arising from relations involving the same actor. Existing estimators of parameter standard errors that recognize such relational dependence rely on estimating extremely complex, heterogeneous structure across actors. Leveraging an exchangeability assumption, we derive parsimonious standard error estimators that pool information across actors and are substantially more accurate than existing estimators in a variety of settings. This exchangeability assumption is pervasive in network and array models in the statistics literature, but not previously considered when adjusting for dependence in a regression setting with relational data. We show that our estimator is consistent and demonstrate improvements in inference through simulation and a data set involving international trade. "
Calculating normal tissue complication probabilities and probabilities of complication-free tumour control from stochastic models of population dynamics,"  We use a stochastic birth-death model for a population of cells to estimate the normal tissue complication probability (NTCP) under a particular radiotherapy protocol. We specifically allow for interaction between cells, via a nonlinear logistic growth model. To capture some of the effects of intrinsic noise in the population we develop several approximations of NTCP, using Kramers-Moyal expansion techniques. These approaches provide an approximation to the first and second moments of a general first-passage time problem in the limit of large, but finite populations. We use this method to study NTCP in a simple model of normal cells and in a model of normal and damaged cells. We also study a combined model of normal tissue cells and tumour cells. Based on existing methods to calculate tumour control probabilities, and our procedure to approximate NTCP, we estimate the probability of complication free tumour control. "
Pathwise Derivatives Beyond the Reparameterization Trick,"  We observe that gradients computed via the reparameterization trick are in direct correspondence with solutions of the transport equation in the formalism of optimal transport. We use this perspective to compute (approximate) pathwise gradients for probability distributions not directly amenable to the reparameterization trick: Gamma, Beta, and Dirichlet. We further observe that when the reparameterization trick is applied to the Cholesky-factorized multivariate Normal distribution, the resulting gradients are suboptimal in the sense of optimal transport. We derive the optimal gradients and show that they have reduced variance in a Gaussian Process regression task. We demonstrate with a variety of synthetic experiments and stochastic variational inference tasks that our pathwise gradients are competitive with other methods. "
Network-size independent covering number bounds for deep networks,"  We give a covering number bound for deep learning networks that is independent of the size of the network. The key for the simple analysis is that for linear classifiers, rotating the data doesn't affect the covering number. Thus, we can ignore the rotation part of each layer's linear transformation, and get the covering number bound by concentrating on the scaling part. "
NIF: A Framework for Quantifying Neural Information Flow in Deep Networks,"  In this paper, we present a new approach to interpreting deep learning models. More precisely, by coupling mutual information with network science, we explore how information flows through feed forward networks. We show that efficiently approximating mutual information via the dual representation of Kullback-Leibler divergence allows us to create an information measure that quantifies how much information flows between any two neurons of a deep learning model. To that end, we propose NIF, Neural Information Flow, a new metric for codifying information flow which exposes the internals of a deep learning model while providing feature attributions. "
On functions given by algebraic power series over Henselian valued fields,"  This paper provides, over Henselian valued fields, some theorems on implicit function and of Artin--Mazur on algebraic power series. Also discussed are certain versions of the theorems of Abhyankar--Jung and Newton--Puiseux. The latter is used in analysis of functions of one variable, definable in the language of Denef--Pas, to obtain a theorem on existence of the limit, proven over rank one valued fields in one of our recent papers. This result along with the technique of fiber shrinking (developed there over rank one valued fields) were, in turn, two basic tools in the proof of the closedness theorem. "
What can one learn about material structure given a single first-principles calculation?,"  We extract a variable $X$ from electron orbitals $\Psi_{n\bf{k}}$ and energies $E_{n\bf{k}}$ in the parent high-symmetry structure of a wide range of complex oxides: perovskites, rutiles, pyrochlores, and cristobalites. Even though calculation was done only in the parent structure, with no distortions, we show that $X$ dictates material's true ground state structure. We propose using Wannier functions to extract concealed variables such as $X$ both for material structure prediction and for high-throughput approaches. "
xUnit: Learning a Spatial Activation Function for Efficient Image Restoration,"In recent years, deep neural networks (DNNs) achieved unprecedented performance in many low-level vision tasks. However, state-of-the-art results are typically achieved by very deep networks, which can reach tens of layers with tens of millions of parameters. To make DNNs implementable on platforms with limited resources, it is necessary to weaken the tradeoff between performance and efficiency. In this paper, we propose a new activation unit, which is particularly suitable for image restoration problems. In contrast to the widespread per-pixel activation units, like ReLUs and sigmoids, our unit implements a learnable nonlinear function with spatial connections. This enables the net to capture much more complex features, thus requiring a significantly smaller number of layers in order to reach the same performance. We illustrate the effectiveness of our units through experiments with state-of-the-art nets for denoising, de-raining, and super resolution, which are already considered to be very small. With our approach, we are able to further reduce these models by nearly 50% without incurring any degradation in performance."
"Multiwavelength study of VHE emission from Markarian 501 using TACTIC observations during April-May, 2012","We have observed Markarian 501 in Very High Energy (VHE) gamma-ray wavelength band for 70.6 hours from 15 April to 30 May, 2012 using TACTIC telescope. Detailed analysis of $\sim$66.3 hours of clean data revealed the presence of a TeV $\gamma$-ray signal (686$\pm$77 $\gamma$-ray events) from the source direction with a statistical significance of 8.89$\sigma$ above 850 GeV. Further, a total of 375 $\pm$ 47 $\gamma$-ray like events were detected in 25.2 hours of observation from 22 - 27 May, 2012 with a statistical significance of 8.05$\sigma$ indicating that the source has possibly switched over to a relatively high gamma-ray emission state. We have derived time-averaged differential energy spectrum of the state in the energy range 850 GeV - 17.24 TeV which fits well with a power law function of the form $dF/dE=f_{0}E^{-\Gamma}$ with $f_{0}= (2.27 \pm 0.38) \times 10 ^{-11} $ photons cm$^{-2}$ s$^{-1}$ TeV$^{-1}$ and $\Gamma=2.57 \pm 0.15$. In order to investigate the source state, we have also used almost simultaneous multiwavelength observations viz: high energy data collected by $\it{Fermi}$-LAT, X-ray data collected by $\it{Swift}$-XRT and MAXI, optical and UV data collected by $\it{Swift}$-UVOT, and radio data collected by OVRO, and reconstructed broad-band Spectral Energy Distribution (SED). The obtained SED supports leptonic model (homogeneous single zone) for VHE gamma-ray emission involving synchrotron and synchrotron self Compton (SSC) processes."
"Sparsity enforcing priors in inverse problems via Normal variance mixtures: model selection, algorithms and applications","The sparse structure of the solution for an inverse problem can be modelled using different sparsity enforcing priors when the Bayesian approach is considered. Analytical expression for the unknowns of the model can be obtained by building hierarchical models based on sparsity enforcing distributions expressed via conjugate priors. We consider heavy tailed distributions with this property: the Student-t distribution, which is expressed as a Normal scale mixture, with the mixing distribution the Inverse Gamma distribution, the Laplace distribution, which can also be expressed as a Normal scale mixture, with the mixing distribution the Exponential distribution or can be expressed as a Normal inverse scale mixture, with the mixing distribution the Inverse Gamma distribution, the Hyperbolic distribution, the Variance-Gamma distribution, the Normal-Inverse Gaussian distribution, all three expressed via conjugate distributions using the Generalized Hyperbolic distribution. For all distributions iterative algorithms are derived based on hierarchical models that account for the uncertainties of the forward model. For estimation, Maximum A Posterior (MAP) and Posterior Mean (PM) via variational Bayesian approximation (VBA) are used. The performances of resulting algorithm are compared in applications in 3D computed tomography (3D-CT) and chronobiology. Finally, a theoretical study is developed for comparison between sparsity enforcing algorithms obtained via the Bayesian approach and the sparsity enforcing algorithms issued from regularization techniques, like LASSO and some others."
Learning Koopman Invariant Subspaces for Dynamic Mode Decomposition,"  Spectral decomposition of the Koopman operator is attracting attention as a tool for the analysis of nonlinear dynamical systems. Dynamic mode decomposition is a popular numerical algorithm for Koopman spectral analysis; however, we often need to prepare nonlinear observables manually according to the underlying dynamics, which is not always possible since we may not have any a priori knowledge about them. In this paper, we propose a fully data-driven method for Koopman spectral analysis based on the principle of learning Koopman invariant subspaces from observed data. To this end, we propose minimization of the residual sum of squares of linear least-squares regression to estimate a set of functions that transforms data into a form in which the linear regression fits well. We introduce an implementation with neural networks and evaluate performance empirically using nonlinear dynamical systems and applications. "
Structure and Content of the Visible Darknet,"In this paper, we analyze the topology and the content found on the ""darknet"", the set of websites accessible via Tor. We created a darknet spider and crawled the darknet starting from a bootstrap list by recursively following links. We explored the whole connected component of more than 34,000 hidden services, of which we found 10,000 to be online. Contrary to folklore belief, the visible part of the darknet is surprisingly well-connected through hub websites such as wikis and forums. We performed a comprehensive categorization of the content using supervised machine learning. We observe that about half of the visible dark web content is related to apparently licit activities based on our classifier. A significant amount of content pertains to software repositories, blogs, and activism-related websites. Among unlawful hidden services, most pertain to fraudulent websites, services selling counterfeit goods, and drug markets."
Pro-arrhythmogenic effects of heterogeneous tissue curvature: A suggestion for role of left atrial appendage in atrial fibrillation,"Background: The arrhythmogenic role of atrial complex morphology has not yet been clearly elucidated. We hypothesized that bumpy tissue geometry can induce action potential duration (APD) dispersion and wavebreak in atrial fibrillation (AF). Methods and Results: We simulated 2D-bumpy atrial model by varying the degree of bumpiness, and 3D-left atrial (LA) models integrated by LA computed tomographic (CT) images taken from 14 patients with persistent AF. We also analyzed wave-dynamic parameters with bipolar electrograms during AF and compared them with LA-CT geometry in 30 patients with persistent AF. In 2D-bumpy model, APD dispersion increased (p<0.001) and wavebreak occurred spontaneously when the surface bumpiness was higher, showing phase transition-like behavior (p<0.001). Bumpiness gradient 2D-model showed that spiral wave drifted in the direction of higher bumpiness, and phase singularity (PS) points were mostly located in areas with higher bumpiness. In 3D-LA model, PS density was higher in LA appendage (LAA) compared to other LA parts (p<0.05). In 30 persistent AF patients, the surface bumpiness of LAA was 5.8-times that of other LA parts (p<0.001), and exceeded critical bumpiness to induce wavebreak. Wave dynamics complexity parameters were consistently dominant in LAA (p<0.001). Conclusion: The bumpy tissue geometry promotes APD dispersion, wavebreak, and spiral wave drift in in silico human atrial tissue, and corresponds to clinical electro-anatomical maps."
Fragmentation of phase-fluctuating condensates,"  We study zero-temperature quantum phase fluctuations in harmonically trapped one-dimensional interacting Bose gases, using the self-consistent multiconfigurational time-dependent Hartree method. In a regime of mesoscopic particle numbers and moderate contact couplings, it is shown that the phase-fluctuating condensate is properly described as a fragmented condensate. In addition, we demonstrate that the spatial dependence of the amplitude of phase fluctuations significantly deviates from what is obtained in Bogoliubov theory. Our results can be verified in currently available experiments. They therefore provide an opportunity both to experimentally benchmark the multiconfigurational time-dependent Hartree method, as well as to directly observe, for the first time, the quantum many-body phenomenon of fragmentation in single traps. "
Thermoelectric transport parallel to the planes in a multilayered Mott-Hubbard heterostructure,"  We present a theory for charge and heat transport parallel to the interfaces of a multilayer (ML) in which the interfacing gives rise the redistribution of the electronic charges. The ensuing electrical field couples self-consistently to the itinerant electrons, so that the properties of the ML crucially depend on an interplay between the on-site Coulomb forces and the long range electrostatic forces. The ML is described by the Falicov-Kimball model and the self-consistent solution is obtained by iterating simultaneously the DMFT and the Poisson equations. This yields the reconstructed charge profile, the electrical potential, the planar density of states, the transport function, and the transport coefficients of the device. We find that a heterostructure built of two Mott-Hubbard insulators exhibits, in a large temperature interval, a linear conductivity and a large temperature-independent thermopower. The charge and energy currents are confined to the central part of the ML. Our results indicate that correlated multilayers have the potential for applications; by tuning the band shift and the Coulomb correlation on the central planes, we can bring the chemical potential in the immediate proximity of the Mott-Hubbard gap edge and optimize the transport properties of the device. In such a heterostructure, a small gate voltage can easily induce a MI transition. This switching does not involve the diffusion of electrons over macroscopic distances and it is much faster than in ordinary semiconductors. Furthermore, the right combination of strongly correlated materials with small ZT can produce, theoretically at least, a heterostructure with a large ZT. "
Mining Device-Specific Apps Usage Patterns from Large-Scale Android Users,"When smartphones, applications (a.k.a, apps), and app stores have been widely adopted by the billions, an interesting debate emerges: whether and to what extent do device models influence the behaviors of their users? The answer to this question is critical to almost every stakeholder in the smartphone app ecosystem, including app store operators, developers, end-users, and network providers. To approach this question, we collect a longitudinal data set of app usage through a leading Android app store in China, called Wandoujia. The data set covers the detailed behavioral profiles of 0.7 million (761,262) unique users who use 500 popular types of Android devices and about 0.2 million (228,144) apps, including their app management activities, daily network access time, and network traffic of apps. We present a comprehensive study on investigating how the choices of device models affect user behaviors such as the adoption of app stores, app selection and abandonment, data plan usage, online time length, the tendency to use paid/free apps, and the preferences to choosing competing apps. Some significant correlations between device models and app usage are derived, leading to important findings on the various user behaviors. For example, users owning different device models have a substantial diversity of selecting competing apps, and users owning lower-end devices spend more money to purchase apps and spend more time under cellular network."
Homotopical Stable Ranks for Certain C*-algebras,"  We study the general and connected stable ranks for C*-algebras. We estimate these ranks for pullbacks of C*-algebras, and for tensor products by commutative C*-algebras. Finally, we apply these results to determine these ranks for certain commutative C*-algebras, and non-commutative CW-complexes. "
Covariantly functorial wrapped Floer theory on Liouville sectors,"  We introduce a class of Liouville manifolds with boundary which we call Liouville sectors. We define the wrapped Fukaya category, symplectic cohomology, and the open-closed map for Liouville sectors, and we show that these invariants are covariantly functorial with respect to inclusions of Liouville sectors. From this foundational setup, a local-to-global principle for Abouzaid's generation criterion follows. "
Robust Sparse Covariance Estimation by Thresholding Tyler's M-Estimator,"Estimating a high-dimensional sparse covariance matrix from a limited number of samples is a fundamental problem in contemporary data analysis. Most proposals to date, however, are not robust to outliers or heavy tails. Towards bridging this gap, in this work we consider estimating a sparse shape matrix from $n$ samples following a possibly heavy tailed elliptical distribution. We propose estimators based on thresholding either Tyler's M-estimator or its regularized variant. We derive bounds on the difference in spectral norm between our estimators and the shape matrix in the joint limit as the dimension $p$ and sample size $n$ tend to infinity with $p/n\to\gamma>0$. These bounds are minimax rate-optimal. Results on simulated data support our theoretical analysis."
Human Perceptions of Fairness in Algorithmic Decision Making: A Case Study of Criminal Risk Prediction,"As algorithms are increasingly used to make important decisions that affect human lives, ranging from social benefit assignment to predicting risk of criminal recidivism, concerns have been raised about the fairness of algorithmic decision making. Most prior works on algorithmic fairness normatively prescribe how fair decisions ought to be made. In contrast, here, we descriptively survey users for how they perceive and reason about fairness in algorithmic decision making. A key contribution of this work is the framework we propose to understand why people perceive certain features as fair or unfair to be used in algorithms. Our framework identifies eight properties of features, such as relevance, volitionality and reliability, as latent considerations that inform people's moral judgments about the fairness of feature use in decision-making algorithms. We validate our framework through a series of scenario-based surveys with 576 people. We find that, based on a person's assessment of the eight latent properties of a feature in our exemplar scenario, we can accurately (> 85%) predict if the person will judge the use of the feature as fair. Our findings have important implications. At a high-level, we show that people's unfairness concerns are multi-dimensional and argue that future studies need to address unfairness concerns beyond discrimination. At a low-level, we find considerable disagreements in people's fairness judgments. We identify root causes of the disagreements, and note possible pathways to resolve them."
Genetic Algorithm Based Floor Planning System,  Genetic Algorithms are widely used in many different optimization problems including layout design. The layout of the shelves play an important role in the total sales metrics for superstores since this affects the customers' shopping behaviour. This paper employed a genetic algorithm based approach to design shelf layout of superstores. The layout design problem was tackled by using a novel chromosome representation which takes many different parameters to prevent dead-ends and improve shelf visibility into consideration. Results show that the approach can produce reasonably good layout designs in very short amounts of time. 
"On SDEs with Lipschitz coefficients, driven by continuous, model-free price paths","  Using similar assumptions as in Revuz and Yor's book we prove the existence and uniqueness of the solutions of SDEs with Lipschitz coefficients, driven by continuous, model-free price paths. The main tool in our reasonings is a model-free version of the Burkholder-Davis-Gundy inequality for integrals driven by model-free, continuous price paths. "
Stability of receding traveling waves for a fourth order degenerate parabolic free boundary problem,"Consider the thin-film equation $h_t + \left(h h_{yyy}\right)_y = 0$ with a zero contact angle at the free boundary, that is, at the triple junction where liquid, gas, and solid meet. Previous results on stability and well-posedness of this equation have focused on perturbations of equilibrium-stationary or self-similar profiles, the latter eventually wetting the whole surface. These solutions have their counterparts for the second-order porous-medium equation $h_t - (h^m)_{yy} = 0$, where $m > 1$ is a free parameter. Both porous-medium and thin-film equation degenerate as $h \searrow 0$, but the porous-medium equation additionally fulfills a comparison principle while the thin-film equation does not. In this note, we consider traveling waves $h = \frac V 6 x^3 + \nu x^2$ for $x \ge 0$, where $x = y-V t$ and $V, \nu \ge 0$ are free parameters. These traveling waves are receding and therefore describe de-wetting, a phenomenon genuinely linked to the fourth-order nature of the thin-film equation and not encountered in the porous-medium case as it violates the comparison principle. The linear stability analysis leads to a linear fourth-order degenerate-parabolic operator for which we prove maximal-regularity estimates to arbitrary orders of the expansion in $x$ in a right-neighborhood of the contact line $x = 0$. This leads to a well-posedness and stability result for the corresponding nonlinear equation. As the linearized evolution has different scaling as $x \searrow 0$ and $x \to \infty$, the analysis is more intricate than in related previous works. We anticipate that our approach is a natural step towards investigating other situations in which the comparison principle is violated, such as droplet rupture."
Group Importance Sampling for Particle Filtering and MCMC,"  Bayesian methods and their implementations by means of sophisticated Monte Carlo techniques have become very popular in signal processing over the last years. Importance Sampling (IS) is a well-known Monte Carlo technique that approximates integrals involving a posterior distribution by means of weighted samples. In this work, we study the assignation of a single weighted sample which compresses the information contained in a population of weighted samples. Part of the theory that we present as Group Importance Sampling (GIS) has been employed implicitly in different works in the literature. The provided analysis yields several theoretical and practical consequences. For instance, we discuss the application of GIS into the Sequential Importance Resampling framework and show that Independent Multiple Try Metropolis schemes can be interpreted as a standard Metropolis-Hastings algorithm, following the GIS approach. We also introduce two novel Markov Chain Monte Carlo (MCMC) techniques based on GIS. The first one, named Group Metropolis Sampling method, produces a Markov chain of sets of weighted samples. All these sets are then employed for obtaining a unique global estimator. The second one is the Distributed Particle Metropolis-Hastings technique, where different parallel particle filters are jointly used to drive an MCMC algorithm. Different resampled trajectories are compared and then tested with a proper acceptance probability. The novel schemes are tested in different numerical experiments such as learning the hyperparameters of Gaussian Processes, two localization problems in a wireless sensor network (with synthetic and real data) and the tracking of vegetation parameters given satellite observations, where they are compared with several benchmark Monte Carlo techniques. Three illustrative Matlab demos are also provided. "
Double Temporal Sparsity Based Accelerated Reconstruction in Compressed Sensing fMRI,"A number of reconstruction methods have been proposed recently for accelerated functional Magnetic Resonance Imaging (fMRI) data collection. However, existing methods suffer with the challenge of greater artifacts at high acceleration factors. This paper addresses the issue of accelerating fMRI collection via undersampled k-space measurements combined with the proposed Double Temporal Sparsity based Reconstruction (DTSR) method with the l1 -l1 norm constraint. The robustness of the proposed DTSR method has been thoroughly evaluated both at the subject level and at the group level on real fMRI data. Results are presented at various acceleration factors. Quantitative analysis in terms of Peak Signal-to-Noise Ratio (PSNR) and other metrics, and qualitative analysis in terms of reproducibility of brain Resting State Networks (RSNs) demonstrate that the proposed method is accurate and robust. In addition, the proposed DTSR method preserves brain networks that are important for studying fMRI data. Compared to the existing accelerated fMRI reconstruction methods, the DTSR method shows promising potential with an improvement of 10-12dB in PSNR with acceleration factors upto 3.5. Simulation results on real data demonstrate that DTSR method can be used to acquire accelerated fMRI with accurate detection of RSNs."
A continuum model for distributions of dislocations incorporating short-range interactions,"  Dislocations are the main carriers of the permanent deformation of crystals. For simulations of engineering applications, continuum models where material microstructures are represented by continuous density distributions of dislocations are preferred. It is challenging to capture in the continuum model the short-range dislocation interactions, which vanish after the standard averaging procedure from discrete dislocation models. In this study, we consider systems of parallel straight dislocation walls and develop continuum descriptions for the short-range interactions of dislocations by using asymptotic analysis. The obtained continuum short-range interaction formulas are incorporated in the continuum model for dislocation dynamics based on a pair of dislocation density potential functions that represent continuous distributions of dislocations. This derived continuum model is able to describe the anisotropic dislocation interaction and motion. Mathematically, these short-range interaction terms ensure strong stability property of the continuum model that is possessed by the discrete dislocation dynamics model. The derived continuum model is validated by comparisons with the discrete dislocation dynamical simulation results. "
Phase transition and power-law coarsening in Ising-doped voter model,"We examine an opinion formation model, which is a mixture of Voter and Ising agents. Numerical simulations show that even a very small fraction ($\sim 1\%$) of the Ising agents drastically changes the behaviour of the Voter model. The Voter agents act as a medium, which correlates sparsely dispersed Ising agents, and the resulting ferromagnetic ordering persists up to a certain temperature. Upon addition of the Ising agents, a logarithmically slow coarsening of the Voter model ($d=2$), or its active steady state ($d=3$), change into an Ising-type power-law coarsening."
A probabilistic data-driven model for planar pushing,"This paper presents a data-driven approach to model planar pushing interaction to predict both the most likely outcome of a push and its expected variability. The learned models rely on a variation of Gaussian processes with input-dependent noise called Variational Heteroscedastic Gaussian processes (VHGP) that capture the mean and variance of a stochastic function. We show that we can learn accurate models that outperform analytical models after less than 100 samples and saturate in performance with less than 1000 samples. We validate the results against a collected dataset of repeated trajectories, and use the learned models to study questions such as the nature of the variability in pushing, and the validity of the quasi-static assumption."
Dual constant-flux energy cascades to both large scales and small scales,"In this paper, we present an overview of concepts and data concerning inverse cascades of excitation towards scales larger than the forcing scale in a variety of contexts, from two-dimensional fluids and wave turbulence, to geophysical flows in the presence of rotation and stratification. We briefly discuss the role of anisotropy in the occurrence and properties of such cascades. We then show that the cascade of some invariant, for example the total energy, may be transferred through nonlinear interactions to both the small scales and the large scales, with in each case a constant flux. This is in contrast to the classical picture, and we illustrate such a dual cascade in the context of atmospheric and oceanic observations, direct numerical simulations and modeling. We also show that this dual cascade of total energy can in fact be decomposed in some cases into separate cascades of the kinetic and potential energies, provided the Froude and Rossby numbers are small enough. In all cases, the potential energy flux remains small, of the order of 10% or less relative to the kinetic energy flux. Finally, we demonstrate that, in the small-scale inertial range, approximate equipartition between potential and kinetic modes is obtained, leading to an energy ratio close to one, with strong departure at large scales due to the dominant kinetic energy inverse cascade and piling-up at the lowest spatial frequency, and at small scales due to unbalanced dissipation processes, even though the Prandtl number is equal to one."
Worst-case Optimal Submodular Extensions for Marginal Estimation,"  Submodular extensions of an energy function can be used to efficiently compute approximate marginals via variational inference. The accuracy of the marginals depends crucially on the quality of the submodular extension. To identify the best possible extension, we show an equivalence between the submodular extensions of the energy and the objective functions of linear programming (LP) relaxations for the corresponding MAP estimation problem. This allows us to (i) establish the worst-case optimality of the submodular extension for Potts model used in the literature; (ii) identify the worst-case optimal submodular extension for the more general class of metric labeling; and (iii) efficiently compute the marginals for the widely used dense CRF model with the help of a recently proposed Gaussian filtering method. Using synthetic and real data, we show that our approach provides comparable upper bounds on the log-partition function to those obtained using tree-reweighted message passing (TRW) in cases where the latter is computationally feasible. Importantly, unlike TRW, our approach provides the first practical algorithm to compute an upper bound on the dense CRF model. "
Superconducting energy gap in $\rm Ba_{1-x}K_xBiO_3$: Temperature dependence,"The superconducting energy gap of $\rm Ba_{1-x}K_xBiO_3$ has been measured by tunneling. Despite the fact that the sample was macroscopically single phase with very sharp superconducting transition $T_c$ at 32~$K$, some of the measured tunnel junctions made by point contacts between silver tip and single crystal of $\rm Ba_{1-x}K_xBiO_3$ had lower transition at 20~$K$. Local variation of the potassium concentration as well as oxygen deficiency in $\rm Ba_{1-x}K_xBiO_3$ at the place where the point contact is made can account for the change of $T_c$. The conductance curves of the tunnel junctions reveal the BCS behavior with a small broadening of the superconducting-gap structure. A value of the energy gap scales with $T_c$. The reduced gap amounts to $2\Delta/kT_c = 4÷4.3$ indicating a medium coupling strength. Temperature dependence of the energy gap follows the BCS prediction."
Square functions and the Hamming cube: Duality,"For $1<p\leq 2$, any $n\geq 1$ and any $f:\{-1,1\}^{n} \to \mathbb{R}$, we obtain $(\mathbb{E} |\nabla f|^{p})^{1/p} \geq C(p)(\mathbb{E}|f|^{p} - |\mathbb{E}f|^{p})^{1/p}$ where $C(p)$ is the smallest positive zero of the confluent hypergeometric function ${}_{1}F_{1}(\frac{p}{2(1-p)}, \frac{1}{2}, \frac{x^{2}}{2})$. Our approach is based on a certain duality between the classical square function estimates on the Euclidean space and the gradient estimates on the Hamming cube."
A Special Homotopy Continuation Method For A Class of Polynomial Systems,"  A special homotopy continuation method, as a combination of the polyhedral homotopy and the linear product homotopy, is proposed for computing all the isolated solutions to a special class of polynomial systems. The root number bound of this method is between the total degree bound and the mixed volume bound and can be easily computed. The new algorithm has been implemented as a program called LPH using C++. Our experiments show its efficiency compared to the polyhedral or other homotopies on such systems. As an application, the algorithm can be used to find witness points on each connected component of a real variety. "
Arbitrage Opportunities in CDS Term Structure: Theory and Implications for OTC Derivatives,"Absence-of-Arbitrage (AoA) is the basic assumption underpinning derivatives pricing theory. As part of the OTC derivatives market, the CDS market not only provides a vehicle for participants to hedge and speculate on the default risks of corporate and sovereign entities, it also reveals important market-implied default-risk information concerning the counterparties with which financial institutions trade, and for which these financial institutions have to calculate various valuation adjustments (collectively referred to as XVA) as part of their pricing and risk management of OTC derivatives, to account for counterparty default risks. In this study, we derive No-arbitrage conditions for CDS term structures, first in a positive interest rate environment and then in an arbitrary one. Using an extensive CDS dataset which covers the 2007-09 financial crisis, we present a catalogue of 2,416 pairs of anomalous CDS contracts which violate the above conditions. Finally, we show in an example that such anomalies in the CDS term structure can lead to persistent arbitrage profits and to nonsensical default probabilities. The paper is a first systematic study on CDS-term-structure arbitrage providing model-free AoA conditions supported by ample empirical evidence."
Asymptotic behavior of metric spaces at infinity,  A new sequential approach to investigations of structure of metric spaces at infinity is proposed. Criteria for finiteness and boundedness of metric spaces at infinity are found. 
Decoupling feature extraction from policy learning: assessing benefits of state representation learning in goal based robotics,"  Scaling end-to-end reinforcement learning to control real robots from vision presents a series of challenges, in particular in terms of sample efficiency. Against end-to-end learning, state representation learning can help learn a compact, efficient and relevant representation of states that speeds up policy learning, reducing the number of samples needed, and that is easier to interpret. We evaluate several state representation learning methods on goal based robotics tasks and propose a new unsupervised model that stacks representations and combines strengths of several of these approaches. This method encodes all the relevant features, performs on par or better than end-to-end learning, and is robust to hyper-parameters change. "
$q-$difference equations concerning with $q-$gamma function,"  We consider a family of solutions of $q-$difference Riccati equation, and prove the meromorphic solutions of $q-$difference Riccati equation and corresponding second order $q-$difference equation are concerning with $q-$gamma function. The growth and value distribution of differences on solutions of $q-$difference Riccati equation are also investigated. "
Structural and Magnetic Phase Transitions in Chromium Nitride Thin Films Grown by RF Nitrogen Plasma Molecular Beam Epitaxy,"A magneto-structural phase transition is investigated in single crystal CrN thin films grown by rf plasma molecular beam epitaxy on MgO(001) substrates. While still within the vacuum environment following MBE growth, $\it in-situ$ low-temperature scanning tunneling microscopy, and $\it in-situ$ variable low-temperature reflection high energy electron diffraction are applied, revealing an atomically smooth and metallic CrN(001) surface, and an $\it in-plane$ structural transition from 1$\times$1 (primitive CrN unit cell) to $\mathrm{\sqrt{2}\times\sqrt{2}-R45^\circ}$ with a transition temperature of $\sim$ 278 K, respectively. $\it Ex-situ$ temperature dependent measurements are also performed, including x-ray diffraction and neutron diffraction, looking at the structural peaks and likewise revealing a first-order structural transition along both [001] and [111] $\it out-of-plane$ directions, with transition temperatures of 256 K and 268 K, respectively. Turning to the magnetic peaks, neutron diffraction confirms a clear magnetic transition from paramagnetic at room temperature to antiferromagnetic at low temperatures with a sharp, first-order phase transition and a N$é$el temperature of 270 K or 280 K for two different films. In addition to the experimental measurements of structural and magnetic ordering, we also discuss results from first-principles theoretical calculations which explore various possible magneto-structural models."
Predicting Audience's Laughter Using Convolutional Neural Network,"  For the purpose of automatically evaluating speakers' humor usage, we build a presentation corpus containing humorous utterances based on TED talks. Compared to previous data resources supporting humor recognition research, ours has several advantages, including (a) both positive and negative instances coming from a homogeneous data set, (b) containing a large number of speakers, and (c) being open. Focusing on using lexical cues for humor recognition, we systematically compare a newly emerging text classification method based on Convolutional Neural Networks (CNNs) with a well-established conventional method using linguistic knowledge. The advantages of the CNN method are both getting higher detection accuracies and being able to learn essential features automatically. "
The Solar Orbiter Mission: an Energetic Particle Perspective,"Solar Orbiter is a joint ESA-NASA mission planed for launch in October 2018. The science payload includes remote-sensing and in-situ instrumentation designed with the primary goal of understanding how the Sun creates and controls the heliosphere. The spacecraft will follow an elliptical orbit around the Sun, with perihelion as close as 0.28 AU. During the late orbit phase the orbital plane will reach inclinations above 30 degrees, allowing direct observations of the solar polar regions. The Energetic Particle Detector (EPD) is an instrument suite consisting of several sensors measuring electrons, protons and ions over a broad energy interval (2 keV to 15 MeV for electrons, 3 keV to 100 MeV for protons and few tens of keV/nuc to 450 MeV/nuc for ions), providing composition, spectra, timing and anisotropy information. We present an overview of Solar Orbiter from the energetic particle perspective, summarizing the capabilities of EPD and the opportunities that these new observations will provide for understanding how energetic particles are accelerated during solar eruptions and how they propagate through the Heliosphere."
Consistent feature attribution for tree ensembles,"Note that a newer expanded version of this paper is now available at: arXiv:1802.03888 It is critical in many applications to understand what features are important for a model, and why individual predictions were made. For tree ensemble methods these questions are usually answered by attributing importance values to input features, either globally or for a single prediction. Here we show that current feature attribution methods are inconsistent, which means changing the model to rely more on a given feature can actually decrease the importance assigned to that feature. To address this problem we develop fast exact solutions for SHAP (SHapley Additive exPlanation) values, which were recently shown to be the unique additive feature attribution method based on conditional expectations that is both consistent and locally accurate. We integrate these improvements into the latest version of XGBoost, demonstrate the inconsistencies of current methods, and show how using SHAP values results in significantly improved supervised clustering performance. Feature importance values are a key part of understanding widely used models such as gradient boosting trees and random forests, so improvements to them have broad practical implications."
K-orbit closures and Barbasch-Evens-Magyar varieties,"We define the Barbasch-Evens-Magyar variety. We show it is isomorphic to the smooth variety defined in [D. Barbasch-S. Evens '94] that maps finite-to-one to a symmetric orbit closure, thereby giving a resolution of singularities in certain cases. Our definition parallels [P. Magyar '98]'s construction of the Bott-Samelson variety [H. C. Hansen '73, M. Demazure '74]. From this alternative viewpoint, one deduces a graphical description in type A, stratification into closed subvarieties of the same kind, and determination of the torus-fixed points. Moreover, we explain how these manifolds inherit a natural symplectic structure with Hamiltonian torus action. We then prove that the moment polytope is expressed in terms of the moment polytope of a Bott-Samelson variety."
"A Review of Internet of Things Architecture, Technologies and Analysis Smartphone-based Attacks Against 3D printers","Human beings cannot be happy with any kind of tiredness based work, so they focused on machines to work on behalf of humans. The Internet-based latest technology provides the platforms for human beings to relax and unburden feeling. The Internet of Things (IoT) field efficiently helps human beings with smart decisions through Machine-to-Machine (M2M) communication all over the world. It has been difficult to ignore the importance of the IoT field with the new development of applications such as a smartphone in the present era. The IoT field sensor plays a vital role in sensing the intelligent object/things and making an intelligent decision after sensing the objects. The rapid development of new applications using smartphones in the world caused all users of the IoT community to be faced with one major challenge of security in the form of side channel attacks against highly intensive 3D printing systems. The smartphone formulated Intellectual property (IP) of side channel attacks investigate against 3D printer in the physical domain through reconstructed G-code file through primitive operations. The smartphone (Nexus 5) solved the main problems such as orientation fixing, model accuracy of frame size and validate the feasibility and effectiveness in real case studies against the 3D printer. The 3D printing estimated value reached 20.2 billion of dollars in 2021. The thermal camera is used for exploring the side channel attacks after reconstructing the objects against 3D printers. The researcher analyzed IoT security relevant issues which were avoided in future by enhanced strong security mechanism strategy, encryption, and machine learning-based algorithms, latest technologies, schemes and protocols utilized in an efficient way. Keywords: - Internet of Things (IoT), Machine-to-Machine (M2M), Security, 3D printer, smartphone"
Computable structures on topological manifolds,"We propose a definition of computable manifold by introducing computability as a structure that we impose to a given topological manifold, just in the same way as differentiability or piecewise linearity are defined for smooth and PL manifolds respectively. Using the framework of computable topology and Type-2 theory of effectivity, we develop computable versions of all the basic concepts needed to define manifolds, like computable atlases and (computably) compatible computable atlases. We prove that given a computable atlas $\Phi$ defined on a set $M$, we can construct a computable topological space $(M, \tau_\Phi, \beta_\Phi, \nu_\Phi)$, where $\tau_\Phi$ is the topology on $M$ induced by $\Phi$ and that the equivalence class of this computable space characterizes the computable structure determined by $\Phi$. The concept of computable submanifold is also investigated. We show that any compact computable manifold which satisfies a computable version of the $T_2$-separation axiom, can be embedded as a computable submanifold of some euclidean space $\mathbb{R}^{q}$, with a computable embedding, where $\mathbb{R}^{q}$ is equipped with its usual topology and some canonical computable encoding of all open rational balls."
Trace-Based Run-time Analysis of Message-Passing Go Programs,"  We consider the task of analyzing message-passing programs by observing their run-time behavior. We introduce a purely library-based instrumentation method to trace communication events during execution. A model of the dependencies among events can be constructed to identify potential bugs. Compared to the vector clock method, our approach is much simpler and has in general a significant lower run-time overhead. A further advantage is that we also trace events that could not commit. Thus, we can infer alternative communications. This provides the user with additional information to identify potential bugs. We have fully implemented our approach in the Go programming language and provide a number of examples to substantiate our claims. "
An efficient algorithm for global interval solution of nonlinear algebraic equations and its GPGPU implementation,"  Solving nonlinear algebraic equations is a classic mathematics problem, and common in scientific researches and engineering applications. There are many numeric, symbolic and numeric-symbolic methods of solving (real) solutions. Unlucky, these methods are constrained by some factors, e.g., high complexity, slow serial calculation, and the notorious intermediate expression expansion. Especially when the count of variables is larger than six, the efficiency is decreasing drastically. In this paper, according to the property of physical world, we pay attention to nonlinear algebraic equations whose variables are in fixed constraints, and get meaningful real solutions. Combining with parallelism of GPGPU, we present an efficient algorithm, by searching the solution space globally and solving the nonlinear algebraic equations with real interval solutions. Furthermore, we realize the Hansen-Sengupta method on GPGPU. The experiments show that our method can solve many nonlinear algebraic equations, and the results are accurate and more efficient compared to traditional serial methods. "
Efficient improvement of frequency-domain Kalman filter,"  The frequency-domain Kalman filter (FKF) has been utilized in many audio signal processing applications due to its fast convergence speed and robustness. However, the performance of the FKF in under-modeling situations has not been investigated. This paper presents an analysis of the steady-state behavior of the commonly used diagonalized FKF and reveals that it suffers from a biased solution in under-modeling scenarios. Two efficient improvements of the FKF are proposed, both having the benefits of the guaranteed optimal steady-state behavior at the cost of a very limited increase of the computational burden. The convergence behavior of the proposed algorithms is also compared analytically. Computer simulations are conducted to validate the improved performance of the proposed methods. "
"""Higgs"" Factory at the Greek-Turkish Border","We would like to propose the construction of the photon collider based ""Higgs factory"" in the coming years at the Greek-Turkish border, starting from its test facility with a high energy photon beam. This proposal was among the contributions to the Open Symposium of the ESPG'12."
Detection and numerical simulation of optoacoustic near- and farfield signals observed in PVA hydrogel phantoms,"We present numerical simulations for modelling optoacoustic (OA) signals observed in PVA hydrogel tissue phantoms. We review the computational approach to model the underlying mechanisms, i.e. optical absorption of laser energy and acoustic propagation of mechanical stress, geared towards experiments that involve absorbing media only. We apply the numerical procedure to model signals observed in the acoustic near- and farfield in both, forward and backward detection mode, in PVA hydrogel tissue phantoms (i.e. an elastic solid). Further, we illustrate the computational approach by modeling OA signal for several experiments on dye solution (i.e. a liquid) reported in the literature, and benchmark the research code by comparing our fully 3D procedure to limiting cases described in terms of effectively 1D approaches."
Application of Bitcoin Data-Structures & Design Principles to Supply Chain Management,"  Heretofore the concept of ""blockchain"" has not been precisely defined. Accordingly the potential useful applications of this technology have been largely inflated. This work sidesteps the question of what constitutes a blockchain as such and focuses on the architectural components of the Bitcoin cryptocurrency, insofar as possible, in isolation. We consider common problems inherent in the design of effective supply chain management systems. With each identified problem we propose a solution that utilizes one or more component aspects of Bitcoin. This culminates in five design principles for increased efficiency in supply chain management systems through the application of incentive mechanisms and data structures native to the Bitcoin cryptocurrency protocol. "
Measuring Comodules and Enrichment,"  We study the existence of universal measuring comodules Q(M,N) for a pair of modules M,N in a braided monoidal closed category, and the associated enrichment of the global category of modules over the monoidal global category of comodules. In the process, we use results for general fibred adjunctions encompassing the fibred structure of modules over monoids and the opfibred structure of comodules over comonoids. We also explore applications to the theory of Hopf modules. "
Bond Graph Representation of Chemical Reaction Networks,  The Bond Graph approach and the Chemical Reaction Network approach to modelling biomolecular systems developed independently. This paper brings together the two approaches by providing a bond graph interpretation of the chemical reaction network concept of complexes. Both closed and open systems are discussed. The method is illustrated using a simple enzyme-catalysed reaction and a trans-membrane transporter. 
Skew Brownian motion with dry friction: The Pugachev-Sveshnikov approach,"  The Caughey-Dieness process, also known as the Brownian motion with two valued drift, is used in theoretical physics as an advanced model of the Brownian particle velocity if the resistant force is assumed to be dry friction. This process also appears in many other fields, such as applied physics, mechanics, astrophysics, and pure mathematics. In the present paper we are concerned with a more general process, skew Brownian motion with dry friction. The probability distribution of the process itself and of its occupation time on the positive half line are studied. The approach based on the Pugachev-Sveshnikov equation is used. "
Numerical modeling of laser-driven experiments aiming to demonstrate magnetic field amplification via turbulent dynamo,"  The universe is permeated by magnetic fields, with strengths ranging from a femtogauss in the voids between the filaments of galaxy clusters to several teragauss in black holes and neutron stars. The standard model behind cosmological magnetic fields is the nonlinear amplification of seed fields via turbulent dynamo to the values observed. We have conceived experiments that aim to demonstrate and study the turbulent dynamo mechanism in the laboratory. Here we describe the design of these experiments through simulation campaigns using FLASH, a highly capable radiation magnetohydrodynamics code that we have developed, and large-scale three-dimensional simulations on the Mira supercomputer at Argonne National Laboratory. The simulation results indicate that the experimental platform may be capable of reaching a turbulent plasma state and study dynamo amplification. We validate and compare our numerical results with a small subset of experimental data using synthetic diagnostics. "
The Minimum Distance Estimation with Multiple Integral in Panel Data,"  This paper studies the minimum distance estimation problem for panel data model. We propose the minimum distance estimators of regression parameters of the panel data model and investigate their asymptotic distributions. This paper contains two main contributions. First, the domain of application of the minimum distance estimation method is extended to the panel data model. Second, the proposed estimators are more efficient than other existing ones. Simulation studies compare performance of the proposed estimators with performance of others and demonstrate some superiority of our estimators. "
Towards Proving the Adversarial Robustness of Deep Neural Networks,"  Autonomous vehicles are highly complex systems, required to function reliably in a wide variety of situations. Manually crafting software controllers for these vehicles is difficult, but there has been some success in using deep neural networks generated using machine-learning. However, deep neural networks are opaque to human engineers, rendering their correctness very difficult to prove manually; and existing automated techniques, which were not designed to operate on neural networks, fail to scale to large systems. This paper focuses on proving the adversarial robustness of deep neural networks, i.e. proving that small perturbations to a correctly-classified input to the network cannot cause it to be misclassified. We describe some of our recent and ongoing work on verifying the adversarial robustness of networks, and discuss some of the open questions we have encountered and how they might be addressed. "
Deep learning-based numerical methods for high-dimensional parabolic partial differential equations and backward stochastic differential equations,"We propose a new algorithm for solving parabolic partial differential equations (PDEs) and backward stochastic differential equations (BSDEs) in high dimension, by making an analogy between the BSDE and reinforcement learning with the gradient of the solution playing the role of the policy function, and the loss function given by the error between the prescribed terminal condition and the solution of the BSDE. The policy function is then approximated by a neural network, as is done in deep reinforcement learning. Numerical results using TensorFlow illustrate the efficiency and accuracy of the proposed algorithms for several 100-dimensional nonlinear PDEs from physics and finance such as the Allen-Cahn equation, the Hamilton-Jacobi-Bellman equation, and a nonlinear pricing model for financial derivatives."
Robust Fusion Methods for Big Data,"  We address one of the important problems in Big Data, namely how to combine estimators from different subsamples by robust fusion procedures, when we are unable to deal with the whole sample. "
Reconstruction of Galaxy Star Formation Histories through SED Fitting: The Dense Basis Approach,"We introduce the Dense Basis method for Spectral Energy Distribution (SED) fitting. It accurately recovers traditional SED parameters, including M$_*$, SFR and dust attenuation, and reveals previously inaccessible information about the number and duration of star formation episodes and the timing of stellar mass assembly, as well as uncertainties in these quantities. This is done using basis Star Formation Histories (SFHs) chosen by comparing the goodness-of-fit of mock galaxy SEDs to the goodness-of-reconstruction of their SFHs. We train and validate the method using a sample of realistic SFHs at $z =1$ drawn from stochastic realisations, semi-analytic models, and a cosmological hydrodynamical galaxy formation simulation. The method is then applied to a sample of 1100 CANDELS GOODS-S galaxies at $1<z<1.5$ to illustrate its capabilities at moderate S/N with 15 photometric bands. Of the six parametrizations of SFHs considered, we adopt linear-exponential, bessel-exponential, lognormal and gaussian SFHs and reject the traditional parametrizations of constant (Top-Hat) and exponential SFHs. We quantify the bias and scatter of each parametrization. $15\%$ of galaxies in our CANDELS sample exhibit multiple episodes of star formation, with this fraction decreasing above $M_*>10^{9.5}M_\odot$. About $40\%$ of the CANDELS galaxies have SFHs whose maximum occurs at or near the epoch of observation. The Dense Basis method is scalable and offers a general approach to a broad class of data-science problems."
Two-stage multipolar ordering in Pr(TM)$_2$Al$_{20}$ Kondo materials,"Among heavy fermion materials, there is a set of rare-earth intermetallics with non-Kramers Pr$^{3+}$ $4f^2$ moments which exhibit a rich phase diagram with intertwined quadrupolar orders, superconductivity, and non-Fermi liquid behavior. However, more subtle broken symmetries such as multipolar orders in these Kondo materials remain poorly studied. Here, we argue that multi-spin interactions between local moments beyond the conventional two-spin exchange must play an important role in Kondo materials near the ordered to heavy Fermi liquid transition. We show that this drives a plethora of phases with coexisting multipolar orders and multiple thermal phase transitions, providing a natural framework for interpreting experiments on the Pr(TM)$_2$Al$_{20}$ class of compounds."
Spin-Momentum Locking in the Near Field of Metal Nanoparticles,"  Light carries both spin and momentum. Spin-orbit interactions of light come into play at the subwavelength scale of nano-optics and nano-photonics, where they determine the behaviour of light. These phenomena, in which the spin affects and controls the spatial degrees of freedom of light, are attracting rapidly growing interest. Here we present results on the spin-momentum locking in the near field of metal nanostructures supporting localized surface resonances. These systems can confine light to very small dimensions below the diffraction limit, leading to a striking near-field enhancement. In contrast to the propagating evanescent waves of surface plasmon-polariton modes, the electromagnetic near-field of localized surface resonances does not exhibit a definite position-independent momentum or polarization. Our results can be useful to investigate the spin-orbit interactions of light for complex evanescent fields. Note that the spin of the incident light can control the rotation direction of the canonical momentum. "
Persistence and extinction for stochastic ecological difference equations with feedbacks,"  Species' densities experience both internal frequency-dependent feedbacks due to population structure and external feedbacks from abiotic factors. These feedbacks can determine whether populations persist or go extinct, and may be subject to stochastic fluctuations. To provide a general mathematical framework for studying these effects, we develop theorems for stochastic persistence and exclusion for stochastic ecological difference equations accounting for feedbacks. Specifically, we use the stochastic analog of average Lyapunov functions to develop sufficient and necessary conditions for (i) all population densities spending little time at low densities i.e. stochastic persistence, and (ii) population trajectories asymptotically approaching the extinction set with positive probability. For (i) and (ii), respectively, we provide quantitative estimates on the fraction of time that the system is near the extinction set, and the probability of asymptotic extinction as a function of the initial state of the system. Furthermore, in the case of persistence, we provide lower bounds for the time to escape neighborhoods of the extinction set. To illustrate the applicability of our results, we analyze models of evolutionary games, stochastic Lotka-Volterra difference equations, trait evolution, and spatially structured disease dynamics. Our analysis of these models demonstrates environmental stochasticity facilitates coexistence in the hawk-dove game, but inhibits coexistence in the rock-paper-scissors game and Lotka-Volterra predator-prey model. Furthermore, environmental fluctuations with positive auto-correlations can promote persistence of evolving populations and disease persistence in patchy landscapes. While these results help close the gap between persistence theories for deterministic and stochastic systems, we conclude by highlighting several challenges for future research. "
Stochastic reachability of a target tube: Theory and computation,"  Given a discrete-time stochastic system and a time-varying sequence of target sets, we consider the problem of maximizing the probability of the state evolving within this tube under bounded control authority. This problem subsumes existing work on stochastic viability and terminal hitting-time stochastic reach-avoid problems. Of special interest is the stochastic reach set, the set of all initial states from which the probability of staying in the target tube is above a desired threshold. This set provides non-trivial information about the safety and the performance of the system. In this paper, we provide sufficient conditions under which the stochastic reach set is closed, compact, and convex. We also discuss an underapproximative interpolation technique for stochastic reach sets. Finally, we propose a scalable, grid-free, and anytime algorithm that computes a polytopic underapproximation of the stochastic reach set and synthesizes an open-loop controller using convex optimization. We demonstrate the efficacy and scalability of our approach over existing techniques using three numerical simulations --- stochastic viability of a chain of integrators, stochastic reach-avoid computation for a satellite rendezvous and docking problem, and stochastic reachability of a target tube for a Dubin's car with a known turning rate sequence. "
Bright soliton to quantum droplet transition in a mixture of Bose-Einstein condensates,"  Attractive Bose-Einstein condensates can host two types of macroscopic self-bound states of different nature: bright solitons and quantum liquid droplets. Here, we investigate the connection between them with a Bose-Bose mixture confined in an optical waveguide. We develop a simple theoretical model to show that, depending on atom number and interaction strength, solitons and droplets can be smoothly connected or remain distinct states coexisting only in a bi-stable region. We experimentally measure their spin composition, extract their density for a broad range of parameters and map out the boundary of the region separating solitons from droplets. "
Upper bounds for the spectral function on homogeneous spaces via volume growth,"We use spectral embeddings to give upper bounds on the spectral function of the Laplace--Beltrami operator on homogeneous spaces in terms of the volume growth of balls. In the case of compact manifolds, our bounds extend the 1980 lower bound of Peter Li for the smallest positive eigenvalue to all eigenvalues. We also improve Li's bound itself. Our bounds translate to explicit upper bounds on the heat kernel for both compact and noncompact homogeneous spaces."
An impossibility theorem for gerrymandering,"  The U.S. Supreme Court is currently deliberating over whether a proposed mathematical formula should be used to detect unconstitutional partisan gerrymandering. We show that in some cases, this formula will only flag bizarrely shaped districts as potentially constitutional. "
Cooling-Rate Effects in Sodium Silicate Glasses: Bridging the Gap between Molecular Dynamics Simulations and Experiments,"Although molecular dynamics (MD) simulations are commonly used to predict the structure and properties of glasses, they are intrinsically limited to short time scales, necessitating the use of fast cooling rates. It is therefore challenging to compare results from MD simulations to experimental results for glasses cooled on typical laboratory time scales. Based on MD simulations of a sodium silicate glass with varying cooling rate (from 0.01 to 100 K/ps), here we show that thermal history primarily affects the medium-range order structure, while the short-range order is largely unaffected over the range of cooling rates simulated. This results in a decoupling between the enthalpy and volume relaxation functions, where the enthalpy quickly plateaus as the cooling rate decreases, whereas density exhibits a slower relaxation. Finally, we demonstrate that the outcomes of MD simulations can be meaningfully compared to experimental values if properly extrapolated to slower cooling rates."
Representation Learning for Scale-free Networks,"  Network embedding aims to learn the low-dimensional representations of vertexes in a network, while structure and inherent properties of the network is preserved. Existing network embedding works primarily focus on preserving the microscopic structure, such as the first- and second-order proximity of vertexes, while the macroscopic scale-free property is largely ignored. Scale-free property depicts the fact that vertex degrees follow a heavy-tailed distribution (i.e., only a few vertexes have high degrees) and is a critical property of real-world networks, such as social networks. In this paper, we study the problem of learning representations for scale-free networks. We first theoretically analyze the difficulty of embedding and reconstructing a scale-free network in the Euclidean space, by converting our problem to the sphere packing problem. Then, we propose the ""degree penalty"" principle for designing scale-free property preserving network embedding algorithm: punishing the proximity between high-degree vertexes. We introduce two implementations of our principle by utilizing the spectral techniques and a skip-gram model respectively. Extensive experiments on six datasets show that our algorithms are able to not only reconstruct heavy-tailed distributed degree distribution, but also outperform state-of-the-art embedding models in various network mining tasks, such as vertex classification and link prediction. "
"Extractive Summarization: Limits, Compression, Generalized Model and Heuristics","Due to its promise to alleviate information overload, text summarization has attracted the attention of many researchers. However, it has remained a serious challenge. Here, we first prove empirical limits on the recall (and F1-scores) of extractive summarizers on the DUC datasets under ROUGE evaluation for both the single-document and multi-document summarization tasks. Next we define the concept of compressibility of a document and present a new model of summarization, which generalizes existing models in the literature and integrates several dimensions of the summarization, viz., abstractive versus extractive, single versus multi-document, and syntactic versus semantic. Finally, we examine some new and existing single-document summarization algorithms in a single framework and compare with state of the art summarizers on DUC data."
Backprop as Functor: A compositional perspective on supervised learning,"  A supervised learning algorithm searches over a set of functions $A \to B$ parametrised by a space $P$ to find the best approximation to some ideal function $f\colon A \to B$. It does this by taking examples $(a,f(a)) \in A\times B$, and updating the parameter according to some rule. We define a category where these update rules may be composed, and show that gradient descent---with respect to a fixed step size and an error function satisfying a certain property---defines a monoidal functor from a category of parametrised functions to this category of update rules. This provides a structural perspective on backpropagation, as well as a broad generalisation of neural networks. "
Entropic selection of concepts unveils hidden topics in documents corpora,"  The organization and evolution of science has recently become itself an object of scientific quantitative investigation, thanks to the wealth of information that can be extracted from scientific documents, such as citations between papers and co-authorship between researchers. However, only few studies have focused on the concepts that characterize full documents and that can be extracted and analyzed, revealing the deeper organization of scientific knowledge. Unfortunately, several concepts can be so common across documents that they hinder the emergence of the underlying topical structure of the document corpus, because they give rise to a large amount of spurious and trivial relations among documents. To identify and remove common concepts, we introduce a method to gauge their relevance according to an objective information-theoretic measure related to the statistics of their occurrence across the document corpus. After progressively removing concepts that, according to this metric, can be considered as generic, we find that the topic organization displays a correspondingly more refined structure. "
An Optimal Multi-layer Reinsurance Policy under Conditional Tail Expectation,"A usual reinsurance policy for insurance companies admits one or two layers of the payment deductions. Under optimal criterion of minimizing the conditional tail expectation (CTE) risk measure of the insurer's total risk, this article generalized an optimal stop-loss reinsurance policy to an optimal multi-layer reinsurance policy. To achieve such optimal multi-layer reinsurance policy, this article starts from a given optimal stop-loss reinsurance policy $f(\cdot).$ In the first step, it cuts down an interval $[0,\infty)$ into two intervals $[0,M_1)$ and $[M_1,\infty).$ By shifting the origin of Cartesian coordinate system to $(M_{1},f(M_{1})),$ and showing that under the $CTE$ criteria $f(x)I_{[0, M_1)}(x)+(f(M_1)+f(x-M_1))I_{[M_1,\infty)}(x)$ is, again, an optimal policy. This extension procedure can be repeated to obtain an optimal k-layer reinsurance policy. Finally, unknown parameters of the optimal multi-layer reinsurance policy are estimated using some additional appropriate criteria. Three simulation-based studies have been conducted to demonstrate: ({\bf 1}) The practical applications of our findings and ({\bf 2}) How one may employ other appropriate criteria to estimate unknown parameters of an optimal multi-layer contract. The multi-layer reinsurance policy, similar to the original stop-loss reinsurance policy is optimal, in a same sense. Moreover it has some other optimal criteria which the original policy does not have. Under optimal criterion of minimizing general translative and monotone risk measure $\rho(\cdot)$ of {\it either} the insurer's total risk {\it or} both the insurer's and the reinsurer's total risks, this article (in its discussion) also extends a given optimal reinsurance contract $f(\cdot)$ to a multi-layer and continuous reinsurance policy."
"Tusnády's problem, the transference principle, and non-uniform QMC sampling","It is well-known that for every $N \geq 1$ and $d \geq 1$ there exist point sets $x_1, \dots, x_N \in [0,1]^d$ whose discrepancy with respect to the Lebesgue measure is of order at most $(\log N)^{d-1} N^{-1}$. In a more general setting, the first author proved together with Josef Dick that for any normalized measure $\mu$ on $[0,1]^d$ there exist points $x_1, \dots, x_N$ whose discrepancy with respect to $\mu$ is of order at most $(\log N)^{(3d+1)/2} N^{-1}$. The proof used methods from combinatorial mathematics, and in particular a result of Banaszczyk on balancings of vectors. In the present note we use a version of the so-called transference principle together with recent results on the discrepancy of red-blue colorings to show that for any $\mu$ there even exist points having discrepancy of order at most $(\log N)^{d-\frac12} N^{-1}$, which is almost as good as the discrepancy bound in the case of the Lebesgue measure."
HBT+: an improved code for finding subhalos and building merger trees in cosmological simulations,"  Dark matter subhalos are the remnants of (incomplete) halo mergers. Identifying them and establishing their evolutionary links in the form of merger trees is one of the most important applications of cosmological simulations. The Hierachical Bound-Tracing (HBT) code identifies halos as they form and tracks their evolution as they merge, simultaneously detecting subhalos and building their merger trees. Here we present a new implementation of this approach, HBT+, that is much faster, more user friendly, and more physically complete than the original code. Applying HBT+ to cosmological simulations we show that both the subhalo mass function and the peak-mass function are well fit by similar double-Schechter functions.The ratio between the two is highest at the high mass end, reflecting the resilience of massive subhalos that experience substantial dynamical friction but limited tidal stripping. The radial distribution of the most massive subhalos is more concentrated than the universal radial distribution of lower mass subhalos. Subhalo finders that work in configuration space tend to underestimate the masses of massive subhalos, an effect that is stronger in the host centre. This may explain, at least in part, the excess of massive subhalos in galaxy cluster centres inferred from recent lensing observations. We demonstrate that the peak-mass function is a powerful diagnostic of merger tree defects, and the merger trees constructed using HBT+ do not suffer from the missing or switched links that tend to afflict merger trees constructed from more conventional halo finders. We make the HBT+ code publicly available. "
On Statistical Non-Significance,"  Significance tests are probably the most extended form of inference in empirical research, and significance is often interpreted as providing greater informational content than non-significance. In this article we show, however, that rejection of a point null often carries very little information, while failure to reject may be highly informative. This is particularly true in empirical contexts where data sets are large and where there are rarely reasons to put substantial prior probability on a point null. Our results challenge the usual practice of conferring point null rejections a higher level of scientific significance than non-rejections. In consequence, we advocate a visible reporting and discussion of non-significant results in empirical practice. "
On the Interplay between Strong Regularity and Graph Densification,"  In this paper we analyze the practical implications of Szemerédi's regularity lemma in the preservation of metric information contained in large graphs. To this end, we present a heuristic algorithm to find regular partitions. Our experiments show that this method is quite robust to the natural sparsification of proximity graphs. In addition, this robustness can be enforced by graph densification. "
Nonabelian Landau-Ginzburg orbifolds and Calabi-Yau/Landau-Ginzburg correspondence,"  In this paper, we study the bigraded vector space structure of Landau-Ginzburg orbifolds. We prove the formula for the generating function of the Hodge numbers of possibly nonabelian Landau-Ginzburg orbifolds. As an application, we calculate the Hodge numbers for all nondegenerate quintic homogeneous polynomials with five variables. These results yield an evidence for the Calabi-Yau/Landau-Ginzburg correspondence between the Calabi-Yau geometries and the Landau-Ginzburg B-models. "
Setpoint Tracking with Partially Observed Loads,"  We use online convex optimization (OCO) for setpoint tracking with uncertain, flexible loads. We consider full feedback from the loads, bandit feedback, and two intermediate types of feedback: partial bandit where a subset of the loads are individually observed and the rest are observed in aggregate, and Bernoulli feedback where in each round the aggregator receives either full or bandit feedback according to a known probability. We give sublinear regret bounds in all cases. We numerically evaluate our algorithms on examples with thermostatically controlled loads and electric vehicles. "
Size effect in the ionization energy of PAH clusters,"We report the first experimental measurement of the near-threshold photo-ionization spectra of polycyclic aromatic hydrocarbon clusters made of pyrene C16H10 and coronene C24H12, obtained using imaging photoelectron photoion coincidence spectrometry with a VUV synchrotron beamline. The experimental results of the ionization energy are confronted to calculated ones obtained from simulations using dedicated electronic structure treatment for large ionized molecular clusters. Experiment and theory consistently find a decrease of the ionization energy with cluster size. The inclusion of temperature effects in the simulations leads to a lowering of this energy and to a quantitative agreement with the experiment. In the case of pyrene, both theory and experiment show a discontinuity in the IE trend for the hexamer."
Runaway Feedback Loops in Predictive Policing,"  Predictive policing systems are increasingly used to determine how to allocate police across a city in order to best prevent crime. Discovered crime data (e.g., arrest counts) are used to help update the model, and the process is repeated. Such systems have been empirically shown to be susceptible to runaway feedback loops, where police are repeatedly sent back to the same neighborhoods regardless of the true crime rate. In response, we develop a mathematical model of predictive policing that proves why this feedback loop occurs, show empirically that this model exhibits such problems, and demonstrate how to change the inputs to a predictive policing system (in a black-box manner) so the runaway feedback loop does not occur, allowing the true crime rate to be learned. Our results are quantitative: we can establish a link (in our model) between the degree to which runaway feedback causes problems and the disparity in crime rates between areas. Moreover, we can also demonstrate the way in which \emph{reported} incidents of crime (those reported by residents) and \emph{discovered} incidents of crime (i.e. those directly observed by police officers dispatched as a result of the predictive policing algorithm) interact: in brief, while reported incidents can attenuate the degree of runaway feedback, they cannot entirely remove it without the interventions we suggest. "
AWAKE-related benchmarking tests for simulation codes,  Two tests are described that were developed for benchmarking and comparison of numerical codes in the context of AWAKE experiment. 
Ricci-flat metrics on the cone over $\mathbb{CP}^2 \# \overline{\mathbb{CP}^2}$,We describe a framework for constructing the Ricci-flat metrics on the total space of the canonical bundle over $\mathbb{CP}^2 \# \overline{\mathbb{CP}^2}$ (the del Pezzo surface of rank one). We construct explicitly the first-order deformation of the so-called `orthotoric metric' on this manifold. We also show that the deformation of the corresponding conformal Killing-Yano form does not exist.
Solving Zero-sum Games using Best Response Oracles with Applications to Search Games,"  We present efficient algorithms for computing optimal or approximately optimal strategies in a zero-sum game for which Player I has n pure strategies and Player II has an arbitrary number of pure strategies. We assume that for any given mixed strategy of Player I, a best response or ""approximate"" best response of Player II can be found by an oracle in time polynomial in n. We then show how our algorithms may be applied to several search games with applications to security and counter-terrorism. We evaluate our main algorithm experimentally on a prototypical search game. Our results show it performs well compared to an existing, well-known algorithm for solving zero-sum games that can also be used to solve search games, given a best response oracle. "
On the idea of a new artificial intelligence based optimization algorithm inspired from the nature of vortex,"  In this paper, the idea of a new artificial intelligence based optimization algorithm, which is inspired from the nature of vortex, has been provided briefly. As also a bio-inspired computation algorithm, the idea is generally focused on a typical vortex flow / behavior in nature and inspires from some dynamics that are occurred in the sense of vortex nature. Briefly, the algorithm is also a swarm-oriented evolutional problem solution approach; because it includes many methods related to elimination of weak swarm members and trying to improve the solution process by supporting the solution space via new swarm members. In order have better idea about success of the algorithm; it has been tested via some benchmark functions. At this point, the obtained results show that the algorithm can be an alternative to the literature in terms of single-objective optimization solution ways. Vortex Optimization Algorithm (VOA) is the name suggestion by the authors; for this new idea of intelligent optimization approach. "
Sulfur Hazes in Giant Exoplanet Atmospheres: Impacts on Reflected Light Spectra,"Recent work has shown that sulfur hazes may arise in the atmospheres of some giant exoplanets due to the photolysis of H$_{2}$S. We investigate the impact such a haze would have on an exoplanet's geometric albedo spectrum and how it may affect the direct imaging results of WFIRST, a planned NASA space telescope. For temperate (250 K $<$ T$_{\rm eq}$ $<$ 700 K) Jupiter--mass planets, photochemical destruction of H$_{2}$S results in the production of $\sim$1 ppmv of \seight between 100 and 0.1 mbar, which, if cool enough, will condense to form a haze. Nominal haze masses are found to drastically alter a planet's geometric albedo spectrum: whereas a clear atmosphere is dark at wavelengths between 0.5 and 1 $\mu$m due to molecular absorption, the addition of a sulfur haze boosts the albedo there to $\sim$0.7 due to scattering. Strong absorption by the haze shortward of 0.4 $\mu$m results in albedos $<$0.1, in contrast to the high albedos produced by Rayleigh scattering in a clear atmosphere. As a result, the color of the planet shifts from blue to orange. The existence of a sulfur haze masks the molecular signatures of methane and water, thereby complicating the characterization of atmospheric composition. Detection of such a haze by WFIRST is possible, though discriminating between a sulfur haze and any other highly reflective, high altitude scatterer will require observations shortward of 0.4 $\mu$m, which is currently beyond WFIRST's design."
Real-time fMRI neurofeedback training of the amygdala activity with simultaneous EEG in veterans with combat-related PTSD,"Posttraumatic stress disorder (PTSD) is a chronic and disabling neuropsychiatric disorder characterized by insufficient top-down modulation of the amygdala activity by the prefrontal cortex. Real-time fMRI neurofeedback (rtfMRI-nf) is an emerging method with potential for modifying the amygdala-prefrontal interactions. We report the first controlled emotion self-regulation study in veterans with combat-related PTSD utilizing rtfMRI-nf of the amygdala activity. PTSD patients in the experimental group (EG, n=20) learned to upregulate BOLD activity of the left amygdala (LA) using rtfMRI-nf during a happy emotion induction task. PTSD patients in the control group (CG, n=11) were provided with a sham rtfMRI-nf. The study included three rtfMRI-nf training sessions, and EEG recordings were performed simultaneously with fMRI. PTSD severity was assessed using the Clinician-Administered PTSD Scale (CAPS). The EG participants showed a significant reduction in total CAPS ratings, including significant reductions in avoidance and hyperarousal symptoms. Overall, 80% of the EG participants demonstrated clinically meaningful reductions in CAPS ratings, compared to 38% in the CG. During the first session, fMRI connectivity of the LA with the orbitofrontal cortex and the dorsolateral prefrontal cortex (DLPFC) was progressively enhanced, and this enhancement significantly and positively correlated with initial CAPS ratings. Left-lateralized enhancement in upper alpha EEG coherence also exhibited a significant positive correlation with the initial CAPS. Reduction in PTSD severity between the first and last rtfMRI-nf sessions significantly correlated with enhancement in functional connectivity between the LA and the left DLPFC. Our results demonstrate that the rtfMRI-nf of the amygdala activity has the potential to correct the amygdala-prefrontal functional connectivity deficiencies specific to PTSD."
Approximations from Anywhere and General Rough Sets,"  Not all approximations arise from information systems. The problem of fitting approximations, subjected to some rules (and related data), to information systems in a rough scheme of things is known as the \emph{inverse problem}. The inverse problem is more general than the duality (or abstract representation) problems and was introduced by the present author in her earlier papers. From the practical perspective, a few (as opposed to one) theoretical frameworks may be suitable for formulating the problem itself. \emph{Granular operator spaces} have been recently introduced and investigated by the present author in her recent work in the context of antichain based and dialectical semantics for general rough sets. The nature of the inverse problem is examined from number-theoretic and combinatorial perspectives in a higher order variant of granular operator spaces and some necessary conditions are proved. The results and the novel approach would be useful in a number of unsupervised and semi supervised learning contexts and algorithms. "
The minimal volume of simplices containing a convex body,"Let $K \subset \mathbb R^n$ be a convex body with barycenter at the origin. We show there is a simplex $S \subset K$ having also barycenter at the origin such that $\left(\frac{vol(S)}{vol(K)}\right)^{1/n} \geq \frac{c}{\sqrt{n}},$ where $c>0$ is an absolute constant. This is achieved using stochastic geometric techniques. Precisely, if $K$ is in isotropic position, we present a method to find centered simplices verifying the above bound that works with very high probability. As a consequence, we provide correct asymptotic estimates on an old problem in convex geometry. Namely, we show that the simplex $S_{min}(K)$ of minimal volume enclosing a given convex body $K \subset \mathbb R^n$, fulfills the following inequality $$\left(\frac{vol(S_{min}(K))}{vol(K)}\right)^{1/n} \leq d \sqrt{n},$$ for some absolute constant $d>0$. Up to the constant, the estimate cannot be lessened."
Socioeconomic bias in influenza surveillance,"Individuals in low socioeconomic brackets are considered at-risk for developing influenza-related complications and often exhibit higher than average influenza-related hospitalization rates. This disparity has been attributed to various factors, including restricted access to preventative and therapeutic health care, limited sick leave, and household structure. Adequate influenza surveillance in these at-risk populations is a critical precursor to accurate risk assessments and effective intervention. However, the United States of America's primary national influenza surveillance system (ILINet) monitors outpatient healthcare providers, which may be largely inaccessible to lower socioeconomic populations. Recent initiatives to incorporate internet-source and hospital electronic medical records data into surveillance systems seek to improve the timeliness, coverage, and accuracy of outbreak detection and situational awareness. Here, we use a flexible statistical framework for integrating multiple surveillance data sources to evaluate the adequacy of traditional (ILINet) and next generation (BioSense 2.0 and Google Flu Trends) data for situational awareness of influenza across poverty levels. We find that zip codes in the highest poverty quartile are a critical blind-spot for ILINet that the integration of next generation data fails to ameliorate."
Java Code Analysis and Transformation into AWS Lambda Functions,"  Software developers are faced with the issue of either adapting their programming model to the execution model (e.g. cloud platforms) or finding appropriate tools to adapt the model and code automatically. A recent execution model which would benefit from automated enablement is Function-as-a-Service. Automating this process requires a pipeline which includes steps for code analysis, transformation and deployment. In this paper, we outline the design and runtime characteristics of Podilizer, a tool which implements the pipeline specifically for Java source code as input and AWS Lambda as output. We contribute technical and economic metrics about this concrete 'FaaSification' process by observing the behaviour of Podilizer with two representative Java software projects. "
Criteria for Finite Difference Groebner Bases of Normal Binomial Difference Ideals,"  In this paper, we give decision criteria for normal binomial difference polynomial ideals in the univariate difference polynomial ring F{y} to have finite difference Groebner bases and an algorithm to compute the finite difference Groebner bases if these criteria are satisfied. The novelty of these criteria lies in the fact that complicated properties about difference polynomial ideals are reduced to elementary properties of univariate polynomials in Z[x]. "
$p$-Euler equations and $p$-Navier-Stokes equations,"We propose in this work new systems of equations which we call $p$-Euler equations and $p$-Navier-Stokes equations. $p$-Euler equations are derived as the Euler-Lagrange equations for the action represented by the Benamou-Brenier characterization of Wasserstein-$p$ distances, with incompressibility constraint. $p$-Euler equations have similar structures with the usual Euler equations but the `momentum' is the signed ($p-1$)-th power of the velocity. In the 2D case, the $p$-Euler equations have streamfunction-vorticity formulation, where the vorticity is given by the $p$-Laplacian of the streamfunction. By adding diffusion presented by $\gamma$-Laplacian of the velocity, we obtain what we call $p$-Navier-Stokes equations. If $\gamma=p$, the {\it a priori} energy estimates for the velocity and momentum have dual symmetries. Using these energy estimates and a time-shift estimate, we show the global existence of weak solutions for the $p$-Navier-Stokes equations in $\mathbb{R}^d$ for $\gamma=p$ and $p\ge d\ge 2$ through a compactness criterion."
Unconventional minimal subtraction and Bogoliubov-Parasyuk-Hepp-Zimmermann: massive scalar theory and critical exponents,"We introduce a simpler although unconventional minimal subtraction renormalization procedure in the case of a massive scalar $\lambda \phi^{4}$ theory in Euclidean space using dimensional regularization. We show that this method is very similar to its counterpart in massless field theory. In particular, the choice of using the bare mass at higher perturbative order instead of employing its tree-level counterpart eliminates all tadpole insertions at that order. As an application, we compute diagrammatically the critical exponents $\eta$ and $\nu$ at least up to two loops. We perform an explicit comparison with the Bogoliubov-Parasyuk-Hepp-Zimmermann ($BPHZ$) method at the same loop order, show that the proposed method requires fewer diagrams and establish a connection between the two approaches."
A covering theorem for singular measures in the Euclidean space,"We prove that for any singular measure $\mu$ on $\mathbb{R}^n$ it is possible to cover $\mu$-almost every point with $n$ families of Lipschitz slabs of arbitrarily small total width. More precisely, up to a rotation, for every $\delta>0$ there are $n$ countable families of $1$-Lipschitz functions $\{f_i^1\}_{i\in\mathbb{N}},\ldots, \{f_i^n\}_{i\in\mathbb{N}},$ $f_i^j:\{x_j=0\}\subset\mathbb{R}^n\to\mathbb{R}$, and $n$ sequences of positive real numbers $\{\varepsilon_i^1\}_{i\in\mathbb{N}},\ldots, \{\varepsilon_i^n\}_{i\in\mathbb{N}}$ such that, denoting $\hat x_j$ the orthogonal projection of the point $x$ onto $\{x_j=0\}$ and $$I_i^j:=\{x=(x_1,\ldots,x_n)\in \mathbb{R}^n:f_i^j(\hat x_j)-\varepsilon_i^j< x_j< f_i^j(\hat x_j)+\varepsilon_i^j\},$$ it holds $\sum_{i,j}\varepsilon_i^j\leq \delta$ and $\mu(\mathbb{R}^n\setminus\bigcup_{i,j}I_i^j)=0.$ We apply this result to show that, if $\mu$ is not absolutely continuous, it is possible to approximate the identity with a sequence $g_h$ of smooth equi-Lipschitz maps satisfying $$\limsup_{h\to\infty}\int_{\mathbb{R}^n}{\rm{det}}(\nabla g_h) d\mu<\mu(\mathbb{R}^n).$$ From this, we deduce a simple proof of the fact that every top-dimensional Ambrosio-Kirchheim metric current in $\mathbb{R}^n$ is a Federer-Fleming flat chain."
Linear-Time Algorithms for Maximum-Weight Induced Matchings and Minimum Chain Covers in Convex Bipartite Graphs,"A bipartite graph $G=(U,V,E)$ is convex if the vertices in $V$ can be linearly ordered such that for each vertex $u\in U$, the neighbors of $u$ are consecutive in the ordering of $V$. An induced matching $H$ of $G$ is a matching such that no edge of $E$ connects endpoints of two different edges of $H$. We show that in a convex bipartite graph with $n$ vertices and $m$ weighted edges, an induced matching of maximum total weight can be computed in $O(n+m)$ time. An unweighted convex bipartite graph has a representation of size $O(n)$ that records for each vertex $u\in U$ the first and last neighbor in the ordering of $V$. Given such a compact representation, we compute an induced matching of maximum cardinality in $O(n)$ time. In convex bipartite graphs, maximum-cardinality induced matchings are dual to minimum chain covers. A chain cover is a covering of the edge set by chain subgraphs, that is, subgraphs that do not contain induced matchings of more than one edge. Given a compact representation, we compute a representation of a minimum chain cover in $O(n)$ time. If no compact representation is given, the cover can be computed in $O(n+m)$ time. All of our algorithms achieve optimal running time for the respective problem and model. Previous algorithms considered only the unweighted case, and the best algorithm for computing a maximum-cardinality induced matching or a minimum chain cover in a convex bipartite graph had a running time of $O(n^2)$."
Enhanced steady-state dissolution flux in reactive convective dissolution,"Chemical reactions can accelerate, slow down or even be at the very origin of the development of dissolution-driven convection in partially miscible stratifications, when they impact the density profile in the host fluid phase. We numerically analyze the dynamics of this reactive convective dissolution in the fully developed non-linear regime for a phase A dissolving into a host layer containing a dissolved reactant B. We show that for a general A+B$\rightarrow$C reaction in solution, the dynamics vary with the Rayleigh numbers of the chemical species, i.e. with the nature of the chemicals in the host phase. Depending on whether the reaction slows down, accelerates or is at the origin of the development of convection, the spatial distributions of species A, B or C, the dissolution flux and the reaction rate are different. We show that chemical reactions enhance the steady-state flux as they consume A and can induce more intense convection than in the absence of reactions. This result is important in the context of CO$_2$ geological sequestration where quantifying the storage rate of CO$_2$ dissolving into the host oil or aqueous phase is crucial to assess the efficiency and the safety of the project."
Vapor Condensed and Supercooled Glassy Nanoclusters,"  We use molecular simulation to study the structural and dynamic properties of glassy nanoclusters formed both through the direct condensation of the vapor below the glass transition temperature, without the presence of a substrate, and \textit{via} the slow supercooling of unsupported liquid nanodroplets. An analysis of local structure using Voronoi polyhedra shows that the energetic stability of the clusters is characterized by a large, increasing fraction of bicapped square antiprism motifs. We also show that nanoclusters with similar inherent structure energies are structurally similar, independent of their history, which suggests the supercooled clusters access the same low energy regions of the potential energy landscape as the vapor condensed clusters despite their different methods of formation. By measuring the intermediate scattering function at different radii from the cluster center, we find that the relaxation dynamics of the clusters are inhomogeneous, with the core becoming glassy above the glass transition temperature while the surface remains mobile at low temperatures. This helps the clusters sample the highly stable, low energy structures on the potential energy surface. Our work suggests the nanocluster systems are structurally more stable than the ultra-stable glassy thin films, formed through vapor deposition onto a cold substrate, but the nanoclusters do not exhibit the superheating effects characteristic of the ultra-stable glass states. "
The Dayenu Boolean Function Is Almost Always True!,"The Boolean function implicit in the famous Dayenu song, sung at the Passover meal, is expressed in full conjunctive normal form, and it is proved that if there are n miracles the number of truth-vectors satisfying it is $2^n -(n+1)$."
Autocalibrating and Calibrationless Parallel Magnetic Resonance Imaging as a Bilinear Inverse Problem,"  Modern reconstruction methods for magnetic resonance imaging (MRI) exploit the spatially varying sensitivity profiles of receive-coil arrays as additional source of information. This allows to reduce the number of time-consuming Fourier-encoding steps by undersampling. The receive sensitivities are a priori unknown and influenced by geometry and electric properties of the (moving) subject. For optimal results, they need to be estimated jointly with the image from the same undersampled measurement data. Formulated as an inverse problem, this leads to a bilinear reconstruction problem related to multi-channel blind deconvolution. In this work, we will discuss some recently developed approaches for the solution of this problem. "
"Friend, Collaborator, Student, Manager: How Design of an AI-Driven Game Level Editor Affects Creators","Machine learning advances have afforded an increase in algorithms capable of creating art, music, stories, games, and more. However, it is not yet well-understood how machine learning algorithms might best collaborate with people to support creative expression. To investigate how practicing designers perceive the role of AI in the creative process, we developed a game level design tool for Super Mario Bros.-style games with a built-in AI level designer. In this paper we discuss our design of the Morai Maker intelligent tool through two mixed-methods studies with a total of over one-hundred participants. Our findings are as follows: (1) level designers vary in their desired interactions with, and role of, the AI, (2) the AI prompted the level designers to alter their design practices, and (3) the level designers perceived the AI as having potential value in their design practice, varying based on their desired role for the AI."
Approximation Algorithms for Barrier Sweep Coverage,"Time-varying coverage, namely sweep coverage is a recent development in the area of wireless sensor networks, where a small number of mobile sensors sweep or monitor comparatively large number of locations periodically. In this article we study barrier sweep coverage with mobile sensors where the barrier is considered as a finite length continuous curve on a plane. The coverage at every point on the curve is time-variant. We propose an optimal solution for sweep coverage of a finite length continuous curve. Usually energy source of a mobile sensor is battery with limited power, so energy restricted sweep coverage is a challenging problem for long running applications. We propose an energy restricted sweep coverage problem where every mobile sensors must visit an energy source frequently to recharge or replace its battery. We propose a $\frac{13}{3}$-approximation algorithm for this problem. The proposed algorithm for multiple curves achieves the best possible approximation factor 2 for a special case. We propose a 5-approximation algorithm for the general problem. As an application of the barrier sweep coverage problem for a set of line segments, we formulate a data gathering problem. In this problem a set of mobile sensors is arbitrarily monitoring the line segments one for each. A set of data mules periodically collects the monitoring data from the set of mobile sensors. We prove that finding the minimum number of data mules to collect data periodically from every mobile sensor is NP-hard and propose a 3-approximation algorithm to solve it."
Reinforcement Learning via Recurrent Convolutional Neural Networks,"Deep Reinforcement Learning has enabled the learning of policies for complex tasks in partially observable environments, without explicitly learning the underlying model of the tasks. While such model-free methods achieve considerable performance, they often ignore the structure of task. We present a natural representation of to Reinforcement Learning (RL) problems using Recurrent Convolutional Neural Networks (RCNNs), to better exploit this inherent structure. We define 3 such RCNNs, whose forward passes execute an efficient Value Iteration, propagate beliefs of state in partially observable environments, and choose optimal actions respectively. Backpropagating gradients through these RCNNs allows the system to explicitly learn the Transition Model and Reward Function associated with the underlying MDP, serving as an elegant alternative to classical model-based RL. We evaluate the proposed algorithms in simulation, considering a robot planning problem. We demonstrate the capability of our framework to reduce the cost of replanning, learn accurate MDP models, and finally re-plan with learnt models to achieve near-optimal policies."
Evaluating Deep Convolutional Neural Networks for Material Classification,"Determining the material category of a surface from an image is a demanding task in perception that is drawing increasing attention. Following the recent remarkable results achieved for image classification and object detection utilising Convolutional Neural Networks (CNNs), we empirically study material classification of everyday objects employing these techniques. More specifically, we conduct a rigorous evaluation of how state-of-the art CNN architectures compare on a common ground over widely used material databases. Experimental results on three challenging material databases show that the best performing CNN architectures can achieve up to 94.99\% mean average precision when classifying materials."
Localization of JPEG double compression through multi-domain convolutional neural networks,"  When an attacker wants to falsify an image, in most of cases she/he will perform a JPEG recompression. Different techniques have been developed based on diverse theoretical assumptions but very effective solutions have not been developed yet. Recently, machine learning based approaches have been started to appear in the field of image forensics to solve diverse tasks such as acquisition source identification and forgery detection. In this last case, the aim ahead would be to get a trained neural network able, given a to-be-checked image, to reliably localize the forged areas. With this in mind, our paper proposes a step forward in this direction by analyzing how a single or double JPEG compression can be revealed and localized using convolutional neural networks (CNNs). Different kinds of input to the CNN have been taken into consideration, and various experiments have been carried out trying also to evidence potential issues to be further investigated. "
From CDF to PDF --- A Density Estimation Method for High Dimensional Data,"CDF2PDF is a method of PDF estimation by approximating CDF. The original idea of it was previously proposed in [1] called SIC. However, SIC requires additional hyper-parameter tunning, and no algorithms for computing higher order derivative from a trained NN are provided in [1]. CDF2PDF improves SIC by avoiding the time-consuming hyper-parameter tuning part and enabling higher order derivative computation to be done in polynomial time. Experiments of this method for one-dimensional data shows promising results."
Spectrally-normalized margin bounds for neural networks,"This paper presents a margin-based multiclass generalization bound for neural networks that scales with their margin-normalized ""spectral complexity"": their Lipschitz constant, meaning the product of the spectral norms of the weight matrices, times a certain correction factor. This bound is empirically investigated for a standard AlexNet network trained with SGD on the mnist and cifar10 datasets, with both original and random labels; the bound, the Lipschitz constants, and the excess risks are all in direct correlation, suggesting both that SGD selects predictors whose complexity scales with the difficulty of the learning task, and secondly that the presented bound is sensitive to this complexity."
Fully Coupled Simulation of Cosmic Reionization. III. Stochastic Early Reionization by the Smallest Galaxies,"Previously we identified a new class of early galaxy that we estimate contributes up to 30\% of the ionizing photons responsible for reionization. These are low mass halos in the range $M_h =10^{6.5}-10^{8} M_{\odot}$ that have been chemically enriched by supernova ejecta from prior Pop III star formation. Despite their low star formation rates, these Metal Cooling halos (MCs) are significant sources of ionizing radiation, especially at the onset of reionization, due to their high number density and ionizing escape fractions. Here we present a fully-coupled radiation hydrodynamic simulation of reionization that includes these MCs as well the more massive hydrogen atomic line cooling halos. Our method is novel: we perform halo finding inline with the radiation hydrodynamical simulation, and assign escaping ionizing fluxes to halos using a probability distribution function (PDF) measured from the galaxy-resolving Renaissance Simulations. The PDF captures the mass dependence of the ionizing escape fraction as well as the probability that a halo is actively forming stars. With MCs, reionization starts earlier than if only halos of $10^8 M_{\odot}$ and above are included, however the redshift when reionization completes is only marginally affected as this is driven by more massive galaxies. Because star formation is intermittent in MCs, the earliest phase of reionization exhibits a stochastic nature, with small H II regions forming and recombining. Only later, once halos of mass $\sim 10^9 M_{\odot}$ and above begin to dominate the ionizing emissivity, does reionization proceed smoothly in the usual manner deduced from previous studies. This occurs at $z\approx 10$ in our simulation."
Open Source Dataset and Deep Learning Models for Online Digit Gesture Recognition on Touchscreens,"This paper presents an evaluation of deep neural networks for recognition of digits entered by users on a smartphone touchscreen. A new large dataset of Arabic numerals was collected for training and evaluation of the network. The dataset consists of spatial and temporal touch data recorded for 80 digits entered by 260 users. Two neural network models were investigated. The first model was a 2D convolutional neural (ConvNet) network applied to bitmaps of the glpyhs created by interpolation of the sensed screen touches and its topology is similar to that of previously published models for offline handwriting recognition from scanned images. The second model used a 1D ConvNet architecture but was applied to the sequence of polar vectors connecting the touch points. The models were found to provide accuracies of 98.50% and 95.86%, respectively. The second model was much simpler, providing a reduction in the number of parameters from 1,663,370 to 287,690. The dataset has been made available to the community as an open source resource."
On Counting Perfect Matchings in General Graphs,"Counting perfect matchings has played a central role in the theory of counting problems. The permanent, corresponding to bipartite graphs, was shown to be #P-complete to compute exactly by Valiant (1979), and a fully polynomial randomized approximation scheme (FPRAS) was presented by Jerrum, Sinclair, and Vigoda (2004) using a Markov chain Monte Carlo (MCMC) approach. However, it has remained an open question whether there exists an FPRAS for counting perfect matchings in general graphs. In fact, it was unresolved whether the same Markov chain defined by JSV is rapidly mixing in general. In this paper, we show that it is not. We prove torpid mixing for any weighting scheme on hole patterns in the JSV chain. As a first step toward overcoming this obstacle, we introduce a new algorithm for counting matchings based on the Gallai-Edmonds decomposition of a graph, and give an FPRAS for counting matchings in graphs that are sufficiently close to bipartite. In particular, we obtain a fixed-parameter tractable algorithm for counting matchings in general graphs, parameterized by the greatest ""order"" of a factor-critical subgraph."
Density and spin modes in imbalanced normal Fermi gases from collisionless to hydrodynamic regime,"  We study mass and population imbalance effect on density (in-phase) and spin (out-of-phase) collective modes in a two-component normal Fermi gas. By calculating eigenmodes of the linearized Boltzmann equation as well as the density/spin dynamic structure factor, we show that mass and population imbalance effects offer a variety of collective mode crossover behaviors from collisionless to hydrodynamic regimes. The mass imbalance effect shifts the crossover regime to the higher-temperature, and a significant peak of the spin dynamic structure factor emerges only in the collisionless regime. This is in contrast to the case of mass and population balanced normal Fermi gases, where the spin dynamic response is always absent. Although the population imbalance effect does not shift the crossover regime, the spin dynamic structure factor survives both in the collisionless and hydrodynamic regimes. "
Bounds on parameters of minimally non-linear patterns,"Let $ex(n, P)$ be the maximum possible number of ones in any 0-1 matrix of dimensions $n \times n$ that avoids $P$. Matrix $P$ is called minimally non-linear if $ex(n, P) = \omega(n)$ but $ex(n, P') = O(n)$ for every strict subpattern $P'$ of $P$. We prove that the ratio between the length and width of any minimally non-linear 0-1 matrix is at most $4$, and that a minimally non-linear 0-1 matrix with $k$ rows has at most $5k-3$ ones. We also obtain an upper bound on the number of minimally non-linear 0-1 matrices with $k$ rows. In addition, we prove corresponding bounds for minimally non-linear ordered graphs. The minimal non-linearity that we investigate for ordered graphs is for the extremal function $ex_{<}(n, G)$, which is the maximum possible number of edges in any ordered graph on $n$ vertices with no ordered subgraph isomorphic to $G$."
The sum of multidimensional divisor function over values of quadratic polynomial,"Let $F({\bf x})={\bf x}^tQ_m{\bf x}+\mathbf{b}^t{\bf x}+c\in\mathbb{Z}[{\bf x}]$ be a quadratic polynomial in $\ell (\ge 3 )$ variables ${\bf x} =(x_{1},...,x_{\ell})$, where $F({\bf x})$ is positive when ${\bf x}\in\mathbb{R}_{\ge 1}^{\ell}$, $Q_m\in {\rm M}_{\ell}(\mathbb{Z})$ is an $\ell\times\ell$ matrix and its discriminant $\det\left(Q_m^t+Q_m\right)\neq 0$. It gives explicit asymptotic formulas for the following sum \[ T_{k,F}(X)=\sum_{{\bf x}\in [1,X]^{\ell}\cap\mathbb{Z}^{\ell}}\tau_{k}\left(F({\bf x})\right) \] with the help of the circle method. Here $\tau_{k}(n)=\#\{(x_1,x_2,...,x_{k})\in\mathbb{N}^{k}: n=x_1x_2...x_{k}\}$ with $k\in\mathbb{Z}_{\ge 2}$ is the multidimensional divisor function."
Blazhko effect in the Galactic bulge fundamental mode RR~Lyrae stars I: Incidence rate and differences between modulated and non-modulated stars,"We present the first paper of a series focused on the Blazhko effect in RR Lyrae type stars pulsating in the fundamental mode, that are located in the Galactic bulge. A~comprehensive overview about the incidence rate and light-curve characteristics of the Blazhko stars is given. We analysed 8\,282 stars having the best quality data in the OGLE-IV survey, and found that at least $40.3$\,\% of stars show modulation of their light curves. The number of Blazhko stars we identified is 3\,341, which is the largest sample ever studied implying the most relevant statistical results currently available. Using combined data sets with OGLE-III observations, we found that 50\,\% of stars that show unresolved close peaks to the main component in OGLE-IV are actually Blazhko stars with extremely long periods. Blazhko stars with modulation occur preferentially among RR Lyrae stars with shorter pulsation periods in the Galactic bulge. Fourier amplitude and phase coefficients based on the mean light curves appear to be substantially lower for Blazhko stars than for stars with unmodulated light curve in average. We derived new relations for the compatibility parameter $D_{m}$ in $I$ passband and relations that allow for differentiating modulated and non-modulated stars easily on the basis of $R_{31}$, $\phi_{21}$ and $\phi_{31}$. Photometric metallicities, intrinsic colours and absolute magnitudes computed using empirical relations are the same for Blazhko and non-modulated stars in the Galactic bulge suggesting no correlation between the occurrence of the Blazhko effect and these parameters."
Radiation damage and thermal shock response of carbon-fiber-reinforced materials to intense high-energy proton beams,"  A comprehensive study on the effects of energetic protons on carbon-fiber composites and compounds under consideration for use as low-Z pion production targets in future high-power accelerators and low-impedance collimating elements for intercepting TeV-level protons at the Large Hadron Collider has been undertaken addressing two key areas, namely, thermal shock absorption and resistance to irradiation damage. "
Common greedy wiring and rewiring heuristics do not guarantee maximum assortative graphs of given degree,"  We examine two greedy heuristics - wiring and rewiring - for constructing maximum assortative graphs over all simple connected graphs with a target degree sequence. Counterexamples show that natural greedy rewiring heuristics do not necessarily return a maximum assortative graph, even though it is known that the meta-graph of all simple connected graphs with given degree is connected under rewiring. Counterexamples show an elegant greedy graph wiring heuristic from the literature may fail to achieve the target degree sequence or may fail to wire a maximally assortative graph. "
Electromagnetically Induced Transparency with Superradiant and Subradiant states,"  We construct the electromagnetically induced transparency (EIT) by dynamically coupling a superradiant state with a subradiant state. The superradiant and subradiant states with enhanced and inhibited decay rates act as the excited and metastable states in EIT, respectively. Their energy difference determined by the distance between the atoms can be measured by the EIT spectra, which renders this method useful in subwavelength metrology. The scheme can also be applied to many atoms in nuclear quantum optics, where the transparency point due to counter-rotating wave terms can be observed. "
Highly efficient angularly resolving x-ray spectrometer optimized for absorption measurements with collimated sources,Highly collimated betatron radiation from a laser wakefield accelerator is a promising tool for spectroscopic measurements. Therefore there is a requirement to create spectrometers suited to the unique properties of such a source. We demonstrate a spectrometer which achieves an energy resolution of < 5 eV at 9 keV and is angularly resolving the x-ray emission allowing the reference and spectrum to be recorded at the same time. The single photon analysis is used to significantly reduce the background noise. Theoretical performance of various configurations of the spectrometer is calculated by a ray-tracing algorithm. The properties and performance of the spectrometer including the angular and spectral resolution are demonstrated experimentally on absorption above the K-edge of a Cu foil backlit by laser-produced betatron radiation x-ray beam.
Assessing the impact of non-vaccinators: quantifying the average length of infection chains in outbreaks of vaccine-preventable disease,"Analytical expressions for the basic reproduction number, R0, have been obtained in the past for a wide variety of mathematical models for infectious disease spread, along with expressions for the expected final size of an outbreak. However, what has so far not been studied is the average number of infections that descend down the chains of infection begun by each of the individuals infected in an outbreak (we refer to this quantity as the ""average number of descendant infections"" per infectious individual, or ANDI). ANDI includes not only the number of people that an individual directly contacts and infects, but also the number of people that those go on to infect, and so on until that particular chain of infection dies out. Quantification of ANDI has relevance to the vaccination debate, since with ANDI one can calculate the probability that one or more people are hospitalised (or die) from a disease down an average chain of infection descending from an infected un-vaccinated individual. Here we obtain estimates of ANDI using both deterministic and stochastic modelling formalisms. With both formalisms we find that even for relatively small community sizes and under most scenarios for R0 and initial fraction vaccinated, ANDI can be surprisingly large when the effective reproduction number is >1, leading to high probabilities of adverse outcomes for one or more people down an average chain of infection in outbreaks of diseases like measles."
A Framework and Comparative Analysis of Control Plane Security of SDN and Conventional Networks,"  Software defined networking implements the network control plane in an external entity, rather than in each individual device as in conventional networks. This architectural difference implies a different design for control functions necessary for essential network properties, e.g., loop prevention and link redundancy. We explore how such differences redefine the security weaknesses in the SDN control plane and provide a framework for comparative analysis which focuses on essential network properties required by typical production networks. This enables analysis of how these properties are delivered by the control planes of SDN and conventional networks, and to compare security risks and mitigations. Despite the architectural difference, we find similar, but not identical, exposures in control plane security if both network paradigms provide the same network properties and are analyzed under the same threat model. However, defenses vary; SDN cannot depend on edge based filtering to protect its control plane, while this is arguably the primary defense in conventional networks. Our concrete security analysis suggests that a distributed SDN architecture that supports fault tolerance and consistency checks is important for SDN control plane security. Our analysis methodology may be of independent interest for future security analysis of SDN and conventional networks. "
Relative homological algebra via truncations,"To do homological algebra with unbounded chain complexes one needs to first find a way of constructing resolutions. Spaltenstein solved this problem for chain complexes of R-modules by truncating further and further to the left, resolving the pieces, and gluing back the partial resolutions. Our aim is to give a homotopy theoretical interpretation of this procedure, which may be extended to a relative setting. We work in an arbitrary abelian category A and fix a class I of ""injective objects"". We show that Spaltenstein's construction can be captured by a pair of adjoint functors between unbounded chain complexes and towers of non-positively graded ones. This pair of adjoint functors forms what we call a Quillen pair and the above process of truncations, partial resolutions, and gluing, gives a meaningful way to resolve complexes in a relative setting up to a split error term. In order to do homotopy theory, and in particular to construct a well behaved relative derived category D(A; I), we need more: the split error term must vanish. This is the case when I is the class of all injective R-modules but not in general, not even for certain classes of injectives modules over a Noetherian ring. The key property is a relative analogue of Roos's AB4*-n axiom for abelian categories. Various concrete examples such as Gorenstein homological algebra and purity are also discussed."
Note: A pairwise form of the Ewald sum for non-neutral systems,"  Using an example of a mixed discrete-continuum representation of charges under the periodic boundary condition, we show that the exact pairwise form of the Ewald sum, which is well-defined even if the system is non-neutral, provides a natural starting point for deriving unambiguous Coulomb energies that must remove all spurious dependence on the choice of the Ewald screening factor. "
Model selection and model averaging in MACML-estimated MNP models,"  This paper provides a review of model selection and model averaging methods for multinomial probit models estimated using the MACML approach. The proposed approaches are partitioned into test based methods (mostly derived from the likelihood ratio paradigm), methods based on information criteria and model averaging methods. Many of the approaches first have been derived for models estimated using maximum likelihood and later adapted to the composite marginal likelihood framework. In this paper all approaches are applied to the MACML approach for estimation. The investigation lists advantages and disadvantages of the various methods in terms of asymptotic properties as well as computational aspects. We find that likelihood-ratio-type tests and information criteria have a spotty performance when applied to MACML models and instead propose the use of an empirical likelihood test. Furthermore, we show that model averaging is easily adaptable to CML estimation and has promising performance w.r.t to parameter recovery. Finally model averaging is applied to a real world example in order to demonstrate the feasibility of the method in real world sized problems. "
Fuzzy Ontology-Based Sentiment Analysis of Transportation and City Feature Reviews for Safe Traveling,"  Traffic congestion is rapidly increasing in urban areas, particularly in mega cities. To date, there exist a few sensor network based systems to address this problem. However, these techniques are not suitable enough in terms of monitoring an entire transportation system and delivering emergency services when needed. These techniques require real-time data and intelligent ways to quickly determine traffic activity from useful information. In addition, these existing systems and websites on city transportation and travel rely on rating scores for different factors (e.g., safety, low crime rate, cleanliness, etc.). These rating scores are not efficient enough to deliver precise information, whereas reviews or tweets are significant, because they help travelers and transportation administrators to know about each aspect of the city. However, it is difficult for travelers to read, and for transportation systems to process, all reviews and tweets to obtain expressive sentiments regarding the needs of the city. The optimum solution for this kind of problem is analyzing the information available on social network platforms and performing sentiment analysis. On the other hand, crisp ontology-based frameworks cannot extract blurred information from tweets and reviews; therefore, they produce inadequate results. In this regard, this paper proposes fuzzy ontology-based sentiment analysis and SWRL rule-based decision-making to monitor transportation activities and to make a city- feature polarity map for travelers. This system retrieves reviews and tweets related to city features and transportation activities. The feature opinions are extracted from these retrieved data, and then fuzzy ontology is used to determine the transportation and city-feature polarity. A fuzzy ontology and an intelligent system prototype are developed by using Protégé OWL and Java, respectively. "
On the Gap Between Strict-Saddles and True Convexity: An Omega(log d) Lower Bound for Eigenvector Approximation,"We prove a \emph{query complexity} lower bound on rank-one principal component analysis (PCA). We consider an oracle model where, given a symmetric matrix $M \in \mathbb{R}^{d \times d}$, an algorithm is allowed to make $T$ \emph{exact} queries of the form $w^{(i)} = Mv^{(i)}$ for $i \in \{1,\dots,T\}$, where $v^{(i)}$ is drawn from a distribution which depends arbitrarily on the past queries and measurements $\{v^{(j)},w^{(j)}\}_{1 \le j \le i-1}$. We show that for a small constant $\epsilon$, any adaptive, randomized algorithm which can find a unit vector $\widehat{v}$ for which $\widehat{v}^{\top}M\widehat{v} \ge (1-\epsilon)\|M\|$, with even small probability, must make $T = \Omega(\log d)$ queries. In addition to settling a widely-held folk conjecture, this bound demonstrates a fundamental gap between convex optimization and ""strict-saddle"" non-convex optimization of which PCA is a canonical example: in the former, first-order methods can have dimension-free iteration complexity, whereas in PCA, the iteration complexity of gradient-based methods must necessarily grow with the dimension. Our argument proceeds via a reduction to estimating the rank-one spike in a deformed Wigner model. We establish lower bounds for this model by developing a ""truncated"" analogue of the $\chi^2$ Bayes-risk lower bound of Chen et al."
A Predictive Approach Using Deep Feature Learning for Electronic Medical Records: A Comparative Study,"  Massive amount of electronic medical records accumulating from patients and populations motivates clinicians and data scientists to collaborate for the advanced analytics to extract knowledge that is essential to address the extensive personalized insights needed for patients, clinicians, providers, scientists, and health policy makers. In this paper, we propose a new predictive approach based on feature representation using deep feature learning and word embedding techniques. Our method uses different deep architectures for feature representation in higher-level abstraction to obtain effective and more robust features from EMRs, and then build prediction models on the top of them. Our approach is particularly useful when the unlabeled data is abundant whereas labeled one is scarce. We investigate the performance of representation learning through a supervised approach. First, we apply our method on a small dataset related to a specific precision medicine problem, which focuses on prediction of left ventricular mass indexed to body surface area (LVMI) as an indicator of heart damage risk in a vulnerable demographic subgroup (African-Americans). Then we use two large datasets from eICU collaborative research database to predict the length of stay in Cardiac-ICU and Neuro-ICU based on high dimensional features. Finally we provide a comparative study and show that our predictive approach leads to better results in comparison with others. "
Gelfand numbers related to structured sparsity and Besov space embeddings with small mixed smoothness,"We consider the problem of determining the asymptotic order of the Gelfand numbers of mixed-(quasi-)norm embeddings $\ell^b_p(\ell^d_q) \hookrightarrow \ell^b_r(\ell^d_u)$ given that $p \leq r$ and $q \leq u$, with emphasis on cases with $p\leq 1$ and/or $q\leq 1$. These cases turn out to be related to structured sparsity. We obtain sharp bounds in a number of interesting parameter constellations. Our new matching bounds for the Gelfand numbers of the embeddings of $\ell_1^b(\ell_2^d)$ and $\ell_2^b(\ell_1^d)$ into $\ell_2^b(\ell_2^d)$ imply optimality assertions for the recovery of block-sparse and sparse-in-levels vectors, respectively. In addition, we apply the sharp estimates for $\ell^b_p(\ell^d_q)$-spaces to obtain new two-sided estimates for the Gelfand numbers of multivariate Besov space embeddings in regimes of small mixed smoothness. It turns out that in some particular cases these estimates show the same asymptotic behaviour as in the univariate situation. In the remaining cases they differ at most by a $\log\log$ factor from the univariate bound."
"Dust attenuation, bulge formation and inside-out cessation of star-formation in Star-Forming Main Sequence galaxies at z~2","We derive two-dimensional dust attenuation maps at $\sim1~\mathrm{kpc}$ resolution from the UV continuum for ten galaxies on the $z\sim2$ Star-Forming Main Sequence (SFMS). Comparison with IR data shows that 9 out of 10 galaxies do not require further obscuration in addition to the UV-based correction, though our sample does not include the most heavily obscured, massive galaxies. The individual rest-frame $V$-band dust attenuation (A$_{\rm V}$) radial profiles scatter around an average profile that gently decreases from $\sim1.8$ mag in the center down to $\sim0.6$ mag at $\sim3-4$ half-mass radii. We use these maps to correct UV- and H$\alpha$-based star-formation rates (SFRs), which agree with each other. At masses $<10^{11}~M_{\rm sun}$, the dust-corrected specific SFR (sSFR) profiles are on average radially constant at a mass-doubling timescale of $\sim300~\mathrm{Myr}$, pointing at a synchronous growth of bulge and disk components. At masses $>10^{11}~M_{\rm sun}$, the sSFR profiles are typically centrally-suppressed by a factor of $\sim10$ relative to the galaxy outskirts. With total central obscuration disfavored, this indicates that at least a fraction of massive $z\sim2$ SFMS galaxies have started their inside-out star-formation quenching that will move them to the quenched sequence. In combination with other observations, galaxies above and below the ridge of the SFMS relation have respectively centrally-enhanced and centrally-suppressed sSFRs relative to their outskirts, supporting a picture where bulges are built due to gas `compaction' that leads to a high central SFR as galaxies move towards the upper envelope of SFMS."
Shannon Shakes Hands with Chernoff: Big Data Viewpoint On Channel Information Measures,"  Shannon entropy is the most crucial foundation of Information Theory, which has been proven to be effective in many fields such as communications. Renyi entropy and Chernoff information are other two popular measures of information with wide applications. The mutual information is effective to measure the channel information for the fact that it reflects the relation between output variables and input variables. In this paper, we reexamine these channel information measures in big data viewpoint by means of ACE algorithm. The simulated results show us that decomposition results of Shannon and Chernoff mutual information with respect to channel parametersare almost the same. In this sense, Shannon shakes hands with Chernoff since they are different measures of the same information quantity. We also propose a conjecture that there is nature of channel information which is only decided by the channel parameters. "
A Morse theoretic description of the Goresky-Hingston product,  The Goresky-Hingston coproduct was first introduced by D. Sullivan and later extended by M. Goresky and N. Hingston. In this article we give a Morse theoretic description of the coproduct. Using the description we prove homotopy invariance property of the coproduct. We describe a connection between our Morse theoretic coproduct and a coproduct on Floer homology of cotangent bundle. 
SoAx: A generic C++ Structure of Arrays for handling Particles in HPC Codes,"  The numerical study of physical problems often require integrating the dynamics of a large number of particles evolving according to a given set of equations. Particles are characterized by the information they are carrying such as an identity, a position other. There are generally speaking two different possibilities for handling particles in high performance computing (HPC) codes. The concept of an Array of Structures (AoS) is in the spirit of the object-oriented programming (OOP) paradigm in that the particle information is implemented as a structure. Here, an object (realization of the structure) represents one particle and a set of many particles is stored in an array. In contrast, using the concept of a Structure of Arrays (SoA), a single structure holds several arrays each representing one property (such as the identity) of the whole set of particles. The AoS approach is often implemented in HPC codes due to its handiness and flexibility. For a class of problems, however, it is know that the performance of SoA is much better than that of AoS. We confirm this observation for our particle problem. Using a benchmark we show that on modern Intel Xeon processors the SoA implementation is typically several times faster than the AoS one. On Intel's MIC co-processors the performance gap even attains a factor of ten. The same is true for GPU computing, using both computational and multi-purpose GPUs. Combining performance and handiness, we present the library SoAx that has optimal performance (on CPUs, MICs, and GPUs) while providing the same handiness as AoS. For this, SoAx uses modern C++ design techniques such template meta programming that allows to automatically generate code for user defined heterogeneous data structures. "
Multi-scenario deep learning for multi-speaker source separation,"  Research in deep learning for multi-speaker source separation has received a boost in the last years. However, most studies are restricted to mixtures of a specific number of speakers, called a specific scenario. While some works included experiments for different scenarios, research towards combining data of different scenarios or creating a single model for multiple scenarios have been very rare. In this work it is shown that data of a specific scenario is relevant for solving another scenario. Furthermore, it is concluded that a single model, trained on different scenarios is capable of matching performance of scenario specific models. "
Strongly mixed random errors in Mann's iteration algorithm for a contractive real function,"  This work deals with the Mann's stochastic iteration algorithm under strong mixing random errors. We establish the Fuk-Nagaev's inequalities that enable us to prove the almost complete convergence with its corresponding rate of convergence. Moreover, these inequalities give us the possibility of constructing a confidence interval for the unique fixed point. Finally, to check the feasibility and validity of our theoretical results, we consider some numerical examples, namely a classical example from astronomy. "
Attribution Modeling Increases Efficiency of Bidding in Display Advertising,"  Predicting click and conversion probabilities when bidding on ad exchanges is at the core of the programmatic advertising industry. Two separated lines of previous works respectively address i) the prediction of user conversion probability and ii) the attribution of these conversions to advertising events (such as clicks) after the fact. We argue that attribution modeling improves the efficiency of the bidding policy in the context of performance advertising. Firstly we explain the inefficiency of the standard bidding policy with respect to attribution. Secondly we learn and utilize an attribution model in the bidder itself and show how it modifies the average bid after a click. Finally we produce evidence of the effectiveness of the proposed method on both offline and online experiments with data spanning several weeks of real traffic from Criteo, a leader in performance advertising. "
Identifying high betweenness centrality nodes in large social networks,"This paper proposes an alternative way to identify nodes with high betweenness centrality. It introduces a new metric, k-path centrality, and a randomized algorithm for estimating it, and shows empirically that nodes with high k-path centrality have high node betweenness centrality. The randomized algorithm runs in time $O(\kappa^{3}n^{2-2\alpha}\log n)$ and outputs, for each vertex v, an estimate of its k-path centrality up to additive error of $\pm n^{1/2+ \alpha}$ with probability $1-1/n^2$. Experimental evaluations on real and synthetic social networks show improved accuracy in detecting high betweenness centrality nodes and significantly reduced execution time when compared with existing randomized algorithms."
A functional limit theorem for the sine-process,"The main result of this paper is a functional limit theorem for the sine-process. In particular, we study the limit distribution, in the space of trajectories, for the number of particles in a growing interval. The sine-process has the Kolmogorov property and satisfies the Central Limit Theorem, but our functional limit theorem is very different from the Donsker Invariance Principle. We show that the time integral of our process can be approximated by the sum of a linear Gaussian process and independent Gaussian fluctuations whose covariance matrix is computed explicitly. We interpret these results in terms of the Gaussian Free Field convergence for the random matrix models. The proof relies on a general form of the multidimensional Central Limit Theorem under the sine-process for linear statistics of two types: those having growing variance and those with bounded variance corresponding to observables of Sobolev regularity $1/2$."
Multi-scale Lipschitz percolation of increasing events for Poisson random walks,"Consider the graph induced by $\mathbb{Z}^d$, equipped with uniformly elliptic random conductances. At time $0$, place a Poisson point process of particles on $\mathbb{Z}^d$ and let them perform independent simple random walks. Tessellate the graph into cubes indexed by $i\in\mathbb{Z}^d$ and tessellate time into intervals indexed by $\tau$. Given a local event $E(i,\tau)$ that depends only on the particles inside the space time region given by the cube $i$ and the time interval $\tau$, we prove the existence of a Lipschitz connected surface of cells $(i,\tau)$ that separates the origin from infinity on which $E(i,\tau)$ holds. This gives a directly applicable and robust framework for proving results in this setting that need a multi-scale argument. For example, this allows us to prove that an infection spreads with positive speed among the particles."
The Planck numbers and the essence of gravitation: phenomenology,"  We introduce phenomenological understanding of the electromagnetic component of the physical vacuum, the EM vacuum, as a basic medium for all masses of the expanding Universe, and ""Casimir polarization"" of this medium arising in the vicinity of any material object in the Universe as a result of conjugation of the electric field components of the EM vacuum on both sides (""external"" and ""internal"") of atomic nucleus boundary of the each mass with vacuum. It is shown that the gravitational attraction of two material objects in accordance with Newton's law of gravity arises as a result of overlapping of the domains of the EM vacuum Casimir polarization created by atomic nuclei of the objects, taking into account the long-range gravitational influence of all masses of the Universe on each nucleus of these objects (Mach's idea). Newton's law of gravitational attraction between two bodies is generalized to the case of gravitational interaction of a system of bodies when the center of mass of the pair of bodies shifted relative to the center of mass of the system. The unique smallness of gravitational interactions as compared with the fundamental nuclear (strong and weak) and electromagnetic ones is determined by the ratio of the characteristic size of the domain of EM vacuum Casimir polarization in the vicinity of atomic nuclei to the Hubble radius of the Universe. "
Low-Dimensional Spatial Embedding Method for Shape Uncertainty Quantification in Acoustic Scattering,"  This paper introduces a novel boundary integral approach of shape uncertainty quantification for the Helmholtz scattering problem in the framework of the so-called parametric method. The key idea is to construct an integration grid whose associated weight function encompasses the irregularities and nonsmoothness imposed by the random boundary. Thus, the solution can be evaluated accurately with relatively low number of grid points. The integration grid is obtained by employing a low-dimensional spatial embedding using the coarea formula. The proposed method can handle large variation as well as non-smoothness of the random boundary. For the ease of presentation the theory is restricted to star-shaped obstacles in low-dimensional setting. Higher spatial and parametric dimensional cases are discussed, though, not extensively explored in the current study. "
Updates on the background estimates for the X-IFU instrument onboard of the ATHENA mission,"ATHENA, with a launch foreseen in 2028 towards the L2 orbit, addresses the science theme ""The Hot and Energetic Universe"", coupling a high-performance X-ray Telescope with two complementary focal-plane instruments. One of these, the X-ray Integral Field Unit (X-IFU) is a TES based kilo-pixel array providing spatially resolved high-resolution spectroscopy (2.5 eV at 6 keV) over a 5 arcmin FoV. The background for this kind of detectors accounts for several components: the diffuse Cosmic X-ray Background, the low energy particles (<~100 keV) focalized by the mirrors and reaching the detector from inside the field of view, and the high energy particles (>~100 MeV) crossing the spacecraft and reaching the focal plane from every direction. Each one of these components is under study to reduce their impact on the instrumental performances. This task is particularly challenging, given the lack of data on the background of X-ray detectors in L2, the uncertainties on the particle environment to be expected in such orbit, and the reliability of the models used in the Monte Carlo background computations. As a consequence, the activities addressed by the group range from the reanalysis of the data of previous missions like XMM-Newton, to the characterization of the L2 environment by data analysis of the particle monitors onboard of satellites present in the Earth magnetotail, to the characterization of solar events and their occurrence, and to the validation of the physical models involved in the Monte Carlo simulations. All these activities will allow to develop a set of reliable simulations to predict, analyze and find effective solutions to reduce the particle background experienced by the X-IFU, ultimately satisfying the scientific requirement that enables the science of ATHENA. While the activities are still ongoing, we present here some preliminary results already obtained by the group."
Galaxy Zoo: star-formation versus spiral arm number,"Spiral arms are common features in low-redshift disc galaxies, and are prominent sites of star-formation and dust obscuration. However, spiral structure can take many forms: from galaxies displaying two strong `grand design' arms, to those with many `flocculent' arms. We investigate how these different arm types are related to a galaxy's star-formation and gas properties by making use of visual spiral arm number measurements from Galaxy Zoo 2. We combine UV and mid-IR photometry from GALEX and WISE to measure the rates and relative fractions of obscured and unobscured star formation in a sample of low-redshift SDSS spirals. Total star formation rate has little dependence on spiral arm multiplicity, but two-armed spirals convert their gas to stars more efficiently. We find significant differences in the fraction of obscured star-formation: an additional $\sim 10$ per cent of star-formation in two-armed galaxies is identified via mid-IR dust emission, compared to that in many-armed galaxies. The latter are also significantly offset below the IRX-$\beta$ relation for low-redshift star-forming galaxies. We present several explanations for these differences versus arm number: variations in the spatial distribution, sizes or clearing timescales of star-forming regions (i.e., molecular clouds), or contrasting recent star-formation histories."
Dense Associative Memory is Robust to Adversarial Inputs,"  Deep neural networks (DNN) trained in a supervised way suffer from two known problems. First, the minima of the objective function used in learning correspond to data points (also known as rubbish examples or fooling images) that lack semantic similarity with the training data. Second, a clean input can be changed by a small, and often imperceptible for human vision, perturbation, so that the resulting deformed input is misclassified by the network. These findings emphasize the differences between the ways DNN and humans classify patterns, and raise a question of designing learning algorithms that more accurately mimic human perception compared to the existing methods. Our paper examines these questions within the framework of Dense Associative Memory (DAM) models. These models are defined by the energy function, with higher order (higher than quadratic) interactions between the neurons. We show that in the limit when the power of the interaction vertex in the energy function is sufficiently large, these models have the following three properties. First, the minima of the objective function are free from rubbish images, so that each minimum is a semantically meaningful pattern. Second, artificial patterns poised precisely at the decision boundary look ambiguous to human subjects and share aspects of both classes that are separated by that decision boundary. Third, adversarial images constructed by models with small power of the interaction vertex, which are equivalent to DNN with rectified linear units (ReLU), fail to transfer to and fool the models with higher order interactions. This opens up a possibility to use higher order models for detecting and stopping malicious adversarial attacks. The presented results suggest that DAM with higher order energy functions are closer to human visual perception than DNN with ReLUs. "
The Nature of Turbulence in the LITTLE THINGS Dwarf Irregular Galaxies,"We present probability density functions and higher order (skewness and kurtosis) analyses of the galaxy-wide and spatially-resolved HI column density distributions in the LITTLE THINGS sample of dwarf irregular galaxies. This analysis follows that of Burkhart et al. (2010) for the Small Magellanic Cloud. About 60% of our sample have galaxy-wide values of kurtosis that are similar to that found for the Small Magellanic Cloud, with a range up to much higher values, and kurtosis increases with integrated star formation rate. Kurtosis and skewness were calculated for radial annuli and for a grid of 32 pixel X 32 pixel kernels across each galaxy. For most galaxies, kurtosis correlates with skewness. For about half of the galaxies, there is a trend of increasing kurtosis with radius. The range of kurtosis and skewness values is modeled by small variations in the Mach number close to the sonic limit and by conversion of HI to molecules at high column density. The maximum HI column densities decrease with increasing radius in a way that suggests molecules are forming in the weak field limit, where H_2 formation balances photodissociation in optically thin gas at the edges of clouds."
Degeneration of Trigonal Curves and Solutions of the KP-Hierarchy,"  It is known that soliton solutions of the KP-hierarchy corresponds to singular rational curves with only ordinary double points. In this paper we study the degeneration of theta function solutions corresponding to certain trigonal curves. We show that, when the curves degenerate to singular rational curves with only ordinary triple points, the solutions tend to some intermediate solutions between solitons and rational solutions. They are considered as cerain limits of solitons. The Sato Grassmannian is extensively used here to study the degeneration of solutions, since it directly connects solutions of the KP-hierarchy to the defining equations of algebraic curves.We define a class of solutions in the Wronskian form which contains soliton solutions as a subclass and prove that, using the Sato Grassmannian, the degenerate trigonal solutions are connected to those solutions by certain gauge transformations "
Blow-up solutions for a Kirchhoff type elliptic equation with trapping potential,We study a generalized Kirchhoff type equation with trapping potential. The existence and blow-up behavior of solutions with normalized L2-norm for this problem are discussed.
Representing Sentences as Low-Rank Subspaces,"Sentences are important semantic units of natural language. A generic, distributional representation of sentences that can capture the latent semantics is beneficial to multiple downstream applications. We observe a simple geometry of sentences -- the word representations of a given sentence (on average 10.23 words in all SemEval datasets with a standard deviation 4.84) roughly lie in a low-rank subspace (roughly, rank 4). Motivated by this observation, we represent a sentence by the low-rank subspace spanned by its word vectors. Such an unsupervised representation is empirically validated via semantic textual similarity tasks on 19 different datasets, where it outperforms the sophisticated neural network models, including skip-thought vectors, by 15% on average."
Kinetic Energy Density Functionals by Axiomatic Approach,"An axiomatic approach is herein used to determine the physically acceptable forms for general $D$-dimensional kinetic energy density functionals (KEDF). The resulted expansion captures most of the known forms of one-point KEDFs. By statistically training the KEDF forms on a model problem of non-interacting kinetic energy in 1D (6 terms only), the mean relative accuracy for 1000 randomly generated potentials is found to be better than the standard KEDF by several orders of magnitudes. The accuracy improves with the number of occupied states and was found to be better than $10^{-4}$ for a system with four occupied states. Furthermore, we show that free fitting of the coefficients associated with known KEDFs approaches the exactly analytic values. The presented approach can open a new route to search for physically acceptable kinetic energy density functionals and provide an essential step towards more accurate large-scale orbital free density functional theory calculations."
Locating Power Flow Solution Space Boundaries: A Numerical Polynomial Homotopy Approach,"The solution space of any set of power flow equations may contain different number of real-valued solutions. The boundaries that separate these regions are referred to as power flow solution space boundaries. Knowledge of these boundaries is important as they provide a measure for voltage stability. Traditionally, continuation based methods have been employed to compute these boundaries on the basis of initial guesses for the solution. However, with rapid growth of renewable energy sources these boundaries will be increasingly affected by variable parameters such as penetration levels, locations of the renewable sources, and voltage set-points, making it difficult to generate an initial guess that can guarantee all feasible solutions for the power flow problem. In this paper we solve this problem by applying a numerical polynomial homotopy based continuation method. The proposed method guarantees to find all solution boundaries within a given parameter space up to a chosen level of discretization, independent of any initial guess. Power system operators can use this computational tool conveniently to plan the penetration levels of renewable sources at different buses. We illustrate the proposed method through simulations on 3-bus and 10-bus power system examples with renewable generation."
First principles calculations of the interface properties of amorphous-Al2O3/MoS2 under non-strain and biaxial strain conditions,"Al2O3 is a potential dielectric material for metal-oxide-semiconductor (MOS) devices. Al2O3 films deposited on semiconductors usually exhibit amorphous due to lattice mismatch. Compared to two-dimensional graphene, MoS2 is a typical semiconductor, therefore, it has more extensive application. The amorphous-Al2O3/MoS2 (a-Al2O3/MoS2) interface has attracted people's attention because of its unique properties. In this paper, the interface behaviors of a-Al2O3/MoS2 under non-strain and biaxial strain are investigated by first principles calculations based on density functional theory (DFT). First of all, the generation process of a-Al2O3 sample is described, which is calculated by molecular dynamics and geometric optimization. Then, we introduce the band alignment method, and calculate band offset of a-Al2O3/MoS2 interface. It is found that the valence band offset (VBO) and conduction band offset (CBO) change with the number of MoS2 layers. The dependence of leakage current on the band offset is also illustrated. At last, the band structure of monolayer MoS2 under biaxial strain is discussed. The biaxial strain is set in the range from -6% to 6% with the interval of 2%. Impact of the biaxial strain on the band alignment is investigated."
On cohomological Hall algebras of quivers : Yangians,"  We consider the cohomological Hall algebra Y of a Lagrangian substack of the moduli stack of representations of the preprojective algebra of an arbitrary quiver Q, and its actions on the cohomology of quiver varieties. We conjecture that Y is equal, after a suitable extension of scalars, to the Yangian introduced by Maulik and Okounkov, and we construct an embedding of Y in the Yangian, intertwining the respective actions of both algebras on the cohomology of quiver varieties. "
Are Continuum Predictions of Clustering Chaotic?,"Gas-solid multiphase flows are prone to develop an instability known as clustering. Two-fluid models, which treat the particulate phase as a continuum, are known to reproduce the qualitative features of this instability, producing highly-dynamic, spatiotemporal patterns. However, it is unknown whether such simulations are truly aperiodic or a type of complex periodic behavior. By showing that the system possesses a sensitive dependence on initial conditions and a positive largest Lyapunov exponent, $\lambda_1 \approx 1/\tau$, we provide a tentative answer: continuum predictions of clustering are chaotic. We further demonstrate that the chaotic behavior is dimensionally dependent, a conclusion which unifies previous results and strongly suggests that the chaotic behavior is not a result of the fundamental kinematic instability, but of the secondary (inherently multidimensional) instability."
Easing Tensions with Quartessence,"Tensions between cosmic microwave background (CMB) observations and the growth of the large-scale structure (LSS) inferred from late-time probes pose a serious challenge to the concordance $\Lambda$CDM cosmological model. State-of-the-art CMB data from the Planck satellite predicts a higher rate of structure growth than what preferred by low-redshift observables. Such tension has hitherto eluded conclusive explanations in terms of straightforward modifications to $\Lambda$CDM, e.g. the inclusion of massive neutrinos or a dynamical dark energy component. Here, we investigate `quartessence' models, where a single dark component mimics both dark matter and dark energy. We show that such models greatly alleviate the tension between high and low redshift observations, thanks to the non-vanishing sound speed of quartessence that inhibits structure growth at late times on scales smaller than its corresponding Jeans' length. In particular, the $3.4\sigma$ tension between CMB and LSS observables is thoroughly reabsorbed. For this reason, we argue that quartessence deserves further investigation and may lead to a deeper understanding of the physics of the dark Universe."
Stable high-pressure phases in the H-S system determined by chemically reacting hydrogen and sulfur,"Synchrotron X-ray diffraction and Raman spectroscopy have been used to study chemical reactions of molecular hydrogen (H2) with sulfur (S) at high pressures. We find theoretically predicted Cccm and Im-3m H3S to be the reaction products at 50 and 140 GPa, respectively. Im-3m H3S is a stable crystalline phase above 140 GPa and it transforms to R3m H3S on pressure release below 140 GPa. The latter phase is (meta)stable down to at least 70 GPa where it transforms to Cccm H3S upon annealing (T<1300 K) to overcome the kinetic hindrance. Cccm H3S has an extended structure with symmetric hydrogen bonds at 50 GPa and upon decompression it experiences a transformation to a molecular mixed H2S-H2 structure below 40 GPa without any apparent change in the crystal symmetry."
Learning Neurosymbolic Generative Models via Program Synthesis,"Significant strides have been made toward designing better generative models in recent years. Despite this progress, however, state-of-the-art approaches are still largely unable to capture complex global structure in data. For example, images of buildings typically contain spatial patterns such as windows repeating at regular intervals; state-of-the-art generative methods can't easily reproduce these structures. We propose to address this problem by incorporating programs representing global structure into the generative model---e.g., a 2D for-loop may represent a configuration of windows. Furthermore, we propose a framework for learning these models by leveraging program synthesis to generate training data. On both synthetic and real-world data, we demonstrate that our approach is substantially better than the state-of-the-art at both generating and completing images that contain global structure."
Collapsibility of marginal models for categorical data,"  We consider marginal log-linear models for parameterizing distributions on multidimensional contingency tables. These models generalize ordinary log-linear and multivariate logistic models, besides several others. First, we obtain some characteristic properties of marginal log-linear parameters. Then we define collapsibility and strict collapsibility of these parameters in a general sense. Several necessary and sufficient conditions for collapsibility and strict collapsibility are derived using the technique of Möbius inversion. These include results for an arbitrary set of marginal log-linear parameters having some common effects. The connections of collapsibility and strict collapsibility to various forms of independence of the variables are discussed. Finally, we establish a result on the relationship between parameters with the same effect but different margins, and use it to demonstrate smoothness of marginal log-linear models under collapsibility conditions thereby obtaining a curved exponential family. "
Tunable viscosity modification with diluted particles: When particles decrease the viscosity of complex fluids,"  While spherical particles are the most studied viscosity modifiers, they are well known only to increase viscosities, in particular at low concentrations. Extended studies and theories on non-spherical particles find a more complicated behavior, but still a steady increase. Involving platelets in combination with complex fluids displays an even more complex scenario that we analyze experimentally and theoretically as a function of platelet diameter, to find the underlying concepts. Using a broad toolbox of different techniques we were able to decrease the viscosity of crude oils although solid particles were added. This apparent contradiction could lead to a wider range of applications. "
Research Portfolio Analysis and Topic Prominence,"Stakeholders in the science system need to decide where to place their bets. Example questions include: Which areas of research should get more funding? Who should we hire? Which projects should we abandon and which new projects should we start? Making informed choices requires knowledge about these research options. Unfortunately, to date research portfolio options have not been defined in a consistent, transparent and relevant manner. Furthermore, we don't know how to define demand for these options. In this article, we address the issues of consistency, transparency, relevance and demand by using a model of science consisting of 91,726 topics (or research options) that contain over 58 million documents. We present a new indicator of topic prominence - a measure of visibility, momentum and, ultimately, demand. We assign over $203 billion of project-level funding data from STAR METRICS to individual topics in science, and show that the indicator of topic prominence, explains over one-third of the variance in current (or future) funding by topic. We also show that highly prominent topics receive far more funding per researcher than topics that are not prominent. Implications of these results for research planning and portfolio analysis by institutions and researchers are emphasized."
Un Crit{È}Re Simple,"  In this short note, we mimic the proof of the simplicity of the theory ACFA of generic difference fields in order to provide a criterion, valid for certain theories of pure fields and fields equipped with operators, which shows that a complete theory is simple whenever its definable and algebraic closures are controlled by an underlying stable theory. "
Orbits of irreducible binary forms over GF$(p)$,"In this note I give a formula for calculating the number of orbits of irreducible binary forms of degree $n$ over GF$(p)$ under the action of GL$(2,p)$. This formula has applications to the classification of class two groups of exponent $p$ with derived groups of order $p^2$."
A Survey of Entorhinal Grid Cell Properties,"  About a decade ago grid cells were discovered in the medial entorhinal cortex of rat. Their peculiar firing patterns, which correlate with periodic locations in the environment, led to early hypothesis that grid cells may provide some form of metric for space. Subsequent research has since uncovered a wealth of new insights into the characteristics of grid cells and their neural neighborhood, the parahippocampal-hippocampal region, calling for a revision and refinement of earlier grid cell models. This survey paper aims to provide a comprehensive summary of grid cell research published in the past decade. It focuses on the functional characteristics of grid cells such as the influence of external cues or the alignment to environmental geometry, but also provides a basic overview of the underlying neural substrate. "
An Automata-based Abstract Semantics for String Manipulation Languages,"  In recent years, dynamic languages, such as JavaScript or Python, have faced an important increment of usage in a wide range of fields and applications. Their tricky and misunderstood behaviors pose a hard challenge for static analysis of these programming languages. A key aspect of any dynamic language program is the multiple usage of strings, since they can be implicitly converted to another type value, transformed by string-to-code primitives or used to access an object-property. Unfortunately, string analyses for dynamic languages still lack of precision and do not take into account some important string features. Moreover, string obfuscation is very popular in the context of dynamic language malicious code, for example, to hide code information inside strings and then to dynamically transform strings into executable code. In this scenario, more precise string analyses become a necessity. This paper proposes a new semantics for string analysis placing a first step for handling dynamic languages string features. "
Context encoders as a simple but powerful extension of word2vec,"With a simple architecture and the ability to learn meaningful word embeddings efficiently from texts containing billions of words, word2vec remains one of the most popular neural language models used today. However, as only a single embedding is learned for every word in the vocabulary, the model fails to optimally represent words with multiple meanings. Additionally, it is not possible to create embeddings for new (out-of-vocabulary) words on the spot. Based on an intuitive interpretation of the continuous bag-of-words (CBOW) word2vec model's negative sampling training objective in terms of predicting context based similarities, we motivate an extension of the model we call context encoders (ConEc). By multiplying the matrix of trained word2vec embeddings with a word's average context vector, out-of-vocabulary (OOV) embeddings and representations for a word with multiple meanings can be created based on the word's local contexts. The benefits of this approach are illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition (NER) task."
Sandwiches Missing Two Ingredients of Order Four,"For a set ${\cal F}$ of graphs, an instance of the ${\cal F}$-{\sc free Sandwich Problem} is a pair $(G_1,G_2)$ consisting of two graphs $G_1$ and $G_2$ with the same vertex set such that $G_1$ is a subgraph of $G_2$, and the task is to determine an ${\cal F}$-free graph $G$ containing $G_1$ and contained in $G_2$, or to decide that such a graph does not exist. Initially motivated by the graph sandwich problem for trivially perfect graphs, which are the $\{ P_4,C_4\}$-free graphs, we study the complexity of the ${\cal F}$-{\sc free Sandwich Problem} for sets ${\cal F}$ containing two non-isomorphic graphs of order four. We show that if ${\cal F}$ is one of the sets $\left\{ {\rm diamond},K_4\right\}$, $\left\{ {\rm diamond},C_4\right\}$, $\left\{ {\rm diamond},{\rm paw}\right\}$, $\left\{ K_4,\overline{K_4}\right\}$, $\left\{ P_4,C_4\right\}$, $\left\{ P_4,\overline{\rm claw}\right\}$, $\left\{ P_4,\overline{\rm paw}\right\}$, $\left\{ P_4,\overline{\rm diamond}\right\}$, $\left\{ {\rm paw},C_4\right\}$, $\left\{ {\rm paw},{\rm claw}\right\}$, $\left\{ {\rm paw},\overline{\rm claw}\right\}$, $\left\{ {\rm paw},\overline{\rm paw}\right\}$, $\left\{ C_4,\overline{C_4}\right\}$, $\left\{ {\rm claw},\overline{\rm claw}\right\}$, and $\left\{ {\rm claw},\overline{C_4}\right\}$, then the ${\cal F}$-{\sc free Sandwich Problem} can be solved in polynomial time, and, if ${\cal F}$ is one of the sets $\left\{ C_4,K_4\right\}$, $\left\{ {\rm paw},K_4\right\}$, $\left\{ {\rm paw},\overline{K_4}\right\}$, $\left\{ {\rm paw},\overline{C_4}\right\}$, $\left\{ {\rm diamond},\overline{C_4}\right\}$, $\left\{ {\rm paw},\overline{\rm diamond}\right\}$, and $\left\{ {\rm diamond},\overline{\rm diamond}\right\}$, then the decision version of the ${\cal F}$-{\sc free Sandwich Problem} is NP-complete."
Butterfly velocity and bulk causal structure,"  The butterfly velocity was recently proposed as a characteristic velocity of chaos propagation in a local system. Compared to the Lieb-Robinson velocity that bounds the propagation speed of all perturbations, the butterfly velocity, studied in thermal ensembles, is an ""effective"" Lieb-Robinson velocity for a subspace of the Hilbert space defined by the microcanonical ensemble. In this paper, we generalize the concept of butterfly velocity beyond the thermal case to a large class of other subspaces. Based on holographic duality, we consider the code subspace of low energy excitations on a classical background geometry. Using local reconstruction of bulk operators, we prove a general relation between the boundary butterfly velocities (of different operators) and the bulk causal structure. Our result has implications in both directions of the bulk-boundary correspondence. Starting from a boundary theory with a given Lieb-Robinson velocity, our result determines an upper bound of the bulk light cone starting from a given point. Starting from a bulk space-time geometry, the butterfly velocity can be explicitly calculated for all operators that are the local reconstructions of bulk local operators. If the bulk geometry satisfies Einstein equation and the null energy condition, for rotation symmetric geometries we prove that infrared operators always have a slower butterfly velocity that the ultraviolet one. For asymptotic AdS geometries, this also implies that the butterfly velocities of all operators are upper bounded by the speed of light. We further prove that the butterfly velocity is equal to the speed of light if the causal wedge of the boundary region coincides with its entanglement wedge. Finally, we discuss the implication of our result to geometries that are not asymptotically AdS, and in particular, obtain constraints that must be satisfied by a dual theory of flat space gravity. "
Correlated atomic wires on substrates. II. Application to Hubbard wires,"In the first part of our theoretical study of correlated atomic wires on substrates, we introduced lattice models for a one-dimensional quantum wire on a three-dimensional substrate and their approximation by quasi-one-dimensional effective ladder models [arXiv:1704.07350]. In this second part, we apply this approach to the case of a correlated wire with a Hubbard-type electron-electron repulsion deposited on an insulating substrate. The ground-state and spectral properties are investigated numerically using the density-matrix renormalization group method and quantum Monte Carlo simulations. As a function of the model parameters, we observe various phases with quasi-one-dimensional low-energy excitations localized in the wire, namely paramagnetic Mott insulators, Luttinger liquids, and spin-$1/2$ Heisenberg chains. The validity of the effective ladder models is assessed by studying the convergence with the number of legs and comparing to the full three-dimensional model. We find that narrow ladder models accurately reproduce the quasi-one-dimensional excitations of the full three-dimensional model but predict only qualitatively whether excitations are localized around the wire or delocalized in the three-dimensional substrate."
Distributed Bayesian Piecewise Sparse Linear Models,"  The importance of interpretability of machine learning models has been increasing due to emerging enterprise predictive analytics, threat of data privacy, accountability of artificial intelligence in society, and so on. Piecewise linear models have been actively studied to achieve both accuracy and interpretability. They often produce competitive accuracy against state-of-the-art non-linear methods. In addition, their representations (i.e., rule-based segmentation plus sparse linear formula) are often preferred by domain experts. A disadvantage of such models, however, is high computational cost for simultaneous determinations of the number of ""pieces"" and cardinality of each linear predictor, which has restricted their applicability to middle-scale data sets. This paper proposes a distributed factorized asymptotic Bayesian (FAB) inference of learning piece-wise sparse linear models on distributed memory architectures. The distributed FAB inference solves the simultaneous model selection issue without communicating $O(N)$ data where N is the number of training samples and achieves linear scale-out against the number of CPU cores. Experimental results demonstrate that the distributed FAB inference achieves high prediction accuracy and performance scalability with both synthetic and benchmark data. "
Mercury's magnetic field in the MESSENGER era,"MESSENGER magnetometer data show that Mercury's magnetic field is not only exceptionally weak but also has a unique geometry. The internal field resembles an axial dipole that is offset to the North by 20% of the planetary radius. This implies that the axial quadrupol is particularly strong while the dipole tilt is likely below 0.8 degree. The close proximity to the sun in combination with the weak internal field results in a very small and highly dynamic Hermean magnetosphere. We review the current understanding of Mercury's internal and external magnetic field and discuss possible explanations. Classical convection driven core dynamos have a hard time to reproduce the observations. Strong quadrupol contributions can be promoted by different measures, but they always go along with a large dipole tilt and generally rather small scale fields. A stably stratified outer core region seems required to explain not only the particular geometry but also the weakness of the Hermean magnetic field. New interior models suggest that Mercury's core likely hosts an iron snow zone underneath the core-mantle boundary. The positive radial sulfur gradient likely to develop in such a zone would indeed promote stable stratification. However, even dynamo models that include the stable layer show Mercury-like magnetic fields only for a fraction of the total simulation time. Large scale variations in the core-mantle boundary heat flux promise to yield more persistent results but are not compatible with the current understanding of Mercury's lower mantle."
A two-stage approach for estimating the parameters of an age-group epidemic model from incidence data,"  Age-dependent dynamics is an important characteristic of many infectious diseases. Age-group epidemic models describe the infection dynamics in different age-groups by allowing to set distinct parameter values for each. However, such models are highly nonlinear and may have a large number of unknown parameters. Thus, parameter estimation of age-group models, while becoming a fundamental issue for both the scientific study and policy making in infectious diseases, is not a trivial task in practice. In this paper, we examine the estimation of the so called next-generation matrix using incidence data of a single entire outbreak, and extend the approach to deal with recurring outbreaks. Unlike previous studies, we do not assume any constraints regarding the structure of the matrix. A novel two-stage approach is developed, which allows for efficient parameter estimation from both statistical and computational perspectives. Simulation studies corroborate the ability to estimate accurately the parameters of the model for several realistic scenarios. The model and estimation method are applied to real data of influenza-like-illness in Israel. The parameter estimates of the key relevant epidemiological parameters and the recovered structure of the estimated next-generation matrix are in line with results obtained in previous studies. "
Sequential Double Robustness in Right-Censored Longitudinal Models,"  Consider estimating the G-formula for the counterfactual mean outcome under a given treatment regime in a longitudinal study. Bang and Robins provided an estimator for this quantity that relies on a sequential regression formulation of this parameter. This approach is doubly robust in that it is consistent if either the outcome regressions or the treatment mechanisms are consistently estimated. We define a stronger notion of double robustness, termed sequential double robustness, for estimators of the longitudinal G-formula. The definition emerges naturally from a more general definition of sequential double robustness for the outcome regression estimators. An outcome regression estimator is sequentially doubly robust (SDR) if, at each subsequent time point, either the outcome regression or the treatment mechanism is consistently estimated. This form of robustness is exactly what one would anticipate is attainable by studying the remainder term of a first-order expansion of the G-formula parameter. We show that a particular implementation of an existing procedure is SDR. We also introduce a novel SDR estimator, whose development involves a novel translation of ideas used in targeted minimum loss-based estimation to the infinite-dimensional setting. "
On the treewidth of triangulated 3-manifolds,"In graph theory, as well as in 3-manifold topology, there exist several width-type parameters to describe how ""simple"" or ""thin"" a given graph or 3-manifold is. These parameters, such as pathwidth or treewidth for graphs, or the concept of thin position for 3-manifolds, play an important role when studying algorithmic problems; in particular, there is a variety of problems in computational 3-manifold topology - some of them known to be computationally hard in general - that become solvable in polynomial time as soon as the dual graph of the input triangulation has bounded treewidth. In view of these algorithmic results, it is natural to ask whether every 3-manifold admits a triangulation of bounded treewidth. We show that this is not the case, i.e., that there exists an infinite family of closed 3-manifolds not admitting triangulations of bounded pathwidth or treewidth (the latter implies the former, but we present two separate proofs). We derive these results from work of Agol, of Scharlemann and Thompson, and of Scharlemann, Schultens and Saito by exhibiting explicit connections between the topology of a 3-manifold M on the one hand and width-type parameters of the dual graphs of triangulations of M on the other hand, answering a question that had been raised repeatedly by researchers in computational 3-manifold topology. In particular, we show that if a closed, orientable, irreducible, non-Haken 3-manifold M has a triangulation of treewidth (resp. pathwidth) k then the Heegaard genus of M is at most 24(k+1) (resp. 4(3k+1))."
The Geometry of Large Tundra Lakes Observed in Historical Maps and Satellite Images,"Tundra lakes are key components of the Arctic climate system because they represent a source of methane to the atmosphere. In this paper, we aim to analyze the geometry of the patterns formed by large ($>0.8$ km$^2$) tundra lakes in the Russian High Arctic. We have studied images of tundra lakes in historical maps from the State Hydrological Institute, Russia (date 1977; scale $0.21166$ km/pixel) and in Landsat satellite images derived from the Google Earth Engine (G.E.E.; date 2016; scale $0.1503$ km/pixel). The G.E.E. is a cloud-based platform for planetary-scale geospatial analysis on over four decades of Landsat data. We developed an image-processing algorithm to segment these maps and images, measure the area and perimeter of each lake, and compute the fractal dimension of the lakes in the images we have studied. Our results indicate that as lake size increases, their fractal dimension bifurcates. For lakes observed in historical maps, this bifurcation occurs among lakes larger than $100$ km$^2$ (fractal dimension $1.43$ to $1.87$). For lakes observed in satellite images this bifurcation occurs among lakes larger than $\sim$100 km$^2$ (fractal dimension $1.31$ to $1.95$). Tundra lakes with a fractal dimension close to $2$ have a tendency to be self-similar with respect to their area--perimeter relationships. Area--perimeter measurements indicate that lakes with a length scale greater than $70$ km$^2$ are power-law distributed. Preliminary analysis of changes in lake size over time in paired lakes (lakes that were visually matched in both the historical map and the satellite imagery) indicate that some lakes in our study region have increased in size over time, whereas others have decreased in size over time. Lake size change during this 39-year time interval can be up to half the size of the lake as recorded in the historical map."
Deep-Learning Convolutional Neural Networks for scattered shrub detection with Google Earth Imagery,"  There is a growing demand for accurate high-resolution land cover maps in many fields, e.g., in land-use planning and biodiversity conservation. Developing such maps has been performed using Object-Based Image Analysis (OBIA) methods, which usually reach good accuracies, but require a high human supervision and the best configuration for one image can hardly be extrapolated to a different image. Recently, the deep learning Convolutional Neural Networks (CNNs) have shown outstanding results in object recognition in the field of computer vision. However, they have not been fully explored yet in land cover mapping for detecting species of high biodiversity conservation interest. This paper analyzes the potential of CNNs-based methods for plant species detection using free high-resolution Google Earth T M images and provides an objective comparison with the state-of-the-art OBIA-methods. We consider as case study the detection of Ziziphus lotus shrubs, which are protected as a priority habitat under the European Union Habitats Directive. According to our results, compared to OBIA-based methods, the proposed CNN-based detection model, in combination with data-augmentation, transfer learning and pre-processing, achieves higher performance with less human intervention and the knowledge it acquires in the first image can be transferred to other images, which makes the detection process very fast. The provided methodology can be systematically reproduced for other species detection. "
Tight Semi-Nonnegative Matrix Factorization,"  The nonnegative matrix factorization is a widely used, flexible matrix decomposition, finding applications in biology, image and signal processing and information retrieval, among other areas. Here we present a related matrix factorization. A multi-objective optimization problem finds conical combinations of templates that approximate a given data matrix. The templates are chosen so that as far as possible only the initial data set can be represented this way. However, the templates are not required to be nonnegative nor convex combinations of the original data. "
Low Precision RNNs: Quantizing RNNs Without Losing Accuracy,"  Similar to convolution neural networks, recurrent neural networks (RNNs) typically suffer from over-parameterization. Quantizing bit-widths of weights and activations results in runtime efficiency on hardware, yet it often comes at the cost of reduced accuracy. This paper proposes a quantization approach that increases model size with bit-width reduction. This approach will allow networks to perform at their baseline accuracy while still maintaining the benefits of reduced precision and overall model size reduction. "
Unsupervised Representation Adversarial Learning Network: from Reconstruction to Generation,"  A good representation for arbitrarily complicated data should have the capability of semantic generation, clustering and reconstruction. Previous research has already achieved impressive performance on either one. This paper aims at learning a disentangled representation effective for all of them in an unsupervised way. To achieve all the three tasks together, we learn the forward and inverse mapping between data and representation on the basis of a symmetric adversarial process. In theory, we minimize the upper bound of the two conditional entropy loss between the latent variables and the observations together to achieve the cycle consistency. The newly proposed RepGAN is tested on MNIST, fashionMNIST, CelebA, and SVHN datasets to perform unsupervised or semi-supervised classification, generation and reconstruction tasks. The result demonstrates that RepGAN is able to learn a useful and competitive representation. To the author's knowledge, our work is the first one to achieve both a high unsupervised classification accuracy and low reconstruction error on MNIST. "
Quantitative Models of Imperfect Deception in Network Security using Signaling Games with Evidence,"  Deception plays a critical role in many interactions in communication and network security. Game-theoretic models called ""cheap talk signaling games"" capture the dynamic and information asymmetric nature of deceptive interactions. But signaling games inherently model undetectable deception. In this paper, we investigate a model of signaling games in which the receiver can detect deception with some probability. This model nests traditional signaling games and complete information Stackelberg games as special cases. We present the pure strategy perfect Bayesian Nash equilibria of the game. Then we illustrate these analytical results with an application to active network defense. The presence of evidence forces majority-truthful behavior and eliminates some pure strategy equilibria. It always benefits the deceived player, but surprisingly sometimes also benefits the deceiving player. "
Drawbacks and alternatives to the numerical calculation of the base inertial parameters expressions for low mobility mechanisms,"  Base inertial parameters constitute a minimal inertial parametrization of mechanical systems that is of interest, for example, in parameter estimation and model reduction. Numerical and symbolic methods are available to determine their expressions. In this paper the problems associated with the numerical determination of the base inertial parameters expressions in the context of low mobility mechanisms are analyzed and discussed through and example. To circumvent these problems two alternatives are proposed: a variable precision arithmetic implementation of the customary numerical algorithm and the application of a general symbolic method. Finally, the advantages of both approaches compared to the numerical one are discussed in the context of the proposed low mobility example. "
Self-Paced Multitask Learning with Shared Knowledge,"  This paper introduces self-paced task selection to multitask learning, where instances from more closely related tasks are selected in a progression of easier-to-harder tasks, to emulate an effective human education strategy, but applied to multitask machine learning. We develop the mathematical foundation for the approach based on iterative selection of the most appropriate task, learning the task parameters, and updating the shared knowledge, optimizing a new bi-convex loss function. This proposed method applies quite generally, including to multitask feature learning, multitask learning with alternating structure optimization, etc. Results show that in each of the above formulations self-paced (easier-to-harder) task selection outperforms the baseline version of these methods in all the experiments. "
Morphometric analysis of polygonal cracking patterns in desiccated starch slurries,"  We investigate the geometry of two-dimensional polygonal cracking that forms on the air-exposed surface of dried starch slurries. Two different kinds of starches, made from potato and corn, exhibited distinguished crack evolution, and there were contrasting effects of slurry thickness on the probability distribution of the polygonal cell area. The experimental findings are believed to result from the difference in the shape and size of starch grains, which strongly influence the capillary transport of water and tensile stress field that drives the polygonal cracking. "
Parameterized Shifted Combinatorial Optimization,"Shifted combinatorial optimization is a new nonlinear optimization framework which is a broad extension of standard combinatorial optimization, involving the choice of several feasible solutions at a time. This framework captures well studied and diverse problems ranging from so-called vulnerability problems to sharing and partitioning problems. In particular, every standard combinatorial optimization problem has its shifted counterpart, which is typically much harder. Already with explicitly given input set the shifted problem may be NP-hard. In this article we initiate a study of the parameterized complexity of this framework. First we show that shifting over an explicitly given set with its cardinality as the parameter may be in XP, FPT or P, depending on the objective function. Second, we study the shifted problem over sets definable in MSO logic (which includes, e.g., the well known MSO partitioning problems). Our main results here are that shifted combinatorial optimization over MSO definable sets is in XP with respect to the MSO formula and the treewidth (or more generally clique-width) of the input graph, and is W[1]-hard even under further severe restrictions."
Mermin-Wagner at the Crossover Temperature,"Mermin-Wagner excludes spontaneous (staggered) magnetization in isotropic ferromagnetic (antiferromagnetic) Heisenberg models at finite temperature in spatial dimensions $d \le 2$. While the proof relies on the Bogoliubov inequality, here we illuminate the theorem from an effective field theory point of view. We estimate the crossover temperature $T_c$ and show that, in weak external fields $H$, it tends to zero: $T_c \propto \sqrt{H}$ ($d=1$) and $T_c \propto 1/|\ln H|$ ($d=2$). Including the case $d$=3, we derive upper bounds for the (staggered) magnetization by combining microscopic and effective perspectives -- unfortunately, these bounds are not restrictive."
The injectivity radius of Lie manifolds,"  We prove in a direct, geometric way that for any compatible Riemannian metric on a Lie manifold the injectivity radius is positive "
Inverse Risk-Sensitive Reinforcement Learning,"  We address the problem of inverse reinforcement learning in Markov decision processes where the agent is risk-sensitive. In particular, we model risk-sensitivity in a reinforcement learning framework by making use of models of human decision-making having their origins in behavioral psychology, behavioral economics, and neuroscience. We propose a gradient-based inverse reinforcement learning algorithm that minimizes a loss function defined on the observed behavior. We demonstrate the performance of the proposed technique on two examples, the first of which is the canonical Grid World example and the second of which is a Markov decision process modeling passengers' decisions regarding ride-sharing. In the latter, we use pricing and travel time data from a ride-sharing company to construct the transition probabilities and rewards of the Markov decision process. "
Correlations and confinement of excitations in an asymmetric Hubbard ladder,"  Correlation functions and low-energy excitations are investigated in the asymmetric two-leg ladder consisting of a Hubbard chain and a noninteracting tight-binding (Fermi) chain using the density matrix renormalization group method. The behavior of charge, spin and pairing correlations is discussed for the four phases found at half filling, namely, Luttinger liquid, Kondo-Mott insulator, spin-gapped Mott insulator and correlated band insulator. Quasi-long-range antiferromagnetic spin correlations are found in the Hubbard leg in the Luttinger liquid phase only. Pair-density-wave correlations are studied to understand the structure of bound pairs found in the Fermi leg of the spin-gapped Mott phase at half filling and at light doping but we find no enhanced pairing correlations. Low-energy excitations cause variations of spin and charge densities on both legs that demonstrate the confinement of the lowest charge excitations on the Fermi leg while the lowest spin excitations are localized on the Hubbard leg in the three insulating phases. The velocities of charge, spin, and single-particle excitations are investigated to clarify the confinement of elementary excitations in the Luttinger liquid phase. The observed spatial separation of elementary spin and charge excitations could facilitate the coexistence of different (quasi-)long-range orders in higher-dimensional extensions of the asymmetric Hubbard ladder. "
Notes about collision monochromatization in $e^+e^-$ colliders,  The manuscript describes several monochromatization schemes starting from A.~Renieri \cite{ref:Renieri} proposal for head-on collisions based on correlation between particles transverse position and energy deviation. We briefly explain initial proposal and expand it for crossing angle collisions. Then we discuss new monochromatization scheme for crossing angle collisions based on correlation between particles longitudinal position and energy deviation. 
Combinatorial Cost Sharing,"  We introduce a combinatorial variant of the cost sharing problem: several services can be provided to each player and each player values every combination of services differently. A publicly known cost function specifies the cost of providing every possible combination of services. A combinatorial cost sharing mechanism is a protocol that decides which services each player gets and at what price. We look for dominant strategy mechanisms that are (economically) efficient and cover the cost, ideally without overcharging (i.e., budget balanced). Note that unlike the standard cost sharing setting, combinatorial cost sharing is a multi-parameter domain. This makes designing dominant strategy mechanisms with good guarantees a challenging task. We present the Potential Mechanism -- a combination of the VCG mechanism and a well-known tool from the theory of cooperative games: Hart and Mas-Colell's potential function. The potential mechanism is a dominant strategy mechanism that always covers the incurred cost. When the cost function is subadditive the same mechanism is also approximately efficient. Our main technical contribution shows that when the cost function is submodular the potential mechanism is approximately budget balanced in three settings: supermodular valuations, symmetric cost function and general symmetric valuations, and two players with general valuations. "
A new family of one-coincidence sets of sequences with dispersed elements for frequency hopping CDMA systems,"  We present a new family of one-coincidence sequence sets suitable for frequency hopping code division multiple access (FH-CDMA) systems with dispersed (low density) sequence elements. These sets are derived from one-coincidence prime sequence sets, such that for each one-coincidence prime sequence set there is a new one-coincidence set comprised of sequences with dispersed sequence elements, required in some circumstances, for FH-CDMA systems. Getting rid of crowdedness of sequence elements is achieved by doubling the size of the sequence element alphabet. In addition, this doubling process eases control over the distance between adjacent sequence elements. Properties of the new sets are discussed. "
Crowdsourcing with Tullock contests: A new perspective,"  Incentive mechanisms for crowdsourcing have been extensively studied under the framework of all-pay auctions. Along a distinct line, this paper proposes to use Tullock contests as an alternative tool to design incentive mechanisms for crowdsourcing. We are inspired by the conduciveness of Tullock contests to attracting user entry (yet not necessarily a higher revenue) in other domains. In this paper, we explore a new dimension in optimal Tullock contest design, by superseding the contest prize---which is fixed in conventional Tullock contests---with a prize function that is dependent on the (unknown) winner's contribution, in order to maximize the crowdsourcer's utility. We show that this approach leads to attractive practical advantages: (a) it is well-suited for rapid prototyping in fully distributed web agents and smartphone apps; (b) it overcomes the disincentive to participate caused by players' antagonism to an increasing number of rivals. Furthermore, we optimize conventional, fixed-prize Tullock contests to construct the most superior benchmark to compare against our mechanism. Through extensive evaluations, we show that our mechanism significantly outperforms the optimal benchmark, by over three folds on the crowdsourcer's utility cum profit and up to nine folds on the players' social welfare. "
Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent,"Most distributed machine learning systems nowadays, including TensorFlow and CNTK, are built in a centralized fashion. One bottleneck of centralized algorithms lies on high communication cost on the central node. Motivated by this, we ask, can decentralized algorithms be faster than its centralized counterpart? Although decentralized PSGD (D-PSGD) algorithms have been studied by the control community, existing analysis and theory do not show any advantage over centralized PSGD (C-PSGD) algorithms, simply assuming the application scenario where only the decentralized network is available. In this paper, we study a D-PSGD algorithm and provide the first theoretical analysis that indicates a regime in which decentralized algorithms might outperform centralized algorithms for distributed stochastic gradient descent. This is because D-PSGD has comparable total computational complexities to C-PSGD but requires much less communication cost on the busiest node. We further conduct an empirical study to validate our theoretical analysis across multiple frameworks (CNTK and Torch), different network configurations, and computation platforms up to 112 GPUs. On network configurations with low bandwidth or high latency, D-PSGD can be up to one order of magnitude faster than its well-optimized centralized counterparts."
Learning User Intent from Action Sequences on Interactive Systems,"  Interactive systems have taken over the web and mobile space with increasing participation from users. Applications across every marketing domain can now be accessed through mobile or web where users can directly perform certain actions and reach a desired outcome. Actions of user on a system, though, can be representative of a certain intent. Ability to learn this intent through user's actions can help draw certain insight into the behavior of users on a system. In this paper, we present models to optimize interactive systems by learning and analyzing user intent through their actions on the system. We present a four phased model that uses time-series of interaction actions sequentially using a Long Short-Term Memory (LSTM) based sequence learning system that helps build a model for intent recognition. Our system then provides an objective specific maximization followed by analysis and contrasting methods in order to identify spaces of improvement in the interaction system. We discuss deployment scenarios for such a system and present results from evaluation on an online marketplace using user clickstream data. "
Laman Graphs are Generically Bearing Rigid in Arbitrary Dimensions,"  This paper addresses the problem of constructing bearing rigid networks in arbitrary dimensions. We first show that the bearing rigidity of a network is a generic property that is critically determined by the underlying graph of the network. A new notion termed generic bearing rigidity is defined for graphs. If the underlying graph of a network is generically bearing rigid, then the network is bearing rigid for almost all configurations; otherwise, the network is not bearing rigid for any configuration. As a result, the key to construct bearing rigid networks is to construct generically bearing rigid graphs. The main contribution of this paper is to prove that Laman graphs, which can be generated by the Henneberg construction, are generically bearing rigid in arbitrary dimensions. As a consequence, if the underlying graph of a network is Laman, the network is bearing rigid for almost all configurations in arbitrary dimensions. "
"Finding Dominating Induced Matchings in $(S_{2,2,3})$-Free Graphs in Polynomial Time","Let $G=(V,E)$ be a finite undirected graph. An edge set $E' \subseteq E$ is a {\em dominating induced matching} ({\em d.i.m.}) in $G$ if every edge in $E$ is intersected by exactly one edge of $E'$. The \emph{Dominating Induced Matching} (\emph{DIM}) problem asks for the existence of a d.i.m.\ in $G$; this problem is also known as the \emph{Efficient Edge Domination} problem; it is the Efficient Domination problem for line graphs. The DIM problem is motivated by parallel resource allocation problems, encoding theory and network routing. It is \NP-complete even for very restricted graph classes such as planar bipartite graphs with maximum degree 3 and is solvable in linear time for $P_7$-free graphs, and in polynomial time for $S_{1,2,4}$-free graphs as well as for $S_{2,2,2}$-free graphs. In this paper, combining two distinct approaches, we solve it in polynomial time for $S_{2,2,3}$-free graphs."
Symmetry reduction and soliton-like solutions for the generalized Korteweg-de Vries equation,"We analyze the gKdV equation, a generalized version of Korteweg-de Vries with an arbitrary function $f(u)$. In general, for a function $f(u)$ the Lie algebra of symmetries of gKdV is the $2$-dimensional Lie algebra of translations of the plane $xt$. This implies the existence of plane wave solutions. Indeed, for some specific values of $f(u)$ the equation gKdV admits a Lie algebra of symmetries of dimension grater than $2$. We compute the similarity reductions corresponding to these exceptional symmetries. We prove that the gKdV equation has soliton-like solutions under some general assumptions, and we find a closed formula for the plane wave solutions, that are of hyperbolic secant type."
Existence theory for magma equations in dimension two and higher,"  We examine a degenerate, dispersive, nonlinear wave equation related to the evolution of partially molten rock in dimensions two and higher. This simplified model, for a scalar field capturing the melt fraction by volume, has been studied by direct numerical simulation where it has been observed to develop stable solitary waves. In this work, we prove local in time well-posedness results for the time dependent equation, on both the whole space and the torus, for dimensions two and higher. We also prove the existence of the solitary wave solutions in dimensions two and higher. "
A short guide through integration theorems of generalized distributions,"The generalization of Frobenius' theorem to foliations with singularities is usually attributed to Stefan and Sussmann, for their simultaneous discovery around 1973. However, their result is often referred to without caring much on the precise statement, as some sort of magic spell. This may be explained by the fact that the literature is not consensual on a unique formulation of the theorem, and because the history of the research leading to this result has been flawed by many claims that turned to be refuted some years later. This, together with the difficulty of doing proof-reading on this topic, brought much confusion about the precise statement of Stefan-Sussmann's theorem. This paper is dedicated to bring some light on this subject, by investigating the different statements and arguments that were put forward in geometric control theory between 1962 and 1994 regarding the problem of integrability of generalized distributions. We will present the genealogy of the main ideas and show that many mathematicians that were involved in this field made some mistakes that were successfully refuted. Moreover, we want to address the prominent influence of Hermann on this topic, as well as the fact that some statements of Stefan and Sussmann turned to be wrong. In this paper, we intend to provide the reader with a deeper understanding of the problem of integrability of generalized distributions, and to reduce the confusion surrounding these difficult questions."
Polarization induced interference within electromagnetically induced transparency for atoms of double-V linkage,"  People have been paying attention to the role of atoms' complex internal level structures in the research of electromagnetically induced transparency (EIT) for a long time, where the various degenerate Zeeman levels usually generate complex linkage patterns for the atomic transitions. It turns out, with special choices of the atomic states and the atomic transitions' linkage structure, clear signatures of quantum interference induced by the probe and coupling light's polarizations can emerge from a typical EIT phenomena. We propose to study a four state system with double-V linkage pattern for the transitions and analyze the polarization induced interference under the EIT condition. We show that such interference arises naturally under mild conditions on the optical field and atom manipulation. Its anticipated properties and its potential application of all optical switching in polarization degree of freedom are also discussed. Moreover, we construct a variation form of double-M linkage pattern where the polarization induced interference enables polarization-dependent cross-modulation between incident lights that can be effective even at the few-photon level. The theme is to gain more insight into the essential question: how can we build non-trivial optical medium where incident lights will induce polarization-dependent non-linear optical interactions, covering a wide range of the incidence intensity from the many-photon level to the few-photon level, respectively. "
A note on X-rays of permutations and a problem of Brualdi and Fritscher,"The subject of this note is a challenging conjecture about X-rays of permutations which is a special case of a conjecture regarding Skolem sequences. In relation to this, Brualdi and Fritscher [Linear Algebra and its Applications, 2014] posed the following problem: Determine a bijection between extremal Skolem sets and binary Hankel X-rays of permutation matrices. We give such a bijection, along with some related observations."
Differential Evolution and Bayesian Optimisation for Hyper-Parameter Selection in Mixed-Signal Neuromorphic Circuits Applied to UAV Obstacle Avoidance,"  The Lobula Giant Movement Detector (LGMD) is a an identified neuron of the locust that detects looming objects and triggers its escape responses. Understanding the neural principles and networks that lead to these fast and robust responses can lead to the design of efficient facilitate obstacle avoidance strategies in robotic applications. Here we present a neuromorphic spiking neural network model of the LGMD driven by the output of a neuromorphic Dynamic Vision Sensor (DVS), which has been optimised to produce robust and reliable responses in the face of the constraints and variability of its mixed signal analogue-digital circuits. As this LGMD model has many parameters, we use the Differential Evolution (DE) algorithm to optimise its parameter space. We also investigate the use of Self-Adaptive Differential Evolution (SADE) which has been shown to ameliorate the difficulties of finding appropriate input parameters for DE. We explore the use of two biological mechanisms: synaptic plasticity and membrane adaptivity in the LGMD. We apply DE and SADE to find parameters best suited for an obstacle avoidance system on an unmanned aerial vehicle (UAV), and show how it outperforms state-of-the-art Bayesian optimisation used for comparison. "
Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking,"Methods that learn representations of nodes in a graph play a critical role in network analysis since they enable many downstream learning tasks. We propose Graph2Gauss - an approach that can efficiently learn versatile node embeddings on large scale (attributed) graphs that show strong performance on tasks such as link prediction and node classification. Unlike most approaches that represent nodes as point vectors in a low-dimensional continuous space, we embed each node as a Gaussian distribution, allowing us to capture uncertainty about the representation. Furthermore, we propose an unsupervised method that handles inductive learning scenarios and is applicable to different types of graphs: plain/attributed, directed/undirected. By leveraging both the network structure and the associated node attributes, we are able to generalize to unseen nodes without additional training. To learn the embeddings we adopt a personalized ranking formulation w.r.t. the node distances that exploits the natural ordering of the nodes imposed by the network structure. Experiments on real world networks demonstrate the high performance of our approach, outperforming state-of-the-art network embedding methods on several different tasks. Additionally, we demonstrate the benefits of modeling uncertainty - by analyzing it we can estimate neighborhood diversity and detect the intrinsic latent dimensionality of a graph."
Explaining Anomalies in Groups with Characterizing Subspace Rules,"Anomaly detection has numerous applications and has been studied vastly. We consider a complementary problem that has a much sparser literature: anomaly description. Interpretation of anomalies is crucial for practitioners for sense-making, troubleshooting, and planning actions. To this end, we present a new approach called x-PACS (for eXplaining Patterns of Anomalies with Characterizing Subspaces), which ""reverse-engineers"" the known anomalies by identifying (1) the groups (or patterns) that they form, and (2) the characterizing subspace and feature rules that separate each anomalous pattern from normal instances. Explaining anomalies in groups not only saves analyst time and gives insight into various types of anomalies, but also draws attention to potentially critical, repeating anomalies. In developing x-PACS, we first construct a desiderata for the anomaly description problem. From a descriptive data mining perspective, our method exhibits five desired properties in our desiderata. Namely, it can unearth anomalous patterns (i) of multiple different types, (ii) hidden in arbitrary subspaces of a high dimensional space, (iii) interpretable by the analysts, (iv) different from normal patterns of the data, and finally (v) succinct, providing the shortest data description. Furthermore, x-PACS is highly parallelizable and scales linearly in terms of data size. No existing work on anomaly description satisfies all of these properties simultaneously. While not our primary goal, the anomalous patterns we find serve as interpretable ""signatures"" and can be used for detection. We show the effectiveness of x-PACS in explanation as well as detection on real-world datasets as compared to state-of-the-art."
VAMPnets: Deep learning of molecular kinetics,"  There is an increasing demand for computing the relevant structures, equilibria and long-timescale kinetics of biomolecular processes, such as protein-drug binding, from high-throughput molecular dynamics simulations. Current methods employ transformation of simulated coordinates into structural features, dimension reduction, clustering the dimension-reduced data, and estimation of a Markov state model or related model of the interconversion rates between molecular structures. This handcrafted approach demands a substantial amount of modeling expertise, as poor decisions at any step will lead to large modeling errors. Here we employ the variational approach for Markov processes (VAMP) to develop a deep learning framework for molecular kinetics using neural networks, dubbed VAMPnets. A VAMPnet encodes the entire mapping from molecular coordinates to Markov states, thus combining the whole data processing pipeline in a single end-to-end framework. Our method performs equally or better than state-of-the art Markov modeling methods and provides easily interpretable few-state kinetic models. "
Toward deciphering developmental patterning with deep neural network,"  Dynamics of complex biological systems is driven by intricate networks, the current knowledge of which are often incomplete. The traditional systems biology modeling usually implements an ad hoc fixed set of differential equations with predefined function forms. Such an approach often suffers from overfitting or underfitting and thus inadequate predictive power, especially when dealing with systems of high complexity. This problem could be overcome by deep neuron network (DNN). Choosing pattern formation of the gap genes in Drosophila early embryogenesis as an example, we established a differential equation model whose synthesis term is expressed as a DNN. The model yields perfect fitting and impressively accurate predictions on mutant patterns. We further mapped the trained DNN into a simplified conventional regulation network, which is consistent with the existing body of knowledge. The DNN model could lay a foundation of ""in-silico-embryo"", which can regenerate a great variety of interesting phenomena, and on which one can perform all kinds of perturbations to discover underlying mechanisms. This approach can be readily applied to a variety of complex biological systems. "
Whispered-to-voiced Alaryngeal Speech Conversion with Generative Adversarial Networks,"  Most methods of voice restoration for patients suffering from aphonia either produce whispered or monotone speech. Apart from intelligibility, this type of speech lacks expressiveness and naturalness due to the absence of pitch (whispered speech) or artificial generation of it (monotone speech). Existing techniques to restore prosodic information typically combine a vocoder, which parameterises the speech signal, with machine learning techniques that predict prosodic information. In contrast, this paper describes an end-to-end neural approach for estimating a fully-voiced speech waveform from whispered alaryngeal speech. By adapting our previous work in speech enhancement with generative adversarial networks, we develop a speaker-dependent model to perform whispered-to-voiced speech conversion. Preliminary qualitative results show effectiveness in re-generating voiced speech, with the creation of realistic pitch contours. "
A graph-theoretic description of scale-multiplicative semigroups of automorphisms,"It is shown that a flat subgroup, $H$, of the totally disconnected, locally compact group $G$ decomposes into a finite number of subsemigroups on which the scale function is multiplicative. The image, $P$, of a multiplicative semigroup in the quotient, $H/H(1)$, of $H$ by its uniscalar subgroup has a unique minimal generating set which determines a natural Cayley graph structure on $P$. For each compact, open subgroup $U$ of $G$, a graph is defined and it is shown that if $P$ is multiplicative over $U$ then this graph is a regular, rooted, strongly simple $P$-graph. This extends to higher rank the result of R. Möller that $U$ is tidy for $x$ if and only if a certain graph is a regular, rooted tree."
Half-Heusler alloy LiBaBi: A new topological semimetal with five-fold band degeneracy,"  Based on first-principles study, we report the finding of a new topological semimetal LiBaBi in half-Heusler phase. The remarkable feature of this nonmagnetic, inversion-symmetry-breaking material is that it consists of only simple $s$- and $p$-block elements. Interestingly, the material is ordinary insulator in the absence of spin-orbit coupling (SOC) and becomes nodal-surface topological semimetal showing drumhead states when SOC is included. This is in stark contrast to other nodal-line and nodal-surface semimetals, where the extended nodal structure is destroyed once SOC is included. Importantly, the linear band crossings host three-, four-, five- and six-fold degeneracies near the Fermi level, making this compound very attractive for the study of `unconventional' fermions. The band crossing points form a three-dimensional nodal structure around the zone center at the Fermi level. We identify the surface states responsible for the appearance of the drumhead states. The alloy also shows a phase transition from topological semimetal to a trivial insulator on application of pressure. In addition to revealing an intriguing effect of SOC on the nodal structure, our findings introduce a new half-Heusler alloy in the family of topological semimetals, thus creating more avenues for experimental exploration. "
A task-driven implementation of a simple numerical solver for hyperbolic conservation laws,"  This article describes the implementation of an all-in-one numerical procedure within the runtime StarPU. In order to limit the complexity of the method, for the sake of clarity of the presentation of the non-classical task-driven programming environnement, we have limited the numerics to first order in space and time. Results show that the task distribution is efficient if the tasks are numerous and individually large enough so that the task heap can be saturated by tasks which computational time covers the task management overhead. Next, we also see that even though they are mostly faster on graphic cards, not all the tasks are suitable for GPUs, which brings forward the importance of the task scheduler. Finally, we look at a more realistic system of conservation laws with an expensive source term, what allows us to conclude and open on future works involving higher local arithmetic intensity, by increasing the order of the numerical method or by enriching the model (increased number of parameters and therefore equations). "
Composition of PPT Maps,"  M. Christandl conjectured that the composition of any trace preserving PPT map with itself is entanglement breaking. We prove that Christandl's conjecture holds asymptotically by showing that the distance between the iterates of any unital or trace preserving PPT map and the set of entanglement breaking maps tends to zero. Finally, for every graph we define a one-parameter family of maps on matrices and determine the least value of the parameter such that the map is variously, positive, completely positive, PPT and entanglement breaking in terms of properties of the graph. Our estimates are sharp enough to conclude that Christandl's conjecture holds for these families. "
Multi-View Image Generation from a Single-View,"  This paper addresses a challenging problem -- how to generate multi-view cloth images from only a single view input. To generate realistic-looking images with different views from the input, we propose a new image generation model termed VariGANs that combines the strengths of the variational inference and the Generative Adversarial Networks (GANs). Our proposed VariGANs model generates the target image in a coarse-to-fine manner instead of a single pass which suffers from severe artifacts. It first performs variational inference to model global appearance of the object (e.g., shape and color) and produce a coarse image with a different view. Conditioned on the generated low resolution images, it then proceeds to perform adversarial learning to fill details and generate images of consistent details with the input. Extensive experiments conducted on two clothing datasets, MVC and DeepFashion, have demonstrated that images of a novel view generated by our model are more plausible than those generated by existing approaches, in terms of more consistent global appearance as well as richer and sharper details. "
Equidistribution of Neumann data mass on simplices and a simple inverse problem,"In this paper we study the behaviour of the Neumann data of Dirichlet eigenfunctions on simplices. We prove that the $L^2$ norm of the (semi-classical) Neumann data on each face is equal to $2/n$ times the $(n-1)$-dimensional volume of the face divided by the volume of the simplex. This is a generalization of \cite{Chr-tri} to higher dimensions. Again it is {\it not} an asymptotic, but an exact formula. The proof is by simple integrations by parts and linear algebra. We also consider the following inverse problem: do the {\it norms} of the Neumann data on a simplex determine a constant coefficient elliptic operator? The answer is yes in dimension 2 and no in higher dimensions."
A machine learning approach for efficient uncertainty quantification using multiscale methods,"  Several multiscale methods account for sub-grid scale features using coarse scale basis functions. For example, in the Multiscale Finite Volume method the coarse scale basis functions are obtained by solving a set of local problems over dual-grid cells. We introduce a data-driven approach for the estimation of these coarse scale basis functions. Specifically, we employ a neural network predictor fitted using a set of solution samples from which it learns to generate subsequent basis functions at a lower computational cost than solving the local problems. The computational advantage of this approach is realized for uncertainty quantification tasks where a large number of realizations has to be evaluated. We attribute the ability to learn these basis functions to the modularity of the local problems and the redundancy of the permeability patches between samples. The proposed method is evaluated on elliptic problems yielding very promising results. "
Quadratic Upper Bound for Recursive Teaching Dimension of Finite VC Classes,"In this work we study the quantitative relation between the recursive teaching dimension (RTD) and the VC dimension (VCD) of concept classes of finite sizes. The RTD of a concept class $\mathcal C \subseteq \{0, 1\}^n$, introduced by Zilles et al. (2011), is a combinatorial complexity measure characterized by the worst-case number of examples necessary to identify a concept in $\mathcal C$ according to the recursive teaching model. For any finite concept class $\mathcal C \subseteq \{0,1\}^n$ with $\mathrm{VCD}(\mathcal C)=d$, Simon & Zilles (2015) posed an open problem $\mathrm{RTD}(\mathcal C) = O(d)$, i.e., is RTD linearly upper bounded by VCD? Previously, the best known result is an exponential upper bound $\mathrm{RTD}(\mathcal C) = O(d \cdot 2^d)$, due to Chen et al. (2016). In this paper, we show a quadratic upper bound: $\mathrm{RTD}(\mathcal C) = O(d^2)$, much closer to an answer to the open problem. We also discuss the challenges in fully solving the problem."
Joining Jolie to Docker - Orchestration of Microservices on a Containers-as-a-Service Layer,"  Cloud computing is steadily growing and, as IaaS vendors have started to offer pay-as-you-go billing policies, it is fundamental to achieve as much elasticity as possible, avoiding over-provisioning that would imply higher costs. In this paper, we briefly analyse the orchestration characteristics of PaaSSOA, a proposed architecture already implemented for Jolie microservices, and Kubernetes, one of the various orchestration plugins for Docker; then, we outline similarities and differences of the two approaches, with respect to their own domain of application. Furthermore, we investigate some ideas to achieve a federation of the two technologies, proposing an architectural composition of Jolie microservices on Docker Container-as-a-Service layer. "
On Calculation of Bounds for Greedy Algorithms when Applied to Sensor Selection Problems,"  We consider the problem of studying the performance of greedy algorithm on sensor selection problem for stable linear systems with Kalman Filter. Specifically, the objective is to find the system parameters that affects the performance of greedy algorithms and conditions where greedy algorithm always produces optimal solutions. In this paper, we developed an upper bound for performance ratio of greedy algorithm, which is based on the work of Dr.Zhang \cite{Sundaram} and offers valuable insight into the system parameters that affects the performance of greedy algorithm. We also proposes a set of conditions where greedy algorithm will always produce the optimal solution. We then show in simulations how the system parameters mentioned by the performance ratio bound derived in this work affects the performance of greedy algorithm. "
Generators of reductions of ideals in a local Noetherian ring with finite residue field,"Let $(R,\mathfrak{m})$ be a local Noetherian ring with residue field $k$. While much is known about the generating sets of reductions of ideals of $R$ if $k$ is infinite, the case in which $k$ is finite is less well understood. We investigate the existence (or lack thereof) of proper reductions of an ideal of $R$ and the number of generators needed for a reduction in the case $k$ is a finite field. When $R$ is one-dimensional, we give a formula for the smallest integer $n$ for which every ideal has an $n$-generated reduction. It follows that in a one-dimensional local Noetherian ring every ideal has a principal reduction if and only if the number of maximal ideals in the normalization of the reduced quotient of $R$ is at most $|k|$. In higher dimensions, we show that for any positive integer, there exists an ideal of $R$ that does not have an $n$-generated reduction and that if $n \geq \dim R$ this ideal can be chosen to be $\mathfrak{m}$-primary. In the case where $R$ is a two-dimensional regular local ring, we construct an example of an integrally closed $\mathfrak{m}$-primary ideal that does not have a $2$-generated reduction and thus answer in the negative a question raised by Heinzer and Shannon."
Particle-like Structure of Lie algebras,"If a Lie algebra structure g on a vector space is the sum of a family of mutually compatible Lie algebra structures g_i's, we say that g is simply assembled from the g_i's. Repeating this procedure with a number of Lie algebras, themselves simply assembled from the g_i's, one obtains a Lie algebra assembled in two steps from the g_i's, and so on. We describe the process of modular disassembling of a Lie algebra into a unimodular and a non-unimodular part. We then study two inverse questions: which Lie algebras can be assembled from a given family of Lie algebras, and from which Lie algebras can a given Lie algebra be assembled? We develop some basic assembling and disassembling techniques that constitute the elements of a new approach to the general theory of Lie algebras. The main result of our theory is that any finite-dimensional Lie algebra over an algebraically closed field of characteristic zero or over R can be assembled in a finite number of steps from two elementary constituents, which we call dyons and triadons. Up to an abelian summand, a dyon is a Lie algebra structure isomorphic to the non-abelian 2-dimensional Lie algebra, while a triadon is isomorphic to the 3-dimensional Heisenberg Lie algebra. As an example, we describe constructions of classical Lie algebras from triadons."
"Reversed Dickson polynomials of the $(k+1)$-th kind over finite fields, II","Let $p$ be an odd prime. In this paper, we study the permutation behaviour of the reversed Dickson polynomials of the $(k+1)$-th kind $D_{n,k}(1,x)$ when $n=p^{l_1}+3$, $n=p^{l_1}+p^{l_2}+p^{l_3}$, and $n=p^{l_1}+p^{l_2}+p^{l_3}+p^{l_4}$, where $l_1, l_2$, $l_3$, and $l_4$ are non-negative integers. A generalization to $n=p^{l_1}+p^{l_2}+\cdots +p^{l_i}$ is also shown. We find some conditions under which $D_{n,k}(1,x)$ is not a permutation polynomial over finite fields for certain values of $n$ and $k$. We also present a generalization of a recent result regarding $D_{p^l-1,1}(1,x)$ and present some algebraic and arithmetic properties of $D_{n,k}(1,x)$."
Improving Session Recommendation with Recurrent Neural Networks by Exploiting Dwell Time,"  Recently, Recurrent Neural Networks (RNNs) have been applied to the task of session-based recommendation. These approaches use RNNs to predict the next item in a user session based on the previ- ously visited items. While some approaches consider additional item properties, we argue that item dwell time can be used as an implicit measure of user interest to improve session-based item recommen- dations. We propose an extension to existing RNN approaches that captures user dwell time in addition to the visited items and show that recommendation performance can be improved. Additionally, we investigate the usefulness of a single validation split for model selection in the case of minor improvements and find that in our case the best model is not selected and a fold-like study with different validation sets is necessary to ensure the selection of the best model. "
Universal Spatiotemporal Sampling Sets for Discrete Spatially Invariant Evolution Systems,"Let $(I,+)$ be a finite abelian group and $\mathbf{A}$ be a circular convolution operator on $\ell^2(I)$. The problem under consideration is how to construct minimal $\Omega \subset I$ and $l_i$ such that $Y=\{\mathbf{e}_i, \mathbf{A}\mathbf{e}_i, \cdots, \mathbf{A}^{l_i}\mathbf{e}_i: i\in \Omega\}$ is a frame for $\ell^2(I)$, where $\{\mathbf{e}_i: i\in I\}$ is the canonical basis of $\ell^2(I)$. This problem is motivated by the spatiotemporal sampling problem in discrete spatially invariant evolution systems. We will show that the cardinality of $\Omega $ should be at least equal to the largest geometric multiplicity of eigenvalues of $\mathbf{A}$, and we consider the universal spatiotemporal sampling sets $(\Omega, l_i)$ for convolution operators $\mathbf{A}$ with eigenvalues subject to the same largest geometric multiplicity. We will give an algebraic characterization for such sampling sets and show how this problem is linked with sparse signal processing theory and polynomial interpolation theory."
Self-Organizing Maps Classification with Application to Laptop's Adapters Magnetic Field,"  This paper presents an application of the Self-Organizing-Map classification method, which is used for classification of the extremely low frequency magnetic field emission in the near neighborhood of the laptop adapters. The experiment is performed on different laptop adapters of the same characteristics. After that, the Self-Organizing-Map classification on the obtained emission data is performed. The classification results establish the typical emission levels of the laptop adapters, which are far above the safety standards' limit. At the end, a discussion is carried out about the importance of using the classification as a possible solution for safely use the laptop adapters in order to reduce the negative effects of the magnetic field emission to the laptop users. "
Attention Models in Graphs: A Survey,"  Graph-structured data arise naturally in many different application domains. By representing data as graphs, we can capture entities (i.e., nodes) as well as their relationships (i.e., edges) with each other. Many useful insights can be derived from graph-structured data as demonstrated by an ever-growing body of work focused on graph mining. However, in the real-world, graphs can be both large - with many complex patterns - and noisy which can pose a problem for effective graph mining. An effective way to deal with this issue is to incorporate ""attention"" into graph mining solutions. An attention mechanism allows a method to focus on task-relevant parts of the graph, helping it to make better decisions. In this work, we conduct a comprehensive and focused survey of the literature on the emerging field of graph attention models. We introduce three intuitive taxonomies to group existing work. These are based on problem setting (type of input and output), the type of attention mechanism used, and the task (e.g., graph classification, link prediction, etc.). We motivate our taxonomies through detailed examples and use each to survey competing approaches from a unique standpoint. Finally, we highlight several challenges in the area and discuss promising directions for future work. "
Multiplexing 200 modes on a single digital hologram,"The on-demand tailoring of light's spatial shape is of great relevance in a wide variety of research areas. Computer-controlled devices, such as Spatial Light Modulators (SLMs) or Digital Micromirror Devices (DMDs), offer a very accurate, flexible and fast holographic means to this end. Remarkably, digital holography affords the simultaneous generation of multiple beams (multiplexing), a tool with numerous applications in many fields. Here, we provide a self-contained tutorial on light beam multiplexing. Through the use of several examples, the readers will be guided step by step in the process of light beam shaping and multiplexing. Additionally, on the multiplexing capabilities of SLMs to provide a quantitative analysis on the maximum number of beams that can be multiplexed on a single SLM, showing approximately 200 modes on a single hologram."
Ranking influential spreaders is an ill-defined problem,"Finding influential spreaders of information and disease in networks is an important theoretical problem, and one of considerable recent interest. It has been almost exclusively formulated as a node-ranking problem -- methods for identifying influential spreaders rank nodes according to how influential they are. In this work, we show that the ranking approach does not necessarily work: the set of most influential nodes depends on the number of nodes in the set. Therefore, the set of $n$ most important nodes to vaccinate does not need to have any node in common with the set of $n+1$ most important nodes. We propose a method for quantifying the extent and impact of this phenomenon, and show that it is common in both empirical and model networks."
Fixing a Broken ELBO,"  Recent work in unsupervised representation learning has focused on learning deep directed latent-variable models. Fitting these models by maximizing the marginal likelihood or evidence is typically intractable, thus a common approximation is to maximize the evidence lower bound (ELBO) instead. However, maximum likelihood training (whether exact or approximate) does not necessarily result in a good latent representation, as we demonstrate both theoretically and empirically. In particular, we derive variational lower and upper bounds on the mutual information between the input and the latent variable, and use these bounds to derive a rate-distortion curve that characterizes the tradeoff between compression and reconstruction accuracy. Using this framework, we demonstrate that there is a family of models with identical ELBO, but different quantitative and qualitative characteristics. Our framework also suggests a simple new method to ensure that latent variable models with powerful stochastic decoders do not ignore their latent code. "
Wavelet Shrinkage and Thresholding based Robust Classification for Brain Computer Interface,"  A macaque monkey is trained to perform two different kinds of tasks, memory aided and visually aided. In each task, the monkey saccades to eight possible target locations. A classifier is proposed for direction decoding and task decoding based on local field potentials (LFP) collected from the prefrontal cortex. The LFP time-series data is modeled in a nonparametric regression framework, as a function corrupted by Gaussian noise. It is shown that if the function belongs to Besov bodies, then using the proposed wavelet shrinkage and thresholding based classifier is robust and consistent. The classifier is then applied to the LFP data to achieve high decoding performance. The proposed classifier is also quite general and can be applied for the classification of other types of time-series data as well, not necessarily brain data. "
SCALAR - Simultaneous Calibration of 2D Laser and Robot's Kinematic Parameters Using Three Planar Constraints,"Industrial robots are increasingly used in various applications where the robot accuracy becomes very important, hence calibrations of the robot's kinematic parameters and the measurement system's extrinsic parameters are required. However, the existing calibration approaches are either too cumbersome or require another expensive external measurement system such as laser tracker or measurement spinarm. In this paper, we propose SCALAR, a calibration method to simultaneously improve the kinematic parameters of a 6-DoF robot and the extrinsic parameters of a 2D Laser Range Finder (LRF) which is attached to the robot. Three flat planes are placed around the robot, and for each plane the robot moves to several poses such that the LRF's ray intersect the respective plane. Geometric planar constraints are then used to optimize the calibration parameters using Levenberg- Marquardt nonlinear optimization algorithm. We demonstrate through simulations that SCALAR can reduce the average position and orientation errors of the robot system from 14.6mm and 4.05 degrees to 0.09mm and 0.02 degrees."
"Twitter adoption, students perceptions, Big Five personality traits and learning outcome: Lessons learned from 3 case studies","This study presents the results of the introduction of Twitter in the educational process. It examines the relationship of the tool s use with the participants learning outcome through a series of well-organized activities. Three studies were conducted in the context of 2 academic courses. The participation in the Twitter activity was voluntarily for the students. In all 3 studies the students who participated in the process had a higher laboratory grade than the students who did not participated. Students Conscientiousness and Openness to experience were related to their activity in one study. However, no relationship between the students personality traits and their grade was unveiled. Moreover, the students interventions in the activities are examined as well as the variation in their attitudes towards social media use in learning. The implications of the conducted studies are discussed extensively and a comparison with other related studies is presented."
Entropic theory of Gravitation,"  We construct a manifestly Machian theory of gravitation on the foundation that information in the universe cannot be destroyed (Landauer's principle). If no bit of information in the Universe is lost, than the sum of the entropies of the geometric and the matter fields should be conserved. We propose a local invariant expression for the entropy of the geometric field and formulate a variational principle on the entropic functional which produces entropic field equations. This information-theoretic approach implies that the geometric field does not exist in an empty of matter Universe, the material entropy is geometry dependent, matter can exchange information (entropy) with the geometric field and a quantum condensate can channel energy into the geometric field at a particular coherent state. The entropic field equations feature a non-intuitive direct coupling between the material fields and the geometric field, which acts as an entropy reservoir. Cosmological consequences such as the emergence of the cosmological constant as well as experimental consequences involving gravity-quantum condensate interaction are discussed. The energetic aspect of the theory restores the repertoire of the classical General Relativity up to a different coupling constant between the fields. "
Understanding International Migration using Tensor Factorization,"Understanding human migration is of great interest to demographers and social scientists. User generated digital data has made it easier to study such patterns at a global scale. Geo coded Twitter data, in particular, has been shown to be a promising source to analyse large scale human migration. But given the scale of these datasets, a lot of manual effort has to be put into processing and getting actionable insights from this data. In this paper, we explore feasibility of using a new tool, tensor decomposition, to understand trends in global human migration. We model human migration as a three mode tensor, consisting of (origin country, destination country, time of migration) and apply CP decomposition to get meaningful low dimensional factors. Our experiments on a large Twitter dataset spanning 5 years and over 100M tweets show that we can extract meaningful migration patterns."
Fiber-dependent deautonomization of integrable 2D mappings and discrete Painlevé equations,"  It is well known that two-dimensional mappings preserving a rational elliptic fibration, like the Quispel-Roberts-Thompson mappings, can be deautonomized to discrete Painlevé equations. However, the dependence of this procedure on the choice of a particular elliptic fiber has not been sufficiently investigated. In this paper we establish a way of performing the deautonomization for a pair of an autonomous mapping and a fiber. %By choosing a particular Starting from a single autonomous mapping but varying the type of a chosen fiber, we obtain different types of discrete Painlevé equations using this deautonomization procedure. We also introduce a technique for reconstructing a mapping from the knowledge of its induced action on the Picard group and some additional geometric data. This technique allows us to obtain factorized expressions of discrete Painlevé equations, including the elliptic case. Further, by imposing certain restrictions on such non-autonomous mappings we obtain new and simple elliptic difference Painlevé equations, including examples whose symmetry groups do not appear explicitly in Sakai's classification. "
ZnO and ZnO$_{1-x}$ based thin film memristors: The effects of oxygen deficiency and thickness in resistive switching behavior,"In this study, direct-current reactive sputtered ZnO and ZnO1-x based thin film (30 nm and 300 nm in thickness) memristor devices were produced and the effects of oxygen vacancies and thickness on the memristive characteristics were investigated. The oxygen deficiency of the ZnO1-x structure was confirmed by SIMS analyses. The memristive characteristics of both the ZnO and ZnO1-x devices were determined by time dependent current-voltage (I-V-t) measurements. The distinctive pinched hysteresis I-V loops of memristors were observed in all the fabricated devices. The typical homogeneous interface and filamentary types of memristive behaviors were compared. In addition, conduction mechanisms, on/off ratios and the compliance current were analyzed. The 30 nm ZnO based devices with native oxygen vacancies showed the best on/off ratio. All of the devices exhibited dominant Schottky emissions and weaker Poole-Frenkel conduction mechanisms. Results suggested that the oxygen deficiency was responsible for the Schottky emission mechanism. Moreover, the compliance currents of the devices were related to the decreasing power consumption as the oxygen vacancies increased."
Infrastructure Quality Assessment in Africa using Satellite Imagery and Deep Learning,"The UN Sustainable Development Goals allude to the importance of infrastructure quality in three of its seventeen goals. However, monitoring infrastructure quality in developing regions remains prohibitively expensive and impedes efforts to measure progress toward these goals. To this end, we investigate the use of widely available remote sensing data for the prediction of infrastructure quality in Africa. We train a convolutional neural network to predict ground truth labels from the Afrobarometer Round 6 survey using Landsat 8 and Sentinel 1 satellite imagery. Our best models predict infrastructure quality with AUROC scores of 0.881 on Electricity, 0.862 on Sewerage, 0.739 on Piped Water, and 0.786 on Roads using Landsat 8. These performances are significantly better than models that leverage OpenStreetMap or nighttime light intensity on the same tasks. We also demonstrate that our trained model can accurately make predictions in an unseen country after fine-tuning on a small sample of images. Furthermore, the model can be deployed in regions with limited samples to predict infrastructure outcomes with higher performance than nearest neighbor spatial interpolation."
Fréchet Means and Procrustes Analysis in Wasserstein Space,"  We consider two statistical problems at the intersection of functional and non-Euclidean data analysis: the determination of a Fréchet mean in the Wasserstein space of multivariate distributions; and the optimal registration of deformed random measures and point processes. We elucidate how the two problems are linked, each being in a sense dual to the other. We first study the finite sample version of the problem in the continuum. Exploiting the tangent bundle structure of Wasserstein space, we deduce the Fréchet mean via gradient descent. We show that this is equivalent to a Procrustes analysis for the registration maps, thus only requiring successive solutions to pairwise optimal coupling problems. We then study the population version of the problem, focussing on inference and stability: in practice, the data are i.i.d. realisations from a law on Wasserstein space, and indeed their observation is discrete, where one observes a proxy finite sample or point process. We construct regularised nonparametric estimators, and prove their consistency for the population mean, and uniform consistency for the population Procrustes registration maps. "
Sliding-Window Superposition Coding:Two-User Interference Channels,"  A low-complexity coding scheme is developed to achieve the rate region of maximum likelihood decoding for interference channels. As in the classical rate-splitting multiple access scheme by Grant, Rimoldi, Urbanke, and Whiting, the proposed coding scheme uses superposition of multiple codewords with successive cancellation decoding, which can be implemented using standard point-to-point encoders and decoders. Unlike rate-splitting multiple access, which is not rate-optimal for multiple receivers, the proposed coding scheme transmits codewords over multiple blocks in a staggered manner and recovers them successively over sliding decoding windows, achieving the single-stream optimal rate region as well as the more general Han--Kobayashi inner bound for the two-user interference channel. The feasibility of this scheme in practice is verified by implementing it using commercial channel codes over the two-user Gaussian interference channel. "
Introspective Generative Modeling: Decide Discriminatively,"  We study unsupervised learning by developing introspective generative modeling (IGM) that attains a generator using progressively learned deep convolutional neural networks. The generator is itself a discriminator, capable of introspection: being able to self-evaluate the difference between its generated samples and the given training data. When followed by repeated discriminative learning, desirable properties of modern discriminative classifiers are directly inherited by the generator. IGM learns a cascade of CNN classifiers using a synthesis-by-classification algorithm. In the experiments, we observe encouraging results on a number of applications including texture modeling, artistic style transferring, face modeling, and semi-supervised learning. "
Counting submodules of a module over a noetherian commutative ring,"  We count the number of submodules of an arbitrary module over a countable noetherian commutative ring. We give, along the way, a structural description of meager modules, which are defined as those that do not have the square of a simple module as subquotient, and deduce in particular a characterization of uniserial modules over commutative noetherian rings. "
Modelling the Influence of Cultural Information on Vision-Based Human Home Activity Recognition,"  Daily life activities, such as eating and sleeping, are deeply influenced by a person's culture, hence generating differences in the way a same activity is performed by individuals belonging to different cultures. We argue that taking cultural information into account can improve the performance of systems for the automated recognition of human activities. We propose four different solutions to the problem and present a system which uses a Naive Bayes model to associate cultural information with semantic information extracted from still images. Preliminary experiments with a dataset of images of individuals lying on the floor, sleeping on a futon and sleeping on a bed suggest that: i) solutions explicitly taking cultural information into account are more accurate than culture-unaware solutions; and ii) the proposed system is a promising starting point for the development of culture-aware Human Activity Recognition methods. "
Image classification using local tensor singular value decompositions,"  From linear classifiers to neural networks, image classification has been a widely explored topic in mathematics, and many algorithms have proven to be effective classifiers. However, the most accurate classifiers typically have significantly high storage costs, or require complicated procedures that may be computationally expensive. We present a novel (nonlinear) classification approach using truncation of local tensor singular value decompositions (tSVD) that robustly offers accurate results, while maintaining manageable storage costs. Our approach takes advantage of the optimality of the representation under the tensor algebra described to determine to which class an image belongs. We extend our approach to a method that can determine specific pairwise match scores, which could be useful in, for example, object recognition problems where pose/position are different. We demonstrate the promise of our new techniques on the MNIST data set. "
Short term unpredictability of high Reynolds number turbulence --- rough dependence on initial data,"  Short term unpredictability is discovered numerically for high Reynolds number fluid flows under periodic boundary conditions. Furthermore, the abundance of the short term unpredictability is also discovered. These discoveries support our theory that fully developed turbulence is constantly driven by such short term unpredictability. "
A Three-Dimensional Mathematical Model of Collagen Contraction,"In this paper, we introduce a three-dimensional mathematical model of collagen contraction with microbuckling based on the two-dimensional model previously developed by the authors. The model both qualitatively and quantitatively replicates experimental data including lattice contraction over a time course of 40 hours for lattices with various cell densities, cell density profiles within contracted lattices, radial cut angles in lattices, and cell force propagation within a lattice. The importance of the model lattice formation and the crucial nature of its connectivity are discussed including differences with models which do not include microbuckling. The model suggests that most cells within contracting lattices are engaged in directed motion."
"Freeze Casting: A Review of Processing, Microstructure and Properties via the Open Data Repository, FreezeCasting.net","Freeze-casting produces materials with complex, three-dimensional pore structures which may be tuned during the solidification process. The range of potential applications of freeze-cast materials is vast, and includes: structural materials, biomaterials, filtration membranes, pharmaceuticals, and foodstuffs. Fabrication of materials with application-specific microstructures is possible via freeze casting, however, the templating process is highly complex and the underlying principles are only partially understood. Here, we report the creation of a freeze-casting experimental data repository, which contains data extracted from ~800 different freeze-casting papers (as of August 2017). These data pertain to variables that link processing conditions to microstructural characteristics, and finally, mechanical properties. The aim of this work is to facilitate broad dissemination of relevant data to freeze-casting researchers, promote better informed experimental design, and encourage modeling efforts that relate processing conditions to microstructure formation and material properties. An initial, systematic analysis of these data is provided and key processing-structure-property relationships posited in the freeze-casting literature are discussed and tested against the database. Tools for data visualization and exploration available through the web interface are also provided."
On existence and approximation of solution of nonlinear Hilfer fractional differential equation,"  This paper gives the existence and uniqueness results for solution of fractional differential equations with Hilfer derivative. Using some new techniques and generalizing the restrictive conditions imposed on considered function, the iterative scheme for uniformly approximating the solution is established. "
The congruence subgroup problem for a family of branch groups,"We construct a family of groups which generalize the Hanoi towers group and study the congruence subgroup problem for the groups in this family. We show that unlike the Hanoi towers group, the groups in this generalization are just infinite and have trivial rigid kernel. We also put strict bounds on the branch kernel. Additionally, we show that these groups have subgroups of finite index with non-trivial rigid kernel. The only previously known group where this kernel is non-trivial is the Hanoi towers group and so this adds infinitely many new examples. Finally, we show that the topological closures of these groups have Hausdorff dimension arbitrarily close to 1."
Character formulae in category $\mathcal O$ for exceptional Lie superalgebras $D(2|1;ζ)$,"We establish character formulae for representations of the one-parameter family of simple Lie superalgebras $D(2|1;\zeta)$. We provide a complete description of the Verma flag multiplicities of the tilting modules and the projective modules in the BGG category $\mathcal O$ of $D(2|1;\zeta)$-modules of integral weights, for any complex parameter $\zeta$. The composition factors of all Verma modules in $\mathcal O$ are then obtained."
The M33 Synoptic Stellar Survey. II. Mira Variables,"We present the discovery of 1847 Mira candidates in the Local Group galaxy M33 using a novel semi-parametric periodogram technique coupled with a Random Forest classifier. The algorithms were applied to ~2.4x10^5 I-band light curves previously obtained by the M33 Synoptic Stellar Survey. We derive preliminary Period-Luminosity relations at optical, near- & mid-infrared wavelengths and compare them to the corresponding relations in the Large Magellanic Cloud."
Photometry of the long period dwarf nova GY Hya,"Although comparatively bright, the cataclysmic variable GY Hya has not attracted much attention in the past. As part of a project to better characterize such systems photometrically, we observed light curves in white light, each spanning several hours, at Bronberg Observatory, South Africa, in 2004 and 2005, and at the Observatório do Pico dos Dias, Brazil, in 2014 and 2016. These data permit to study orbital modulations and their variations from season to season. The orbital period, already known from spectroscopic observations of Peters & Thorstensen (2005), is confirmed through strong ellipsoidal variations of the mass donor star in the system and the presence of eclipses of both components. A refined period of 0.34723972~(6) days and revised ephemeris are derived. Seasonal changes in the average orbital light curve can qualitatively be explained by variations of the contribution of a hot spot to the system light together with changes of the disk radius. The amplitude of the ellipsoidal variations and the eclipse contact phases permit to put some constraints on the mass ratio, orbital inclination and the relative brightness of the primary and secondary components. There are some indications that the disk radius during quiescence, expressed in units of the component separation, is smaller than in other dwarf novae."
Network Structure Explains the Impact of Attitudes on Voting Decisions,"Attitudes can have a profound impact on socially relevant behaviours, such as voting. However, this effect is not uniform across situations or individuals, and it is at present difficult to predict whether attitudes will predict behaviour in any given circumstance. Using a network model, we demonstrate that (a) more strongly connected attitude networks have a stronger impact on behaviour, and (b) within any given attitude network, the most central attitude elements have the strongest impact. We test these hypotheses using data on voting and attitudes toward presidential candidates in the US presidential elections from 1980 to 2012. These analyses confirm that the predictive value of attitude networks depends almost entirely on their level of connectivity, with more central attitude elements having stronger impact. The impact of attitudes on voting behaviour can thus be reliably determined before elections take place by using network analyses."
Causal inference for social network data,"We extend recent work by van der Laan (2014) on causal inference for causally connected units to more general social network settings. Our asymptotic results allow for dependence of each observation on a growing number of other units as sample size increases. We are not aware of any previous methods for inference about network members in observational settings that allow the number of ties per node to increase as the network grows. While previous methods have generally implicitly focused on one of two possible sources of dependence among social network observations, we allow for both dependence due to contagion, or transmission of information across network ties, and for dependence due to latent similarities among nodes sharing ties. We describe estimation and inference for causal effects that are specifically of interest in social network settings."
Differential equations and the algebra of confluent spherical functions on semisimple Lie groups,"We consider the notion of a confluent spherical function on a connected semisimple Lie group, $G,$ with finite center and of real rank $1,$ and discuss the properties and relationship of its algebra with the well-known Schwartz algebra of spherical functions on $G.$"
AMORPH: A statistical program for characterizing amorphous materials by X-ray diffraction,"  AMORPH utilizes a new Bayesian statistical approach to interpreting X-ray diffraction results of samples with both crystalline and amorphous components. AMORPH fits X-ray diffraction patterns with a mixture of narrow and wide components, simultaneously inferring all of the model parameters and quantifying their uncertainties. The program simulates background patterns previously applied manually, providing reproducible results, and significantly reducing inter- and intra-user biases. This approach allows for the quantification of amorphous and crystalline materials and for the characterization of the amorphous component, including properties such as the centre of mass, width, skewness, and nongaussianity of the amorphous component. Results demonstrate the applicability of this program for calculating amorphous contents of volcanic materials and independently modeling their properties in compositionally variable materials. "
Unsupervised Adaptation with Domain Separation Networks for Robust Speech Recognition,"Unsupervised domain adaptation of speech signal aims at adapting a well-trained source-domain acoustic model to the unlabeled data from target domain. This can be achieved by adversarial training of deep neural network (DNN) acoustic models to learn an intermediate deep representation that is both senone-discriminative and domain-invariant. Specifically, the DNN is trained to jointly optimize the primary task of senone classification and the secondary task of domain classification with adversarial objective functions. In this work, instead of only focusing on learning a domain-invariant feature (i.e. the shared component between domains), we also characterize the difference between the source and target domain distributions by explicitly modeling the private component of each domain through a private component extractor DNN. The private component is trained to be orthogonal with the shared component and thus implicitly increases the degree of domain-invariance of the shared component. A reconstructor DNN is used to reconstruct the original speech feature from the private and shared components as a regularization. This domain separation framework is applied to the unsupervised environment adaptation task and achieved 11.08% relative WER reduction from the gradient reversal layer training, a representative adversarial training method, for automatic speech recognition on CHiME-3 dataset."
Robust Power System Dynamic State Estimator with Non-Gaussian Measurement Noise: Part I--Theory,"  This paper develops the theoretical framework and the equations of a new robust Generalized Maximum-likelihood-type Unscented Kalman Filter (GM-UKF) that is able to suppress observation and innovation outliers while filtering out non-Gaussian measurement noise. Because the errors of the real and reactive power measurements calculated using Phasor Measurement Units (PMUs) follow long-tailed probability distributions, the conventional UKF provides strongly biased state estimates since it relies on the weighted least squares estimator. By contrast, the state estimates and residuals of our GM-UKF are proved to be roughly Gaussian, allowing the sigma points to reliably approximate the mean and the covariance matrices of the predicted and corrected state vectors. To develop our GM-UKF, we first derive a batch-mode regression form by processing the predictions and observations simultaneously, where the statistical linearization approach is used. We show that the set of equations so derived are equivalent to those of the unscented transformation. Then, a robust GM-estimator that minimizes a convex Huber cost function while using weights calculated via Projection Statistics (PS's) is proposed. The PS's are applied to a two-dimensional matrix that consists of serially correlated predicted state and innovation vectors to detect observation and innovation outliers. These outliers are suppressed by the GM-estimator using the iteratively reweighted least squares algorithm. Finally, the asymptotic error covariance matrix of the GM-UKF state estimates is derived from the total influence function. In the companion paper, extensive simulation results will be shown to verify the effectiveness and robustness of the proposed method. "
Equivariant Algebraic Index Theorem,"  We prove a {\Gamma}-equivariant version of the algebraic index theorem, where {\Gamma} is a discrete group of automorphisms of a formal deformation of a symplectic manifold. The particular cases of this result are the algebraic version of the transversal index theorem related to the theorem of A. Connes and H. Moscovici for hypoelliptic operators and the index theorem for the extension of the algebra of pseudodifferential operators by a group of diffeomorphisms of the underlying manifold due to A. Savin, B. Sternin, E. Schrohe and D. Perrot. "
Partially-averaged Navier-Stokes (PANS) Method for Turbulence Simulations: Near-wall Modeling and Smooth-surface Separation Computations,"The goal of this dissertation is to investigate the PANS model capabilities in providing significant improvement over RANS predictions at slightly higher computational expense and producing LES quality results at significantly lower computational cost. The objectives of this study are: (i) investigate the model fidelity at a fixed level of scale resolution (Generation1-PANS/G1-PANS) for smooth surface separation, (ii) Derive the PANS closure model in regions of resolution variation (Generation2-PANS/G2-PANS), and (iii) Validate G2-PANS model for attached and separated flows. The separated flows considered in this study have been designated as critical benchmark flows by NASA CFD study group. The key contributions of this dissertation are summarized as follows. The turbulence closure model of varying resolution, G2-PANS, is developed by deriving mathematically-consistent commutation residues and using energy conservation principles. The log-layer recovery and accurate computation of Reynolds stress anisotropy is accomplished by transitioning from steady RANS to scaled resolved simulations using the G2-PANS model. Finally, several smooth-separation flows on the NASA turbulence website have been computed with high degree of accuracy at a significantly reduced computational effort over LES using the G1-PANS and G2-PANS models."
Stochastic Approximation of Smooth and Strongly Convex Functions: Beyond the $O(1/T)$ Convergence Rate,"Stochastic approximation (SA) is a classical approach for stochastic convex optimization. Previous studies have demonstrated that the convergence rate of SA can be improved by introducing either smoothness or strong convexity condition. In this paper, we make use of smoothness and strong convexity simultaneously to boost the convergence rate. Let $\lambda$ be the modulus of strong convexity, $\kappa$ be the condition number, $F_*$ be the minimal risk, and $\alpha>1$ be some small constant. First, we demonstrate that, in expectation, an $O(1/[\lambda T^\alpha] + \kappa F_*/T)$ risk bound is attainable when $T = \Omega(\kappa^\alpha)$. Thus, when $F_*$ is small, the convergence rate could be faster than $O(1/[\lambda T])$ and approaches $O(1/[\lambda T^\alpha])$ in the ideal case. Second, to further benefit from small risk, we show that, in expectation, an $O(1/2^{T/\kappa}+F_*)$ risk bound is achievable. Thus, the excess risk reduces exponentially until reaching $O(F_*)$, and if $F_*=0$, we obtain a global linear convergence. Finally, we emphasize that our proof is constructive and each risk bound is equipped with an efficient stochastic algorithm attaining that bound."
A new approach to hierarchical data analysis: Targeted maximum likelihood estimation for the causal effect of a cluster-level exposure,"  We often seek to estimate the impact of an exposure naturally occurring or randomly assigned at the cluster-level. For example, the literature on neighborhood determinants of health continues to grow. Likewise, community randomized trials are applied to learn about real-world implementation, sustainability, and population effects of interventions with proven individual-level efficacy. In these settings, individual-level outcomes are correlated due to shared cluster-level factors, including the exposure, as well as social or biological interactions between individuals. To flexibly and efficiently estimate the effect of a cluster-level exposure, we present two targeted maximum likelihood estimators (TMLEs). The first TMLE is developed under a non-parametric causal model, which allows for arbitrary interactions between individuals within a cluster. These interactions include direct transmission of the outcome (i.e. contagion) and influence of one individual's covariates on another's outcome (i.e. covariate interference). The second TMLE is developed under a causal sub-model assuming the cluster-level and individual-specific covariates are sufficient to control for confounding. Simulations compare the alternative estimators and illustrate the potential gains from pairing individual-level risk factors and outcomes during estimation, while avoiding unwarranted assumptions. Our results suggest that estimation under the sub-model can result in bias and misleading inference in an observational setting. Incorporating working assumptions during estimation is more robust than assuming they hold in the underlying causal model. We illustrate our approach with an application to HIV prevention and treatment. "
Excitation spectrum and Density Matrix Renormalization Group iterations,"  We show that, in certain circumstances, exact excitation energies appear as locally site-independent (or flat) modes if one records the excitation spectrum of the effective Hamiltonian while sweeping through the lattice in the variational Matrix Product State formulation of the Density Matrix Renormalization Group (DMRG), a remarkable property since the effective Hamiltonian is only constructed to target the ground state. Conversely, modes that are very flat over several consecutive iterations are systematically found to correspond to faithful excitations. We suggest to use this property to extract accurate information about excited states using the standard ground state algorithm. The results are spectacular for critical systems, for which the low-energy conformal tower of states can be obtained very accurately at essentially no additional cost, as demonstrated by confirming the predictions of boundary conformal field theory for two simple minimal models - the transverse-field Ising model and the critical three-state Potts model. This approach is also very efficient to detect the quasi-degenerate low-energy excitations in topological phases, and to identify localized excitations in systems with impurities. Finally, using the variance of the Hamiltonian as a criterion, we assess the accuracy of the resulting Matrix Product State representations of the excited states. "
Minimax estimation in linear models with unknown finite alphabet design,"We provide minimax theory for joint estimation of $F$ and $\omega$ in linear models $Y = F \omega + Z$ where the parameter matrix $\omega$ and the design matrix $F$ are unknown but the latter takes values in a known finite set. We show that this allows to separate $F$ and $\omega$ uniquely under weak identifiability conditions, a task which is not doable, in general. These assumptions are justified in a variety of applications, ranging from signal processing to cancer genetics. We then obtain in the noiseless case, that is, $Z = 0$, stable recovery of $F$ and $\omega$ in a neighborhood of $Y$. Based on this, we show for Gaussian error matrix $Z$ that the LSE attains minimax rates for both, prediction error of $F \omega$ and estimation error of $F$ and $\omega$, separately. Due to the finite alphabet, estimation of $F$ amounts to a classification problem, where we show that the classification error $P(\hat{F} \neq F)$ decreases exponentially in the dimension of one component of $Y$."
Rooted trees with the same plucking polynomial,"  In this paper we give a sufficient and necessary condition for two rooted trees with the same plucking polynomial. Furthermore, we give a criteria for a sequence of non-negative integers to be realized as a rooted tree. "
CityPersons: A Diverse Dataset for Pedestrian Detection,"  Convnets have enabled significant progress in pedestrian detection recently, but there are still open questions regarding suitable architectures and training data. We revisit CNN design and point out key adaptations, enabling plain FasterRCNN to obtain state-of-the-art results on the Caltech dataset. To achieve further improvement from more and better data, we introduce CityPersons, a new set of person annotations on top of the Cityscapes dataset. The diversity of CityPersons allows us for the first time to train one single CNN model that generalizes well over multiple benchmarks. Moreover, with additional training with CityPersons, we obtain top results using FasterRCNN on Caltech, improving especially for more difficult cases (heavy occlusion and small scale) and providing higher localization quality. "
The Better Half of Selling Separately,"Separate selling of two independent goods is shown to yield at least 62% of the optimal revenue, and at least 73% when the goods satisfy the Myerson regularity condition. This improves the 50% result of Hart and Nisan (2017, originally circulated in 2012)."
Coulomb GANs: Provably Optimal Nash Equilibria via Potential Fields,"  Generative adversarial networks (GANs) evolved into one of the most successful unsupervised techniques for generating realistic images. Even though it has recently been shown that GAN training converges, GAN models often end up in local Nash equilibria that are associated with mode collapse or otherwise fail to model the target distribution. We introduce Coulomb GANs, which pose the GAN learning problem as a potential field of charged particles, where generated samples are attracted to training set samples but repel each other. The discriminator learns a potential field while the generator decreases the energy by moving its samples along the vector (force) field determined by the gradient of the potential field. Through decreasing the energy, the GAN model learns to generate samples according to the whole target distribution and does not only cover some of its modes. We prove that Coulomb GANs possess only one Nash equilibrium which is optimal in the sense that the model distribution equals the target distribution. We show the efficacy of Coulomb GANs on a variety of image datasets. On LSUN and celebA, Coulomb GANs set a new state of the art and produce a previously unseen variety of different samples. "
Involution on pseudoisotopy spaces and the space of the nonnegatively curved metrics,"We prove that certain involutions defined by Vogell and Burghelea-Fiedorowicz on the rational algebraic K-theory of spaces coincide. This gives a way to compute the positive and negative eigenspaces of the involution on rational homotopy groups of pseudoisotopy spaces from the involution on rational $S^{1}$--homology group of the free loop space of a simply-connected manifold. As an application, we give explicit dimensions of the open manifolds $V$ that appear in Belegradek-Farrell-Kapovitch's work for which the spaces of complete nonnegatively curved metrics on $V$ have nontrivial rational homotopy groups."
A class of semisimple Hopf algebras acting on quantum polynomial algebras,"We construct a class of non-commutative, non-cocommutative, semisimple Hopf algebras of dimension $2n^2$ and present conditions to define an inner faithful action of these Hopf algebras on quantum polynomial algebras, providing, in this way, more examples of semisimple Hopf actions which do not factor through group actions. Also, under certain condition, we classify the inner faithful Hopf actions of the Kac-Paljutkin Hopf algebra of dimension $8$, $H_8$, on the quantum plane."
Fusion systems of blocks with nontrivial strongly closed subgroups,"  In this paper, we find some exotic fusion systems which have non-trivial strongly closed subgroups, and we prove these fusion systems are also not realizable by p-blocks of finite groups. "
Nonequilibrium steady states and transient dynamics of conventional superconductors under phonon driving,"  We perform a systematic analysis of the influence of phonon driving on the superconducting Holstein model coupled to heat baths by studying both the transient dynamics and the nonequilibrium steady state (NESS) in the weak and strong electron-phonon coupling regimes. Our study is based on the nonequilibrium dynamical mean-field theory, and for the NESS we present a Floquet formulation adapted to electron-phonon systems. The analysis of the phonon propagator suggests that the effective attractive interaction can be strongly enhanced in a parametric resonant regime because of the Floquet side bands of phonons. While this may be expected to enhance the superconductivity (SC), our fully self-consistent calculations, which include the effects of heating and nonthermal distributions, show that the parametric phonon driving generically results in a suppression or complete melting of the SC order. In the strong coupling regime, the NESS always shows a suppression of the SC gap, the SC order parameter and the superfluid density as a result of the driving, and this tendency is most prominent at the parametric resonance. Using the real-time nonequilibrium DMFT formalism, we also study the dynamics towards the NESS, which shows that the heating effect dominates the transient dynamics, and SC is weakened by the external modulations, in particular at the parametric resonance. In the weak coupling regime, we find that the SC fluctuations above the transition temperature are generally weakened under the driving. The strongest suppression occurs again around the parametric resonances because of the efficient energy absorption. "
Optimal Portfolio in Intraday Electricity Markets Modelled by Lévy-Ornstein-Uhlenbeck Processes,"  We study an optimal portfolio problem designed for an agent operating in intraday electricity markets. The investor is allowed to trade in a single risky asset modelling the continuously traded power and aims to maximize the expected terminal utility of his wealth. We assume a mean-reverting additive process to drive the power prices. In the case of logarithmic utility, we reduce the fully non-linear Hamilton-Jacobi-Bellman equation to a linear parabolic integro-differential equation, for which we explicitly exhibit a classical solution in two cases of modelling interest. The optimal strategy is given implicitly as the solution of an integral equation, which is possible to solve numerically as well as to describe analytically. An analysis of two different approximations for the optimal policy is provided. Finally, we perform a numerical test by adapting the parameters of a popular electricity spot price model. "
Topological phase transitions in small mesoscopic chiral p-wave superconductors,"  Spin-triplet chiral p-wave superconductivity is typically described by a two-component order parameter, and as such is prone to unique emergent effects when compared to the standard single-component superconductors. Here we present the equilibrium phase diagram for small mesoscopic chiral p-wave superconducting disks in the presence of magnetic field, obtained by solving the microscopic Bogoliubov-de Gennes equations self-consistently. In the ultra-small limit, the cylindrically-symmetric giant-vortex states are the ground state of the system. However, with increasing sample size, the cylindrical symmetry is broken as the two components of the order parameter segregate into domains, and the number of fragmented domain walls between them characterizes the resulting states. Such domain walls are topological defects unique for the p-wave order, and constitute a dominant phase in the mesoscopic regime. Moreover, we find two possible types of domain walls, identified by their chirality-dependent interaction with the edge states. "
Squeezing on momentum states for atom interferometry,"We propose and analyse a method that allows for the production of squeezed states of the atomic center-of-mass motion that can be injected into an atom interferometer. Our scheme employs dispersive probing in a ring resonator on a narrow transition of strontium atoms in order to provide a collective measurement of the relative population of two momentum states. We show that this method is applicable to a Bragg diffraction-based atom interferometer with large diffraction orders. The applicability of this technique can be extended also to small diffraction orders and large atom numbers by inducing atomic transparency at the frequency of the probe field, reaching an interferometer phase resolution scaling $\Delta\phi\sim N^{-3/4}$, where $N$ is the atom number. We show that for realistic parameters it is possible to obtain a 20 dB gain in interferometer phase estimation compared to the Standard Quantum Limit."
Synthesis of Near-regular Natural Textures,"  Texture synthesis is widely used in the field of computer graphics, vision, and image processing. In the present paper, a texture synthesis algorithm is proposed for near-regular natural textures with the help of a representative periodic pattern extracted from the input textures using distance matching function. Local texture statistics is then analyzed against global texture statistics for non-overlapping windows of size same as periodic pattern size and a representative periodic pattern is extracted from the image and used for texture synthesis, while preserving the global regularity and visual appearance. Validation of the algorithm based on experiments with synthetic textures whose periodic pattern sizes are known and containing camouflages / defects proves the strength of the algorithm for texture synthesis and its application in detection of camouflages / defects in textures. "
The Uncertainty Bellman Equation and Exploration,"We consider the exploration/exploitation problem in reinforcement learning. For exploitation, it is well known that the Bellman equation connects the value at any time-step to the expected value at subsequent time-steps. In this paper we consider a similar \textit{uncertainty} Bellman equation (UBE), which connects the uncertainty at any time-step to the expected uncertainties at subsequent time-steps, thereby extending the potential exploratory benefit of a policy beyond individual time-steps. We prove that the unique fixed point of the UBE yields an upper bound on the variance of the posterior distribution of the Q-values induced by any policy. This bound can be much tighter than traditional count-based bonuses that compound standard deviation rather than variance. Importantly, and unlike several existing approaches to optimism, this method scales naturally to large systems with complex generalization. Substituting our UBE-exploration strategy for $\epsilon$-greedy improves DQN performance on 51 out of 57 games in the Atari suite."
Syntax-Preserving Belief Change Operators for Logic Programs,"  Recent methods have adapted the well-established AGM and belief base frameworks for belief change to cover belief revision in logic programs. In this study here, we present two new sets of belief change operators for logic programs. They focus on preserving the explicit relationships expressed in the rules of a program, a feature that is missing in purely semantic approaches that consider programs only in their entirety. In particular, operators of the latter class fail to satisfy preservation and support, two important properties for belief change in logic programs required to ensure intuitive results. We address this shortcoming of existing approaches by introducing partial meet and ensconcement constructions for logic program belief change, which allow us to define syntax-preserving operators that satisfy preservation and support. Our work is novel in that our constructions not only preserve more information from a logic program during a change operation than existing ones, but they also facilitate natural definitions of contraction operators, the first in the field to the best of our knowledge. In order to evaluate the rationality of our operators, we translate the revision and contraction postulates from the AGM and belief base frameworks to the logic programming setting. We show that our operators fully comply with the belief base framework and formally state the interdefinability between our operators. We further propose an algorithm that is based on modularising a logic program to reduce partial meet and ensconcement revisions or contractions to performing the operation only on the relevant modules of that program. Finally, we compare our approach to two state-of-the-art logic program revision methods and demonstrate that our operators address the shortcomings of one and generalise the other method. "
Mapping degrees between spherical $3$-manifolds,"Let $D(M,N)$ be the set of integers that can be realized as the degree of a map between two closed connected orientable manifolds $M$ and $N$ of the same dimension. For closed $3$-manifolds with $S^3$-geometry $M$ and $N$, every such degree $deg f\equiv \overline{deg}\psi$ $(|\pi_1(N)|)$ where $0\le \overline{deg}\psi <|\pi_1(N)|$ and $\overline{deg}\psi$ only depends on the induced homomorphism $\psi=f_{\pi}$ on the fundamental group. In this paper, we calculate explicitly the set $\{\overline{deg}\psi\}$ when $\psi$ is surjective and then we show how to determine $\overline{deg}(\psi)$ for arbitrary homomorphisms. This leads to the determination of the set $D(M,N)$."
Estimating graph parameters with random walks,"An algorithm observes the trajectories of random walks over an unknown graph $G$, starting from the same vertex $x$, as well as the degrees along the trajectories. For all finite connected graphs, one can estimate the number of edges $m$ up to a bounded factor in $O\left(t_{\mathrm{rel}}^{3/4}\sqrt{m/d}\right)$ steps, where $t_{\mathrm{rel}}$ is the relaxation time of the lazy random walk on $G$ and $d$ is the minimum degree in $G$. Alternatively, $m$ can be estimated in $O\left(t_{\mathrm{unif}} +t_{\mathrm{rel}}^{5/6}\sqrt{n}\right)$, where $n$ is the number of vertices and $t_{\mathrm{unif}}$ is the uniform mixing time on $G$. The number of vertices $n$ can then be estimated up to a bounded factor in an additional $O\left(t_{\mathrm{unif}}\frac{m}{n}\right)$ steps. Our algorithms are based on counting the number of intersections of random walk paths $X,Y$, i.e. the number of pairs $(t,s)$ such that $X_t=Y_s$. This improves on previous estimates which only consider collisions (i.e., times $t$ with $X_t=Y_t$). We also show that the complexity of our algorithms is optimal, even when restricting to graphs with a prescribed relaxation time. Finally, we show that, given either $m$ or the mixing time of $G$, we can compute the ""other parameter"" with a self-stopping algorithm."
RoI-based Robotic Grasp Detection in Object Overlapping Scenes Using Convolutional Neural Network,"Grasp detection is an essential skill for widespread use of robots. Recent works demonstrate the advanced performance of Convolutional Neural Network (CNN) on robotic grasp detection. However, a significant shortcoming of existing grasp detection algorithms is that they all ignore the affiliation between grasps and targets. In this paper, we propose a robotic grasp detection algorithm based on Region of Interest (RoI) to simultaneously detect targets and their grasps in object overlapping scenes. Our proposed algorithm uses Regions of Interest (RoIs) to detect grasps while doing classification and location regression of targets. To train the network, we contribute a much bigger multi-object grasp dataset than Cornell Grasp Dataset, which is based on Visual Manipulation Relationship Dataset. Experimental results demonstrate that our algorithm achieves 24.9% miss rate at 1FPPI and 68.2% mAP with grasp on our dataset. Robotic experiments demonstrate that our proposed algorithm can help robots grasp specified target in multi-object scenes at 84% success rate."
Light Source Point Cluster Selection Based Atmosphere Light Estimation,"  Atmosphere light value is a highly critical parameter in defogging algorithms that are based on an atmosphere scattering model. Any error in atmosphere light value will produce a direct impact on the accuracy of scattering computation and thus bring chromatic distortion to restored images. To address this problem, this paper propose a method that relies on clustering statistics to estimate atmosphere light value. It starts by selecting in the original image some potential atmosphere light source points, which are grouped into point clusters by means of clustering technique. From these clusters, a number of clusters containing candidate atmosphere light source points are selected, the points are then analyzed statistically, and the cluster containing the most candidate points is used for estimating atmosphere light value. The mean brightness vector of the candidate atmosphere light points in the chosen point cluster is taken as the estimate of atmosphere light value, while their geometric center in the image is accepted as the location of atmosphere light. Experimental results suggest that this statistics clustering method produces more accurate atmosphere brightness vectors and light source locations. This accuracy translates to, from a subjective perspective, more natural defogging effect on the one hand and to the improvement in various objective image quality indicators on the other hand. "
Complete Submodularity Characterization in the Comparative Independent Cascade Model,"  We study the propagation of comparative ideas or items in social networks. A full characterization for submodularity in the comparative independent cascade (Com-IC) model of two-idea cascade is given, for competing ideas and complementary ideas respectively, with or without reconsideration. We further introduce One-Shot model where agents show less patience toward ideas, and show that in One-Shot model, only the strongest idea spreads with submodularity. "
Physics-guided probabilistic modeling of extreme precipitation under climate change,"  Earth System Models (ESMs) are the state of the art for projecting the effects of climate change. However, longstanding uncertainties in their ability to simulate regional and local precipitation extremes and related processes inhibit decision making. Stakeholders would be best supported by probabilistic projections of changes in extreme precipitation at relevant space-time scales. Here we propose an empirical Bayesian model that extends an existing skill and consensus based weighting framework and test the hypothesis that nontrivial, physics-guided measures of ESM skill can help produce reliable probabilistic characterization of climate extremes. Specifically, the model leverages knowledge of physical relationships between temperature, atmospheric moisture capacity, and extreme precipitation intensity to iteratively weight and combine ESMs and estimate probability distributions of return levels. Out-of-sample validation shows evidence that the Bayesian model is a sound method for deriving reliable probabilistic projections. Beyond precipitation extremes, the framework may be a basis for a generic, physics-guided approach to modeling probability distributions of climate variables in general, extremes or otherwise. "
LISA Detection of Binary Black Holes in the Milky Way Galaxy,"Using the black hole merger rate inferred from LIGO, we calculate the abundance of tightly bound binary black holes in the Milky Way galaxy. Binaries with a small semimajor axis ($\lesssim 10 R_\odot$) originate at larger separations through conventional formation mechanisms and evolve as a result of gravitational wave emission. We find that LISA could detect them in the Milky Way. We also identify possible X-ray signatures of such binaries."
Compacton solutions and (non)integrability for nonlinear evolutionary PDEs associated with a chain of prestressed granules,"  We present the results of study of a nonlinear evolutionary PDE (more precisely, a one-parameter family of PDEs) associated with the chain of pre-stressed granules. The PDE in question supports solitary waves of compression and rarefaction (bright and dark compactons) and can be written in Hamiltonian form. We investigate {\em inter alia} integrability properties of this PDE and its generalized symmetries and conservation laws. For the compacton solutions we perform a stability test followed by the numerical study. In particular, we simulate the temporal evolution of a single compacton, and the interactions of compacton pairs. The results of numerical simulations performed for our model are compared with the numerical evolution of corresponding Cauchy data for the discrete model of chain of pre-stressed elastic granules. "
FormuLog: Datalog for static analysis involving logical formulae,"  Datalog has become a popular language for writing static analyses. Because Datalog is very limited, some implementations of Datalog for static analysis have extended it with new language features. However, even with these features it is hard or impossible to express a large class of analyses because they use logical formulae to represent program state. FormuLog fills this gap by extending Datalog to represent, manipulate, and reason about logical formulae. We have used FormuLog to implement declarative versions of symbolic execution and abstract model checking, analyses previously out of the scope of Datalog-based languages. While this paper focuses on the design of FormuLog and one of the analyses we have implemented in it, it also touches on a prototype implementation of the language and identifies performance optimizations that we believe will be necessary to scale FormuLog to real-world static analysis problems. "
A Double Parametric Bootstrap Test for Topic Models,"  Non-negative matrix factorization (NMF) is a technique for finding latent representations of data. The method has been applied to corpora to construct topic models. However, NMF has likelihood assumptions which are often violated by real document corpora. We present a double parametric bootstrap test for evaluating the fit of an NMF-based topic model based on the duality of the KL divergence and Poisson maximum likelihood estimation. The test correctly identifies whether a topic model based on an NMF approach yields reliable results in simulated and real data. "
Heat flows inferred from a Parker's-like formula for stable or quasi-stable continents,"Surface heat flow is a key parameter for the geothermal structure, rheology, and hence the dynamics of continents. However, the coverage of heat flow measurements is still poor in many continental areas. By transforming the stable nonlinear heat conduction equation into a Poisson's one, we develop a method to infer surface heat flow for a stable or quasi-stable continent from a Parker's-like formula. This formula provides the relationship between the Fourier transform of surface heat flow and the sum of the Fourier transform of the powers of geometry for the heat production (HP) interface in the continental lithosphere. Once the interface geometry is known, one to three dimensional distribution of the surface heat flow can be calculated accurately by this formula. As a case study, we estimate the three-dimensional surface heat flows for the Ordos geological block and its adjacent areas in China on a $1^\circ \times 1^\circ$ grid based on a simple layered constant HP model. Comparing to the measurements, most relative errors of the heat flows inferred are less than 20\%, showing this method is a favorable way to estimate surface heat flow for stable or quasi-stable continental regions where measurements are rare or absent."
Real-time 3D Reconstruction on Construction Site using Visual SLAM and UAV,"3D reconstruction can be used as a platform to monitor the performance of activities on construction site, such as construction progress monitoring, structure inspection and post-disaster rescue. Comparing to other sensors, RGB image has the advantages of low-cost, texture rich and easy to implement that has been used as the primary method for 3D reconstruction in construction industry. However, the image-based 3D reconstruction always requires extended time to acquire and/or to process the image data, which limits its application on time critical projects. Recent progress in Visual Simultaneous Localization and Mapping (SLAM) make it possible to reconstruct a 3D map of construction site in real-time. Integrated with Unmanned Aerial Vehicle (UAV), the obstacles areas that are inaccessible for the ground equipment can also be sensed. Despite these advantages of visual SLAM and UAV, until now, such technique has not been fully investigated on construction site. Therefore, the objective of this research is to present a pilot study of using visual SLAM and UAV for real-time construction site reconstruction. The system architecture and the experimental setup are introduced, and the preliminary results and the potential applications using Visual SLAM and UAV on construction site are discussed."
The Space of Transferable Adversarial Examples,"Adversarial examples are maliciously perturbed inputs designed to mislead machine learning (ML) models at test-time. They often transfer: the same adversarial example fools more than one model. In this work, we propose novel methods for estimating the previously unknown dimensionality of the space of adversarial inputs. We find that adversarial examples span a contiguous subspace of large (~25) dimensionality. Adversarial subspaces with higher dimensionality are more likely to intersect. We find that for two different models, a significant fraction of their subspaces is shared, thus enabling transferability. In the first quantitative analysis of the similarity of different models' decision boundaries, we show that these boundaries are actually close in arbitrary directions, whether adversarial or benign. We conclude by formally studying the limits of transferability. We derive (1) sufficient conditions on the data distribution that imply transferability for simple model classes and (2) examples of scenarios in which transfer does not occur. These findings indicate that it may be possible to design defenses against transfer-based attacks, even for models that are vulnerable to direct attacks."
Sensitivity of the entanglement spectrum to boundary conditions as a characterization of the phase transition from delocalization to localization,"  Sensitivity of entanglement Hamiltonian spectrum to boundary conditions is considered as a phase detection parameter for delocalized-localized phase transition. By employing one-dimensional models that undergo delocalized-localized phase transition, we study the shift in the entanglement energies and the shift in the entanglement entropy when we change boundary conditions from periodic to anti-periodic. Specifically, we show that both these quantities show a change of several orders of magnitude at the transition point in the models considered. Therefore, this shift can be used to indicate the phase transition points in the models. We also show that both these quantities can be used to determine \emph{mobility edges} separating localized and delocalized states. "
"Radial metal abundance profiles in the intra-cluster medium of cool-core galaxy clusters, groups, and ellipticals","The hot intra-cluster medium (ICM) permeating galaxy clusters and groups is not pristine, as it is continuously enriched by metals synthesised in Type Ia (SNIa) and core-collapse (SNcc) supernovae since the major epoch of star formation (z ~ 2-3). The cluster/group enrichment history and the mechanisms responsible for releasing and mixing the metals can be probed via the radial distribution of SNIa and SNcc products within the ICM. In this paper, we use deep XMM-Newton/EPIC observations from a sample of 44 nearby cool-core galaxy clusters, groups, and ellipticals (CHEERS) to constrain the average radial O, Mg, Si, S, Ar, Ca, Fe, and Ni abundance profiles. The radial distributions of all these elements, averaged over a large sample for the first time, represent the best constrained profiles available currently. We find an overall decrease of the Fe abundance with radius out to ~$0.9 r_{500}$ and ~$0.6 r_{500}$ for clusters and groups, respectively, in good agreement with predictions from the most recent hydrodynamical simulations. The average radial profiles of all the other elements (X) are also centrally peaked and, when rescaled to their average central X/Fe ratios, follow well the Fe profile out to at least ~0.5$r_{500}$. Using two sets of SNIa and SNcc yield models reproducing well the X/Fe abundance pattern in the core, we find that, as predicted by recent simulations, the relative contribution of SNIa (SNcc) to the total ICM enrichment is consistent with being uniform at all radii, both for clusters and groups. In addition to implying that the central metal peak is balanced between SNIa and SNcc, our results suggest that the enriching SNIa and SNcc products must share the same origin, and that the delay between the bulk of the SNIa and SNcc explosions must be shorter than the timescale necessary to diffuse out the metals."
Edge states in dynamical superlattices,"We address edge states and rich localization regimes available in the one-dimensional (1D) dynamically modulated superlattices, both theoretically and numerically. In contrast to conventional lattices with straight waveguides, the quasi-energy band of infinite modulated superlattice is periodic not only in the transverse Bloch momentum, but it also changes periodically with increase of the coupling strength between waveguides. Due to collapse of quasi-energy bands dynamical superlattices admit known dynamical localization effect. If, however, such a lattice is truncated, periodic longitudinal modulation leads to appearance of specific edge states that exist within certain periodically spaced intervals of coupling constants. We discuss unusual transport properties of such truncated superlattices and illustrate different excitation regimes and enhanced robustness of edge states in them, that is associated with topology of the quasi-energy band."
"On the Monitoring of Decentralized Specifications Semantics, Properties, Analysis, and Simulation","  We define two complementary approaches to monitor decentralized systems. The first relies on those with a centralized specification, i.e, when the specification is written for the behavior of the entire system. To do so, our approach introduces a data-structure that i) keeps track of the execution of an automaton, ii) has predictable parameters and size, and iii) guarantees strong eventual consistency. The second approach defines decentralized specifications wherein multiple specifications are provided for separate parts of the system. We study two properties of decentralized specifications pertaining to monitorability and compatibility between specification and architecture. We also present a general algorithm for monitoring decentralized specifications. We map three existing algorithms to our approaches and provide a framework for analyzing their behavior. Furthermore, we introduce THEMIS, a framework for designing such decentralized algorithms and simulating their behavior. We show the usage of THEMIS to compare multiple algorithms and verify the trends predicted by the analysis by studying two scenarios: a synthetic benchmark and a real example. "
Morphology and the Color-Mass Diagram as Clues to Galaxy Evolution at z~1,"We study the significance of mergers in the quenching of star formation in galaxies at z~1 by examining their color-mass distributions for different morphology types. We perform two-dimensional light profile fits to GOODS iz images of ~5000 galaxies and X-ray selected active galactic nucleus (AGN) hosts in the CANDELS/GOODS-north and south fields in the redshift range 0.7<z<1.3. Distinguishing between bulge-dominated and disk-dominated morphologies, we find that disks and spheroids have distinct color-mass distributions, in agreement with studies at z~0. The smooth distribution across colors for the disk galaxies corresponds to a slow exhaustion of gas, with no fast quenching event. Meanwhile, blue spheroids most likely come from major mergers of star-forming disk galaxies, and the dearth of spheroids at intermediate green colors is suggestive of rapid quenching. The distribution of moderate luminosity X-ray AGN hosts is even across colors, in contrast, and we find similar numbers and distributions among the two morphology types with no apparent dependence on Eddington ratio. The high fraction of bulge-dominated galaxies that host an AGN in the blue cloud and green valley is consistent with the scenario in which the AGN is triggered after a major merger, and the host galaxy then quickly evolves into the green valley. This suggests AGN feedback may play a role in the quenching of star formation in the minority of galaxies that undergo major mergers."
That's Mine! Learning Ownership Relations and Norms for Robots,"  The ability for autonomous agents to learn and conform to human norms is crucial for their safety and effectiveness in social environments. While recent work has led to frameworks for the representation and inference of simple social rules, research into norm learning remains at an exploratory stage. Here, we present a robotic system capable of representing, learning, and inferring ownership relations and norms. Ownership is represented as a graph of probabilistic relations between objects and their owners, along with a database of predicate-based norms that constrain the actions permissible on owned objects. To learn these norms and relations, our system integrates (i) a novel incremental norm learning algorithm capable of both one-shot learning and induction from specific examples, (ii) Bayesian inference of ownership relations in response to apparent rule violations, and (iii) percept-based prediction of an object's likely owners. Through a series of simulated and real-world experiments, we demonstrate the competence and flexibility of the system in performing object manipulation tasks that require a variety of norms to be followed, laying the groundwork for future research into the acquisition and application of social norms. "
Understanding liquid-jet atomization cascades via vortex dynamics,"Temporal instabilities of a planar liquid jet are studied using direct numerical simulation (DNS) of the incompressible Navier-Stokes equations with level-set (LS) and volume-of-fluid (VoF) surface tracking methods. $\lambda_2$ contours are used to relate the vortex dynamics to the surface dynamics at different stages of the jet breakup, namely, lobe formation, lobe perforation, ligament formation, stretching, and tearing. Three distinct breakup mechanisms are identified in the primary breakup, which are well categorized on the parameter space of gas Weber number ($We_g$) versus liquid Reynolds number ($Re_l$). These mechanisms are analyzed here from a vortex dynamics perspective. Vortex dynamics explains the hairpin formation, and the interaction between the hairpins and the Kelvin-Helmholtz (KH) roller explains the perforation of the lobes, which is attributed to the streamwise overlapping of two oppositely-oriented hairpin vortices on top and bottom of the lobe. The formation of corrugations on the lobe front edge at high $Re_l$ is also related to the location and structure of the hairpins with respect to the KH vortex. The lobe perforation and corrugation formation are inhibited at low $Re_l$ and low $We_g$ due to the high surface tension and viscous forces, which damp the small scale corrugations and resist hole formation. Streamwise vorticity generation - resulting in three-dimensional instabilities - is mainly caused by vortex stretching and baroclinic torque at high and low density ratios, respectively. Generation of streamwise vortices and their interaction with spanwise vortices produce the liquid structures seen at various flow conditions. Understanding the liquid sheet breakup and the related vortex dynamics are crucial for controlling the droplet size distribution in primary atomization."
Multi-kernel learning of deep convolutional features for action recognition,"Image understanding using deep convolutional network has reached human-level performance, yet a closely related problem of video understanding especially, action recognition has not reached the requisite level of maturity. We combine multi-kernels based support-vector-machines (SVM) with a multi-stream deep convolutional neural network to achieve close to state-of-the-art performance on a 51-class activity recognition problem (HMDB-51 dataset); this specific dataset has proved to be particularly challenging for deep neural networks due to the heterogeneity in camera viewpoints, video quality, etc. The resulting architecture is named pillar networks as each (very) deep neural network acts as a pillar for the hierarchical classifiers. In addition, we illustrate that hand-crafted features such as improved dense trajectories (iDT) and Multi-skip Feature Stacking (MIFS), as additional pillars, can further supplement the performance."
"Vector bundles for ""Matrix algebras converge to the sphere""","In the high-energy quantum-physics literature one finds statements such as ""matrix algebras converge to the sphere"". Earlier I provided a general precise setting for understanding such statements, in which the matrix algebras are viewed as quantum metric spaces, and convergence is with respect to a quantum Gromov-Hausdorff-type distance. But physicists want even more to treat structures on spheres (and other spaces), such as vector bundles, Yang-Mills functionals, Dirac operators, etc., and they want to approximate these by corresponding structures on matrix algebras. In the present paper we treat this idea for vector bundles. We develop a general precise way for understanding how, for two compact quantum metric spaces that are close together, to a given vector bundle on one of them there can correspond in a natural way a unique vector bundle on the other. We then show explicitly how this works for the case of matrix algebras converging to the 2-sphere."
Ehrlich-Schwoebel Effect on the Growth Dynamics of GaAs(111)A surfaces,We present a detailed characterization of the growth dynamics of Ga(Al)As(111)A surfaces. We develop a theoretical growth model that well describes the observed behavior on the growth parameters and underlines the Ehrlich-Schwoebel barrier as leading factor that determines the growth dynamics. On such basis we analyze the factors that lead to the huge observed roughness on such surface orientations and we identify the growth conditions that drive the typical three-dimensional growth of Ga(Al)As(111)A towards atomically flat surface. GaAs/AlGaAs quantum wells realized on optimized surface (<0.2 nm roughness) show a record low emission linewidth of 4.5 meV.
Warnings and Caveats in Brain Controllability,"In this work we challenge the main conclusions of Gu et al work (Controllability of structural brain networks. Nature communications 6, 8414, doi:10.1038/ncomms9414, 2015) on brain controllability. Using the same methods and analyses on four datasets we find that the minimum set of nodes to control brain networks is always larger than one. We also find that the relationships between the average/modal controllability and weighted degrees also hold for randomized data and the there are not specific roles played by Resting State Networks in controlling the brain. In conclusion, we show that there is no evidence that topology plays specific and unique roles in the controllability of brain networks. Accordingly, Gu et al. interpretation of their results, in particular in terms of translational applications (e.g. using single node controllability properties to define target region(s) for neurostimulation) should be revisited. Though theoretically intriguing, our understanding of the relationship between controllability and structural brain network remains elusive."
Novelty and Foreseeing Research Trends; The Case of Astrophysics and Astronomy,"  Metrics based on reference lists of research articles or on keywords have been used to predict citation impact. The concept behind such metrics is that original ideas stem from the reconfiguration of the structure of past knowledge, and therefore atypical combinations in the reference lists, keywords, or classification codes indicate future high impact research. The current paper serves as an introduction to this line of research for astronomers and also addresses some methodological questions of this field of innovation studies. It is still not clear if the choice of particular indexes, such as references to journals, articles, or specific bibliometric classification codes would affect the relationship between atypical combinations and citation impact. To understand more aspects of the innovation process, a new metric has been devised to measure to what extent researchers are able to anticipate the changing combinatorial trends of the future. Results show that the variant of the latter anticipation scores that is based on paper combinations is a good predictor of future citation impact of scholarly works. The study also shows that the effect of tested indexes vary with the aggregation level that was used to construct them. A detailed analysis of combinatorial novelty in the field reveals that certain sub-fields of astronomy and astrophysics have different roles in the reconfiguration in past knowledge. "
A Covert Queueing Channel in Round Robin Schedulers,"  We study a covert queueing channel (CQC) between two users sharing a round robin scheduler. Such a covert channel can arise when users share a resource such as a computer processor or a router arbitrated by a round robin policy. We present an information-theoretic framework to model and derive the maximum reliable data transmission rate, i.e., the capacity of this channel, for both noiseless and noisy setups. Our results show that seemingly isolated users can communicate with a high rate over the covert channel and demonstrate the possibility of significant information leakage and privacy threats brought by CQCs in round robin schedulers. Moreover, we propose practical finite-length code constructions, which achieve the capacity limit. "
Relative periodic orbits form the backbone of turbulent pipe flow,"  Chaotic dynamics of low-dimensional systems, such as Lorenz or Rössler flows, is guided by the infinity of periodic orbits embedded in their strange attractors. Whether this also be the case for the infinite-dimensional dynamics of Navier--Stokes equations has long been speculated, and is a topic of ongoing study. Periodic and relative periodic solutions have been shown to be involved in transitions to turbulence. Their relevance to turbulent dynamics---specifically, whether periodic orbits play the same role in high-dimensional nonlinear systems like the Navier--Stokes equations as they do in lower-dimensional systems---is the focus of the present investigation. We perform here a detailed study of pipe flow relative periodic orbits with energies and mean dissipations close to turbulent values. We outline several approaches to reduction of the translational symmetry of the system. We study pipe flow in a minimal computational cell, and report a library of invariant solutions found with the aid of the method of slices. Detailed study of the unstable manifolds of a sample of these solutions is consistent with the picture that relative periodic orbits are embedded in the chaotic saddle and that they guide the turbulent dynamics. "
Tunable hybridization of Majorana bound states at the quantum spin Hall edge,"  Confinement at the helical edge of a topological insulator is possible in the presence of proximity-induced magnetic (F) or superconducting (S) order. The interplay of both phenomena leads to the formation of localized Majorana bound states (MBS) or likewise (under certain resonance conditions) the formation of ordinary Andreev bound states (ABS). We investigate the properties of bound states in junctions composed of alternating regions of F or S barriers. Interestingly, the direction of magnetization in F regions and the relative superconducting phase between S regions can be exploited to hybridize MBS or ABS at will. We show that the local properties of MBS translate into a particular nonlocal superconducting pairing amplitude. Remarkably, the symmetry of the pairing amplitude contains information about the nature of the bound state that it stems from. Hence, this symmetry can in principle be used to distinguish MBS from ABS, owing to the strong connection between local density of states and nonlocal pairing in our setup. "
Vanishing lines for modules over the motivic Steenrod algebra,We study criteria for freeness and for the existence of a vanishing line for modules over certain Hopf subalgebras of the motivic Steenrod algebra over $\mathrm{Spec}(\mathbb{C})$ at the prime 2. These turn out to be determined by the vanishing of certain Margolis homology groups in the quotient Hopf algebra $\mathcal{A}/\tau$.
A 20-Channel Magnetoencephalography System Based on Optically Pumped Magnetometers,"We describe a multichannel magnetoencephalography (MEG) system that uses optically pumped magnetometers (OPMs) to sense the magnetic fields of the human brain. The system consists of an array of 20 OPM channels conforming to the human subject's head, a person-sized magnetic shield containing the array and the human subject, a laser system to drive the OPM array, and various control and data acquisitions systems. We conducted two MEG experiments: auditory evoked magnetic field (AEF) and somatosensory evoked magnetic field (SEF), on three healthy male subjects, using both our OPM array and a 306-channel Elekta-Neuromag superconducting quantum interference device (SQUID) MEG system. The described OPM array measures the tangential components of the magnetic field as opposed to the radial component measured by all SQUID-based MEG systems. Herein we compare the results of the OPM- and SQUID-based MEG systems on the auditory and somatosensory data recorded in the same individuals on both systems."
A universal negative group delay filter for the prediction of band-limited signals,"  A filter for universal real-time prediction of band-limited signals is presented. The filter consists of multiple time-delayed feedback terms in order to accomplish anticipatory coupling, which again leads to a negative group delay for frequencies in the baseband. The universality of the filter arises from its property that it does not rely on a specific model of the signal. Specifically, as long as the signal to be predicted is band-limited with a known cutoff frequency, the filter order, the only parameter of the filter, follows and the filter predicts the signal in real time up to a prediction horizon that depends on the cutoff frequency, too. It is worked out in detail how signal prediction arises from the negative group delay of the filter. Its properties, including stability, are investigated theoretically, by numerical simulations, and by application to a physiological signal. Possible control and signal processing applications of this filter are discussed. "
The odd primary order of the commutator on low rank Lie groups,"  Let $G$ be a simply-connected, compact, simple Lie group of low rank relative to a fixed prime $p$. After localization at $p$, there is a space $A$ which ""generates"" $G$ in a certain sense. Assuming $G$ satisfies a homotopy nilpotency condition relative to $p$, we show that the Samelson product $\langle Id_G, Id_G\rangle$ of the identity of $G$ equals the order of the Samelson product $\langle\imath,\imath\rangle$ of the inclusion $\imath:A\to G$. Applying this result, we calculate the orders of $\langle Id_G,Id_G\rangle$ for all $p$-regular Lie groups and give bounds on the orders of $\langle Id_G,Id_G\rangle$ for certain quasi-$p$-regular Lie groups. "
Communication Complexity of Estimating Correlations,"We characterize the communication complexity of the following distributed estimation problem. Alice and Bob observe infinitely many iid copies of $\rho$-correlated unit-variance (Gaussian or $\pm1$ binary) random variables, with unknown $\rho\in[-1,1]$. By interactively exchanging $k$ bits, Bob wants to produce an estimate $\hat\rho$ of $\rho$. We show that the best possible performance (optimized over interaction protocol $\Pi$ and estimator $\hat \rho$) satisfies $\inf_{\Pi,\hat\rho}\sup_\rho \mathbb{E} [|\rho-\hat\rho|^2] = \Theta(\tfrac{1}{k})$. Furthermore, we show that the best possible unbiased estimator achieves performance of $1+o(1)\over {2k\ln 2}$. Curiously, thus, restricting communication to $k$ bits results in (order-wise) similar minimax estimation error as restricting to $k$ samples. Our results also imply an $\Omega(n)$ lower bound on the information complexity of the Gap-Hamming problem, for which we show a direct information-theoretic proof. Notably, the protocol achieving (almost) optimal performance is one-way (non-interactive). For one-way protocols we also prove the $\Omega(\tfrac{1}{k})$ bound even when $\rho$ is restricted to any small open sub-interval of $[-1,1]$ (i.e. a local minimax lower bound). %We do not know if this local behavior remains true in the interactive setting. Our proof techniques rely on symmetric strong data-processing inequalities, various tensorization techniques from information-theoretic interactive common-randomness extraction, and (for the local lower bound) on the Otto-Villani estimate for the Wasserstein-continuity of trajectories of the Ornstein-Uhlenbeck semigroup."
"Postponement of raa and Glivenko's theorem, revisited (extended version)","  This article focuses on the technique of postponing the application of the reduction ad absurdum rule (raa) in classical natural deduction. First, it is shown how this technique is connected with two normalization strategies for classical logic: one given by Prawitz, and the other by Seldin. Secondly, a variant of Seldin's strategy for the postponement of raa is proposed, and the similarities with Prawitz's approach are investigated. In particular, it is shown that, as for Prawitz, it is possible to use this variant of Seldin's strategy in order to induce a negative translation from classical to intuitionistic and minimal logic, which is nothing but a variant of Kuroda's translation. Through this translation, Glivenko's theorem for intuitionistic and minimal logic is proven. "
Inference for the cross-covariance operator of stationary functional time series,"  When considering two or more time series of functions or curves, for instance those derived from densely observed intraday stock price data of several companies, the empirical cross-covariance operator is of fundamental importance due to its role in functional lagged regression and exploratory data analysis. Despite its relevance, statistical procedures for measuring the significance of such estimators are undeveloped. We present methodology based on a functional central limit theorem for conducting statistical inference for the cross-covariance operator estimated between two stationary, weakly dependent, functional time series. Specifically, we consider testing the null hypothesis that two series possess a specified cross-covariance structure at a given lag. Since this test assumes that the series are jointly stationary, we also develop a change-point detection procedure to validate this assumption, which is of independent interest. The most imposing technical hurdle in implementing the proposed tests involves estimating the spectrum of a high dimensional spectral density operator at frequency zero. We propose a simple dimension reduction procedure based on functional PCA to achieve this, which is shown to perform well in a small simulation study. We illustrate the proposed methodology with an application to densely observed intraday price data of stocks listed on the NYSE. "
Systematic Testing of Convolutional Neural Networks for Autonomous Driving,"  We present a framework to systematically analyze convolutional neural networks (CNNs) used in classification of cars in autonomous vehicles. Our analysis procedure comprises an image generator that produces synthetic pictures by sampling in a lower dimension image modification subspace and a suite of visualization tools. The image generator produces images which can be used to test the CNN and hence expose its vulnerabilities. The presented framework can be used to extract insights of the CNN classifier, compare across classification models, or generate training and validation datasets. "
Sufficient conditions for convergence of multiple Fourier series with $J_k$-lacunary sequence of rectangular partial sums in terms of Weyl multipliers,"We obtain sufficient conditions for convergence (almost everywhere) of multiple trigonometric Fourier series of functions $f$ in $L_2$ in terms of Weyl multipliers. We consider the case where rectangular partial sums of Fourier series $S_n(x;f)$ have indices $n=(n_1,\dots,n_N) \in \mathbb Z^N$, $N\ge 3$, in which $k$ $(1\leq k\leq N-2)$ components on the places $\{j_1,\dots,j_k\}=J_k \subset \{1,\dots,N\} = M$ are elements of (single) lacunary sequences (i.e., we consider the, so called, multiple Fourier series with $J_k$-lacunary sequence of partial sums). We prove that for any sample $J_k\subset M$ the Weyl multiplier for convergence of these series has the form $W(\nu)=\prod \limits_{j=1}^{N-k} \log(|\nu_{{\alpha}_j}|+2)$, where $\alpha_j\in M\setminus J_k $, $\nu=(\nu_1,\dots,\nu_N)\in{\mathbb Z}^N$. So, the ""one-dimensional"" Weyl multiplier -- $\log(|\cdot|+2)$ -- presents in $W(\nu)$ only on the places of ""free"" (nonlacunary) components of the vector $\nu$. Earlier, in the case where $N-1$ components of the index $n$ are elements of lacunary sequences, convergence almost everywhere for multiple Fourier series was obtained in 1977 by M.Kojima in the classes $L_p$, $p>1$, and by D.K.Sanadze, Sh.V.Kheladze in Orlizc class. Note, that presence of two or more ""free"" components in the index $n$ (as follows from the results by Ch.Fefferman (1971)) does not guarantee the convergence almost everywhere of $S_n(x;f)$ for $N\geq 3$ even in the class of continuous functions."
"Emergent phases in iron pnictides: Double-Q antiferromagnetism, charge order and enhanced nematic correlations","Electron correlations produce a rich phase diagram in the iron pnictides. Earlier theoretical studies on the correlation effect demonstrated how quantum fluctuations weaken and concurrently suppress a $C_2$-symmetric single-Q antiferromagnetic order and a nematic order. Here we examine the emergent phases near the quantum phase transition. For a $C_4$-symmetric collinear double-Q antiferromagnetic order, we show that it is accompanied by both a charge order and an enhanced nematic susceptibility. Our results provide understanding for several intriguing recent experiments in hole-doped iron arsenides, and bring out common physics that underlies the different magnetic phases of various iron-based superconductors."
Synergistic effects in threshold models on networks,"  Network structure can have significant effects on the propagation of diseases, memes, and information on social networks. Such effects depend on the specific type of dynamical process that affects the nodes and edges of a network, and it is important to develop tractable models of spreading processes on networks to explore how network structure affects dynamics. In this paper, we incorporate the idea of \emph{synergy} into a two-state (""active"" or ""passive"") threshold model of social influence on networks. Our model's update rule is deterministic, and the influence of each meme-carrying (i.e., active) neighbor can --- depending on a parameter --- either be enhanced or inhibited by an amount that depends on the number of active neighbors of a node. Such a synergistic system models social behavior in which the willingness to adopt either accelerates or saturates depending on the number of neighbors who have adopted that behavior. We illustrate that the synergy parameter in our model has a crucial effect on system dynamics, as it determines whether degree-$k$ nodes are possible or impossible to activate. We simulate synergistic meme spreading on both random-graph models and networks constructed from empirical data. Using a local-tree approximation, we examine the spreading of synergistic memes and find good agreement on all but one of the networks on which we simulate spreading. We find for any network and for a broad family of synergistic models that one can predict which synergy-parameter values allow degree-$k$ nodes to be activated. "
Bayesian stochastic blockmodeling,"  This chapter provides a self-contained introduction to the use of Bayesian inference to extract large-scale modular structures from network data, based on the stochastic blockmodel (SBM), as well as its degree-corrected and overlapping generalizations. We focus on nonparametric formulations that allow their inference in a manner that prevents overfitting, and enables model selection. We discuss aspects of the choice of priors, in particular how to avoid underfitting via increased Bayesian hierarchies, and we contrast the task of sampling network partitions from the posterior distribution with finding the single point estimate that maximizes it, while describing efficient algorithms to perform either one. We also show how inferring the SBM can be used to predict missing and spurious links, and shed light on the fundamental limitations of the detectability of modular structures in networks. "
Hardy Spaces ($0<p<\infty$) over Lipschitz Domains,"Let $0<p<\infty$, $\Gamma$ be a Lipschitz curve on the complex plane~$\mathbb{C}$ and $\Omega_+$ is the domain above $\Gamma$, we define Hardy space $H^p(\Omega_+)$ as the set of analytic functions $F$ satisfying $\sup_{\tau>0}(\int_{\Gamma} |F(\zeta+\mathrm{i}\tau)|^p |\,\mathrm{d}\zeta|)^{\frac1p}< \infty$. We denote the conformal mapping from $\mathbb{C}_+$ onto $\Omega_+$ as $\Phi$, and prove that, $H^p(\Omega_+)$ is isomorphic to $H^p(\mathbb{C}_+)$, the classical Hardy space on the upper half plane~$\mathbb{C}_+$, under the mapping $T\colon F\to F(\Phi)\cdot (\Phi')^{\frac1p}$. Besides, $T$ and $T^{-1}$ are both bounded. We also prove that if $F(w)\in H^p(\Omega_+)$, then $F(w)$ has non-tangential boundary limit $F(\zeta)$ a.e. on $\Gamma$, and, if $1\leqslant p< \infty$, $F(w)$ is the Cauchy integral on $\Gamma$ of $F(\zeta)$."
A Crevice on the Crane Beach: Finite-Degree Predicates,"First-order logic (FO) over words is shown to be equiexpressive with FO equipped with a restricted set of numerical predicates, namely the order, a binary predicate MSB$_0$, and the finite-degree predicates: FO[Arb] = FO[<, MSB$_0$, Fin]. The Crane Beach Property (CBP), introduced more than a decade ago, is true of a logic if all the expressible languages admitting a neutral letter are regular. Although it is known that FO[Arb] does not have the CBP, it is shown here that the (strong form of the) CBP holds for both FO[<, Fin] and FO[<, MSB$_0$]. Thus FO[<, Fin] exhibits a form of locality and the CBP, and can still express a wide variety of languages, while being one simple predicate away from the expressive power of FO[Arb]. The counting ability of FO[<, Fin] is studied as an application."
Dirichlet's and Thomson's principles for non-selfadjoint elliptic operators with application to non-reversible metastable diffusion processes,  We present two variational formulae for the capacity in the context of non-selfadjoint elliptic operators. The minimizers of these variational problems are expressed as solutions of boundary-value elliptic equations. We use these principles to provide a sharp estimate for the transition times between two different wells for non-reversible diffusion processes. This estimate permits to describe the metastable behavior of the system. 
Real-space investigation of short-range magnetic correlations in fluoride pyrochlores NaCaCo$_2$F$_7$ and NaSrCo$_2$F$_7$ with magnetic pair distribution function analysis,"We present time-of-flight neutron total scattering and polarized neutron scattering measurements of the magnetically frustrated compounds NaCaCo$_2$F$_7$ and NaSrCo$_2$F$_7$, which belong to a class of recently discovered pyrochlore compounds based on transition metals and fluorine. The magnetic pair distribution function (mPDF) technique is used to analyze and model the total scattering data in real space. We find that a previously-proposed model of short-range XY-like correlations with a length scale of 10-15 \AA, combined with nearest-neighbor collinear antiferromagnetic correlations, accurately describes the mPDF data at low temperature, confirming the magnetic ground state in these materials. This model is further verified by the polarized neutron scattering data. From an analysis of the temperature dependence of the mPDF and polarized neutron scattering data, we find that short-range correlations persist on the nearest-neighbor length scale up to 200 K, approximately two orders of magnitude higher than the spin freezing temperatures of these compounds. These results highlight the opportunity presented by these new pyrochlore compounds to study the effects of geometric frustration at relatively high temperatures, while also advancing the mPDF technique and providing a novel opportunity to investigate a genuinely short-range-ordered magnetic ground state directly in real space."
Embedding is not Cipher: Understanding the risk of embedding leakages,"Machine Learning (ML) already has been integrated into all kinds of systems, helping developers to solve problems with even higher accuracy than human beings. However, when integrating ML models into a system, developers may accidentally take not enough care of the outputs of ML models, mainly because of their unfamiliarity with ML and AI, resulting in severe consequences like hurting data owners' privacy. In this work, we focus on understanding the risks of abusing embeddings of ML models, an important and popular way of using ML. To show the consequence, we reveal several kinds of channels in which embeddings are accidentally leaked. As our study shows, a face verification system deployed by a government organization leaking only distance to authentic users allows an attacker to exactly recover the embedding of the verifier's pre-installed photo. Further, as we discovered, with the leaked embedding, attackers can easily recover the input photo with negligible quality losses, indicating devastating consequences to users' privacy. This is achieved with our devised GAN-like structure model, which showed 93.65% success rate on popular face embedding model under black box assumption."
Anatomy of an online misinformation network,"Massive amounts of fake news and conspiratorial content have spread over social media before and after the 2016 US Presidential Elections despite intense fact-checking efforts. How do the spread of misinformation and fact-checking compete? What are the structural and dynamic characteristics of the core of the misinformation diffusion network, and who are its main purveyors? How to reduce the overall amount of misinformation? To explore these questions we built Hoaxy, an open platform that enables large-scale, systematic studies of how misinformation and fact-checking spread and compete on Twitter. Hoaxy filters public tweets that include links to unverified claims or fact-checking articles. We perform k-core decomposition on a diffusion network obtained from two million retweets produced by several hundred thousand accounts over the six months before the election. As we move from the periphery to the core of the network, fact-checking nearly disappears, while social bots proliferate. The number of users in the main core reaches equilibrium around the time of the election, with limited churn and increasingly dense connections. We conclude by quantifying how effectively the network can be disrupted by penalizing the most central nodes. These findings provide a first look at the anatomy of a massive online misinformation diffusion network."
Injection Bucket Jitter Compensation Using Phase Lock System At Fermilab Booster,"The extraction bucket position in the Fermilab Booster is controlled with a cogging process that involves the comparison of the Booster RF count and the Recycler Ring revolution marker. A one RF bucket jitter in the extraction bucket position results from the variability of the process that phase matches the Booster to the Recycler. However, the new slow phase lock process used to lock the frequency and phase of the Booster RF to the Recycler RF has been made digital and programmable and has been modified to correct the extraction notch position. The beam loss at the Recycler injection has been reduced by 20%. Beam studies and the phase lock system will be discussed in this paper."
Collaborative Deep Learning in Fixed Topology Networks,"There is significant recent interest to parallelize deep learning algorithms in order to handle the enormous growth in data and model sizes. While most advances focus on model parallelization and engaging multiple computing agents via using a central parameter server, aspect of data parallelization along with decentralized computation has not been explored sufficiently. In this context, this paper presents a new consensus-based distributed SGD (CDSGD) (and its momentum variant, CDMSGD) algorithm for collaborative deep learning over fixed topology networks that enables data parallelization as well as decentralized computation. Such a framework can be extremely useful for learning agents with access to only local/private data in a communication constrained environment. We analyze the convergence properties of the proposed algorithm with strongly convex and nonconvex objective functions with fixed and diminishing step sizes using concepts of Lyapunov function construction. We demonstrate the efficacy of our algorithms in comparison with the baseline centralized SGD and the recently proposed federated averaging algorithm (that also enables data parallelism) based on benchmark datasets such as MNIST, CIFAR-10 and CIFAR-100."
Learning Hidden Markov Models from Pairwise Co-occurrences with Application to Topic Modeling,"  We present a new algorithm for identifying the transition and emission probabilities of a hidden Markov model (HMM) from the emitted data. Expectation-maximization becomes computationally prohibitive for long observation records, which are often required for identification. The new algorithm is particularly suitable for cases where the available sample size is large enough to accurately estimate second-order output probabilities, but not higher-order ones. We show that if one is only able to obtain a reliable estimate of the pairwise co-occurrence probabilities of the emissions, it is still possible to uniquely identify the HMM if the emission probability is \emph{sufficiently scattered}. We apply our method to hidden topic Markov modeling, and demonstrate that we can learn topics with higher quality if documents are modeled as observations of HMMs sharing the same emission (topic) probability, compared to the simple but widely used bag-of-words model. "
An explicit formula for Szego kernels on the Heisenberg group,"In this paper, we give an explicit formula for the Szego kernel for $(0, q)$ forms on the Heisenberg group $H_{n+1}$."
Necessary and sufficient conditions for consistent root reconstruction in Markov models on trees,We establish necessary and sufficient conditions for consistent root reconstruction in continuous-time Markov models with countable state space on bounded-height trees. Here a root state estimator is said to be consistent if the probability that it returns to the true root state converges to 1 as the number of leaves tends to infinity. We also derive quantitative bounds on the error of reconstruction. Our results answer a question of Gascuel and Steel and have implications for ancestral sequence reconstruction in a classical evolutionary model of nucleotide insertion and deletion.
Perfectly Controllable Multi-Agent Networks,"  This note investigates how to design topology structures to ensure the controllability of multi-agent networks (MASs) under any selection of leaders. We put forward a concept of perfect controllability, which means that a multi-agent system is controllable with no matter how the leaders are chosen. In this situation, both the number and the locations of leader agents are arbitrary. A necessary and sufficient condition is derived for the perfect controllability. Moreover, a step-by-step design procedure is proposed by which topologies are constructed and are proved to be perfectly controllable. The principle of the proposed design method is interpreted by schematic diagrams along with the corresponding topology structures from simple to complex. We show that the results are valid for any number and any location of leaders. Both the construction process and the corresponding topology structures are clearly outlined. "
Origin of the fundamental plane of elliptical galaxies in the Coma Cluster without fine-tuning,"After thirty years of the discovery of the fundamental plane, explanations to the tilt of the fundamental plane with respect to the virial plane still suffer from the need of fine-tuning. In this paper, we try to explore the origin of this tilt from the perspective of modified Newtonian dynamics (MOND) by applying the 16 Coma galaxies available in Thomas et al.[1]. Based on the mass models that can reproduce de Vaucouleurs' law closely, we find that the tilt of the traditional fundamental plane is naturally explained by the simple form of the MONDian interpolating function, if we assume a well motivated choice of anisotropic velocity distribution, and adopt the Kroupa or Salpeter stellar mass-to-light ratio. Our analysis does not necessarily rule out a varying stellar mass-to-light ratio."
Dopants Promoting Ferroelectricity in Hafnia: Insights From A Comprehensive Chemical Space Exploration,"Although dopants have been extensively employed to promote ferroelectricity in hafnia films, their role in stabilizing the responsible ferroelectric non-equilibrium Pca21 phase is not well understood. In this work, using first principles computations, we investigate the influence of nearly 40 dopants on the phase stability in bulk hafnia to identify dopants that can favor formation of the polar Pca21 phase. Although no dopant was found to stabilize this polar phase as the ground state, suggesting that dopants alone cannot induce ferroelectricity in hafnia, Ca, Sr, Ba, La, Y and Gd were found to significantly lower the energy of the polar phase with respect to the equilibrium monoclinic phase. These results are consistent with the empirical measurements of large remnant polarization in hafnia films doped with these elements. Additionally, clear chemical trends of dopants with larger ionic radii and lower electronegativity favoring the polar Pca21 phase in hafnia were identified. For this polar phase, an additional bond between the dopant cation and the 2nd nearest oxygen neighbor was identified as the root-cause of these trends. Further, trivalent dopants (Y, La, and Gd) were revealed to stabilize the polar Pca21 phase at lower strains when compared to divalent dopants (Sr and Ba). Based on these insights, we predict that the lanthanide series metals, the lower half of alkaline earth metals (Ca, Sr and Ba) and Y as the most suitable dopants to promote ferroelectricity in hafnia."