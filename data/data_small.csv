Closed-form Marginal Likelihood in Gamma-Poisson Matrix Factorization,"  We present novel understandings of the Gamma-Poisson (GaP) model, a probabilistic matrix factorization model for count data. We show that GaP can be rewritten free of the score/activation matrix. This gives us new insights about the estimation of the topic/dictionary matrix by maximum marginal likelihood estimation. In particular, this explains the robustness of this estimator to over-specified values of the factorization rank, especially its ability to automatically prune irrelevant dictionary columns, as empirically observed in previous work. The marginalization of the activation matrix leads in turn to a new Monte Carlo Expectation-Maximization algorithm with favorable properties. "
Laboratory mid-IR spectra of equilibrated and igneous meteorites. Searching for observables of planetesimal debris,"  Meteorites contain minerals from Solar System asteroids with different properties (like size, presence of water, core formation). We provide new mid-IR transmission spectra of powdered meteorites to obtain templates of how mid-IR spectra of asteroidal debris would look like. This is essential for interpreting mid-IR spectra of past and future space observatories, like the James Webb Space Telescope. We show that the transmission spectra of wet and dry chondrites, carbonaceous and ordinary chondrites and achondrite and chondrite meteorites are distinctly different in a way one can distinguish in astronomical mid-IR spectra. The two observables that spectroscopically separate the different meteorites groups (and thus the different types of parent bodies) are the pyroxene-olivine feature strength ratio and the peak shift of the olivine spectral features due to an increase in the iron concentration of the olivine. "
Case For Static AMSDU Aggregation in WLANs,"  Frame aggregation is a mechanism by which multiple frames are combined into a single transmission unit over the air. Frames aggregated at the AMSDU level use a common CRC check to enforce integrity. For longer aggregated AMSDU frames, the packet error rate increases significantly for the same bit error rate. Hence, multiple studies have proposed doing AMSDU aggregation adaptively based on the error rate. This study evaluates if there is a \emph{practical} advantage in doing adaptive AMSDU aggregation based on the link bit error rate. Evaluations on a model show that instead of implementing a complex adaptive AMSDU frame aggregation mechanism which impact queuing and other implementation aspects, it is easier to influence packet error rate with traditional mechanisms while keeping the AMSDU aggregation logic simple. "
The $Gaia$-ESO Survey: the inner disk intermediate-age open cluster NGC 6802,"Milky Way open clusters are very diverse in terms of age, chemical composition, and kinematic properties. Intermediate-age and old open clusters are less common, and it is even harder to find them inside the solar Galactocentric radius, due to the high mortality rate and strong extinction inside this region. NGC 6802 is one of the inner disk open clusters (IOCs) observed by the $Gaia$-ESO survey (GES). This cluster is an important target for calibrating the abundances derived in the survey due to the kinematic and chemical homogeneity of the members in open clusters. Using the measurements from $Gaia$-ESO internal data release 4 (iDR4), we identify 95 main-sequence dwarfs as cluster members from the GIRAFFE target list, and eight giants as cluster members from the UVES target list. The dwarf cluster members have a median radial velocity of $13.6\pm1.9$ km s$^{-1}$, while the giant cluster members have a median radial velocity of $12.0\pm0.9$ km s$^{-1}$ and a median [Fe/H] of $0.10\pm0.02$ dex. The color-magnitude diagram of these cluster members suggests an age of $0.9\pm0.1$ Gyr, with $(m-M)_0=11.4$ and $E(B-V)=0.86$. We perform the first detailed chemical abundance analysis of NGC 6802, including 27 elemental species. To gain a more general picture about IOCs, the measurements of NGC 6802 are compared with those of other IOCs previously studied by GES, that is, NGC 4815, Trumpler 20, NGC 6705, and Berkeley 81. NGC 6802 shows similar C, N, Na, and Al abundances as other IOCs. These elements are compared with nucleosynthetic models as a function of cluster turn-off mass. The $\alpha$, iron-peak, and neutron-capture elements are also explored in a self-consistent way."
Witness-Functions versus Interpretation-Functions for Secrecy in Cryptographic Protocols: What to Choose?,"  Proving that a cryptographic protocol is correct for secrecy is a hard task. One of the strongest strategies to reach this goal is to show that it is increasing, which means that the security level of every single atomic message exchanged in the protocol, safely evaluated, never deceases. Recently, two families of functions have been proposed to measure the security level of atomic messages. The first one is the family of interpretation-functions. The second is the family of witness-functions. In this paper, we show that the witness-functions are more efficient than interpretation-functions. We give a detailed analysis of an ad-hoc protocol on which the witness-functions succeed in proving its correctness for secrecy while the interpretation-functions fail to do so. "
Pairwise Difference Estimation of High Dimensional Partially Linear Model,"This paper proposes a regularized pairwise difference approach for estimating the linear component coefficient in a partially linear model, with consistency and exact rates of convergence obtained in high dimensions under mild scaling requirements. Our analysis reveals interesting features such as (i) the bandwidth parameter automatically adapts to the model and is actually tuning-insensitive; and (ii) the procedure could even maintain fast rate of convergence for $\alpha$-Hölder class of $\alpha\leq1/2$. Simulation studies show the advantage of the proposed method, and application of our approach to a brain imaging data reveals some biological patterns which fail to be recovered using competing methods."
Dissecting the multivariate extremal index and tail dependence,"  A central issue in the theory of extreme values focuses on suitable conditions such that the well-known results for the limiting distributions of the maximum of i.i.d. sequences can be applied to stationary ones. In this context, the extremal index appears as a key parameter to capture the effect of temporal dependence on the limiting distribution of the maxima. The multivariate extremal index corresponds to a generalization of this concept to a multivariate context and affects the tail dependence structure within the marginal sequences and between them. As it is a function, the inference becomes more difficult, and it is therefore important to obtain characterizations, namely bounds based on the marginal dependence that are easier to estimate. In this work we present two decompositions that emphasize different types of information contained in the multivariate extremal index, an upper limit better than those found in the literature and we analyze its role in dependence on the limiting model of the componentwise maxima of a stationary sequence. We will illustrate the results with examples of recognized interest in applications. "
"Big Universe, Big Data: Machine Learning and Image Analysis for Astronomy","  Astrophysics and cosmology are rich with data. The advent of wide-area digital cameras on large aperture telescopes has led to ever more ambitious surveys of the sky. Data volumes of entire surveys a decade ago can now be acquired in a single night and real-time analysis is often desired. Thus, modern astronomy requires big data know-how, in particular it demands highly efficient machine learning and image analysis algorithms. But scalability is not the only challenge: Astronomy applications touch several current machine learning research questions, such as learning from biased data and dealing with label and measurement noise. We argue that this makes astronomy a great domain for computer science research, as it pushes the boundaries of data analysis. In the following, we will present this exciting application area for data scientists. We will focus on exemplary results, discuss main challenges, and highlight some recent methodological advancements in machine learning and image analysis triggered by astronomical applications. "
Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog,"  A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, all learned without any human supervision! In this paper, using a Task and Tell reference game between two agents as a testbed, we present a sequence of 'negative' results culminating in a 'positive' one -- showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge 'naturally', despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate. "
Properties and Origin of Galaxy Velocity Bias in the Illustris Simulation,"We use the hydrodynamical galaxy formation simulations from the Illustris suite to study the origin and properties of galaxy velocity bias, i.e., the difference between the velocity distributions of galaxies and dark matter inside halos. We find that galaxy velocity bias is a decreasing function of the ratio of galaxy stellar mass to host halo mass. In general, central galaxies are not at rest with respect to dark matter halos or the core of halos, with a velocity dispersion above 0.04 times that of the dark matter. The central galaxy velocity bias is found to be mostly caused by the close interactions between the central and satellite galaxies. For satellite galaxies, the velocity bias is related to their dynamical and tidal evolution history after being accreted onto the host halos. It depends on the time after the accretion and their distances from the halo centers, with massive satellites generally moving more slowly than the dark matter. The results are in broad agreements with those inferred from modeling small-scale redshift-space galaxy clustering data, and the study can help improve models of redshift-space galaxy clustering."
Computer Modeling of Halogen Bonds and Other $σ$-Hole Interactions,"In the field of noncovalent interactions a new paradigm has recently become popular. It stems from the analysis of molecular electrostatic potentials and introduces a label, which has recently attracted enormous attention. The label is {\sigma}-hole, and it was first used in connection with halogens. It initiated a renaissance of interest in halogenated compounds, and later on, when found also on other groups of atoms (chalcogens, pnicogens, tetrels and aerogens), it resulted in a new direction of research of intermolecular interactions. In this review, we summarize advances from about the last 10 years in understanding those interactions related to {\sigma}-hole. We pay particular attention to theoretical and computational techniques, which play a crucial role in the field."
Towards Universal End-to-End Affect Recognition from Multilingual Speech by ConvNets,"We propose an end-to-end affect recognition approach using a Convolutional Neural Network (CNN) that handles multiple languages, with applications to emotion and personality recognition from speech. We lay the foundation of a universal model that is trained on multiple languages at once. As affect is shared across all languages, we are able to leverage shared information between languages and improve the overall performance for each one. We obtained an average improvement of 12.8% on emotion and 10.1% on personality when compared with the same model trained on each language only. It is end-to-end because we directly take narrow-band raw waveforms as input. This allows us to accept as input audio recorded from any source and to avoid the overhead and information loss of feature extraction. It outperforms a similar CNN using spectrograms as input by 12.8% for emotion and 6.3% for personality, based on F-scores. Analysis of the network parameters and layers activation shows that the network learns and extracts significant features in the first layer, in particular pitch, energy and contour variations. Subsequent convolutional layers instead capture language-specific representations through the analysis of supra-segmental features. Our model represents an important step for the development of a fully universal affect recognizer, able to recognize additional descriptors, such as stress, and for the future implementation into affective interactive systems."
A Variational Characterization of Fluid Sloshing with Surface Tension,"  We consider the sloshing problem for an incompressible, inviscid, irrotational fluid in an open container, including effects due to surface tension on the free surface. We restrict ourselves to a constant contact angle and seek time-harmonic solutions of the linearized problem, which describes the time-evolution of the fluid due to a small initial disturbance of the surface at rest. As opposed to the zero surface tension case, where the problem reduces to a partial differential equation for the velocity potential, we obtain a coupled system for the velocity potential and the free surface displacement. We derive a new variational formulation of the coupled problem and establish the existence of solutions using the direct method from the calculus of variations. We prove a domain monotonicity result for the fundamental sloshing eigenvalue. In the limit of zero surface tension, we recover the variational formulation of the mixed Steklov-Neumann eigenvalue problem and give the first-order perturbation formula for a simple eigenvalue. "
diagnoseIT: Expertengestützte automatische Diagnose von Performance-Probleme in Enterprise-Anwendungen (Abschlussbericht),  This is the final report of the collaborative research project diagnoseIT on expert-guided automatic diagnosis of performance problems in enterprise applications. 
Statistical Inference in Political Networks Research,"  Researchers interested in statistically modeling network data have a well-established and quickly growing set of approaches from which to choose. Several of these methods have been regularly applied in research on political networks, while others have yet to permeate the field. Here, we review the most prominent methods of inferential network analysis---both for cross-sectionally and longitudinally observed networks including (temporal) exponential random graph models, latent space models, the quadratic assignment procedure, and stochastic actor oriented models. For each method, we summarize its analytic form, identify prominent published applications in political science and discuss computational considerations. We conclude with a set of guidelines for selecting a method for a given application. "
Controller-jammer game models of Denial of Service in control systems operating over packet-dropping links,"  The paper introduces a class of zero-sum games between the adversary and controller as a scenario for a `denial of service' in a networked control system. The communication link is modeled as a set of transmission regimes controlled by a strategic jammer whose intention is to wage an attack on the plant by choosing a most damaging regime-switching strategy. We demonstrate that even in the one-step case, the introduced games admit a saddle-point equilibrium, at which the jammer's optimal policy is to randomize in a region of the plant's state space, thus requiring the controller to undertake a nontrivial response which is different from what one would expect in a standard stochastic control problem over a packet dropping link. The paper derives conditions for the introduced games to have such a saddle-point equilibrium. Furthermore, we show that in more general multi-stage games, these conditions provide `greedy' jamming strategies for the adversary. "
Conservation of spin supercurrents in superconductors,"  We demonstrate that spin supercurrents are conserved upon transmission through a conventional superconductor, even in the presence of spin-dependent scattering by impurities with magnetic moments or spin-orbit coupling. This is fundamentally different from conventional spin currents, which decay in the presence of such scattering, and this has important implications for the usage of superconducting materials in spintronic hybrid structures. "
Driver Identification Using Automobile Sensor Data from a Single Turn,"As automotive electronics continue to advance, cars are becoming more and more reliant on sensors to perform everyday driving operations. These sensors are omnipresent and help the car navigate, reduce accidents, and provide comfortable rides. However, they can also be used to learn about the drivers themselves. In this paper, we propose a method to predict, from sensor data collected at a single turn, the identity of a driver out of a given set of individuals. We cast the problem in terms of time series classification, where our dataset contains sensor readings at one turn, repeated several times by multiple drivers. We build a classifier to find unique patterns in each individual's driving style, which are visible in the data even on such a short road segment. To test our approach, we analyze a new dataset collected by AUDI AG and Audi Electronics Venture, where a fleet of test vehicles was equipped with automotive data loggers storing all sensor readings on real roads. We show that turns are particularly well-suited for detecting variations across drivers, especially when compared to straightaways. We then focus on the 12 most frequently made turns in the dataset, which include rural, urban, highway on-ramps, and more, obtaining accurate identification results and learning useful insights about driver behavior in a variety of settings."
Same-different problems strain convolutional neural networks,"  The robust and efficient recognition of visual relations in images is a hallmark of biological vision. We argue that, despite recent progress in visual recognition, modern machine vision algorithms are severely limited in their ability to learn visual relations. Through controlled experiments, we demonstrate that visual-relation problems strain convolutional neural networks (CNNs). The networks eventually break altogether when rote memorization becomes impossible, as when intra-class variability exceeds network capacity. Motivated by the comparable success of biological vision, we argue that feedback mechanisms including attention and perceptual grouping may be the key computational components underlying abstract visual reasoning.\ "
Hyper-dimensional computing for a visual question-answering system that is trainable end-to-end,"  In this work we propose a system for visual question answering. Our architecture is composed of two parts, the first part creates the logical knowledge base given the image. The second part evaluates questions against the knowledge base. Differently from previous work, the knowledge base is represented using hyper-dimensional computing. This choice has the advantage that all the operations in the system, namely creating the knowledge base and evaluating the questions against it, are differentiable, thereby making the system easily trainable in an end-to-end fashion. "
The Inner Structure of Time-Dependent Signals,"This paper shows how a time series of measurements of an evolving system can be processed to create an inner time series that is unaffected by any instantaneous invertible, possibly nonlinear transformation of the measurements. An inner time series contains information that does not depend on the nature of the sensors, which the observer chose to monitor the system. Instead, it encodes information that is intrinsic to the evolution of the observed system. Because of its sensor-independence, an inner time series may produce fewer false negatives when it is used to detect events in the presence of sensor drift. Furthermore, if the observed physical system is comprised of non-interacting subsystems, its inner time series is separable; i.e., it consists of a collection of time series, each one being the inner time series of an isolated subsystem. Because of this property, an inner time series can be used to detect a specific behavior of one of the independent subsystems without using blind source separation to disentangle that subsystem from the others. The method is illustrated by applying it to: 1) an analytic example; 2) the audio waveform of one speaker; 3) video images from a moving camera; 4) mixtures of audio waveforms of two speakers."
Classical properties of the leading eigenstates of quantum dissipative systems,"  By analyzing a paradigmatic example of the theory of dissipative systems -- the classical and quantum dissipative standard map -- we are able to explain the main features of the decay to the quantum equilibrium state. The classical isoperiodic stable structures typically present in the parameter space of these kind of systems play a fundamental role. In fact, we have found that the period of stable structures that are near in this space determines the phase of the leading eigenstates of the corresponding quantum superoperator. Moreover, the eigenvectors show a strong localization on the corresponding periodic orbits (limit cycles). We show that this sort of scarring phenomenon (an established property of Hamiltonian and projectively open systems) is present in the dissipative case and it is of extreme simplicity. "
Towards Binary-Valued Gates for Robust LSTM Training,"Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. It aims to use gates to control information flow (e.g., whether to skip some information or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal. In this paper, we propose a new way for LSTM training, which pushes the output values of the gates towards 0 or 1. By doing so, we can better control the information flow: the gates are mostly open or closed, instead of in a middle state, which makes the results more interpretable. Empirical studies show that (1) Although it seems that we restrict the model capacity, there is no performance drop: we achieve better or comparable performances due to its better generalization ability; (2) The outputs of gates are not sensitive to their inputs: we can easily compress the LSTM unit in multiple ways, e.g., low-rank approximation and low-precision approximation. The compressed models are even better than the baseline models without compression."
GUB Covers and Power-Indexed formulations for Wireless Network Design,"We propose a pure 0-1 formulation for the wireless network design problem, i.e. the problem of configuring a set of transmitters to provide service coverage to a set of receivers. In contrast with classical mixed integer formulations, where power emissions are represented by continuous variables, we consider only a finite set of powers values. This has two major advantages: it better fits the usual practice and eliminates the sources of numerical problems which heavily affect continuous models. A crucial ingredient of our approach is an effective basic formulation for the single knapsack problem representing the coverage condition of a receiver. This formulation is based on the GUB cover inequalities introduced by Wolsey (1990) and its core is an extension of the exact formulation of the GUB knapsack polytope with two GUB constraints. This special case corresponds to the very common practical situation where only one major interferer is present. We assess the effectiveness of our formulation by comprehensive computational results over realistic instances of two typical technologies, namely WiMAX and DVB-T."
Orbital contributions to the electron g-factor in semiconductor nanowires,"Recent experiments on Majorana fermions in semiconductor nanowires [Albrecht et al., Nat. 531, 206 (2016)] revealed a surprisingly large electronic Landé g-factor, several times larger than the bulk value - contrary to the expectation that confinement reduces the g-factor. Here we assess the role of orbital contributions to the electron g-factor in nanowires and quantum dots. We show that an LS coupling in higher subbands leads to an enhancement of the g-factor of an order of magnitude or more for small effective mass semiconductors. We validate our theoretical finding with simulations of InAs and InSb, showing that the effect persists even if cylindrical symmetry is broken. A huge anisotropy of the enhanced g-factors under magnetic field rotation allows for a straightforward experimental test of this theory."
Comprehensive routing strategy on multilayer networks,"  Designing an efficient routing strategy is of great importance to alleviate traffic congestion in multilayer networks. In this work, we design an effective routing strategy for multilayer networks by comprehensively considering the roles of nodes' local structures in micro-level, as well as the macro-level differences in transmission speeds between different layers. Both numerical and analytical results indicate that our proposed routing strategy can reasonably redistribute the traffic load of the low speed layer to the high speed layer, and thus the traffic capacity of multilayer networks are significantly enhanced compared with the monolayer low speed networks. There is an optimal combination of macro- and micro-level control parameters at which can remarkably alleviate the congestion and thus maximize the traffic capacity for a given multilayer network. Moreover, we find that increasing the size and the average degree of the high speed layer can enhance the traffic capacity of multilayer networks more effectively. We finally verify that real-world network topology does not invalidate the results. The theoretical predictions agree well with the numerical simulations. "
Long short-term memory and learning-to-learn in networks of spiking neurons,"Recurrent networks of spiking neurons (RSNNs) underlie the astounding computing and learning capabilities of the brain. But computing and learning capabilities of RSNN models have remained poor, at least in comparison with artificial neural networks (ANNs). We address two possible reasons for that. One is that RSNNs in the brain are not randomly connected or designed according to simple rules, and they do not start learning as a tabula rasa network. Rather, RSNNs in the brain were optimized for their tasks through evolution, development, and prior experience. Details of these optimization processes are largely unknown. But their functional contribution can be approximated through powerful optimization methods, such as backpropagation through time (BPTT). A second major mismatch between RSNNs in the brain and models is that the latter only show a small fraction of the dynamics of neurons and synapses in the brain. We include neurons in our RSNN model that reproduce one prominent dynamical process of biological neurons that takes place at the behaviourally relevant time scale of seconds: neuronal adaptation. We denote these networks as LSNNs because of their Long short-term memory. The inclusion of adapting neurons drastically increases the computing and learning capability of RSNNs if they are trained and configured by deep learning (BPTT combined with a rewiring algorithm that optimizes the network architecture). In fact, the computational performance of these RSNNs approaches for the first time that of LSTM networks. In addition RSNNs with adapting neurons can acquire abstract knowledge from prior learning in a Learning-to-Learn (L2L) scheme, and transfer that knowledge in order to learn new but related tasks from very few examples. We demonstrate this for supervised learning and reinforcement learning."
Detecting Molecular Rotational Dynamics Complementing the Low-Frequency Terahertz Vibrations in a Zirconium-Based Metal-Organic Framework,"We show clear experimental evidence of co-operative terahertz (THz) dynamics observed below 3 THz (~100 cm-1), for a low-symmetry Zr-based metal-organic framework (MOF) structure, termed MIL-140A [ZrO(O2C-C6H4-CO2)]. Utilizing a combination of high-resolution inelastic neutron scattering and synchrotron radiation far-infrared spectroscopy, we measured low-energy vibrations originating from the hindered rotations of organic linkers, whose energy barriers and detailed dynamics have been elucidated via ab initio density functional theory (DFT) calculations. For completeness, we obtained Raman spectra and characterized the alterations to the complex pore architecture caused by the THz rotations. We discovered an array of soft modes with trampoline-like motions, which could potentially be the source of anomalous mechanical phenomena, such as negative linear compressibility and negative thermal expansion. Our results also demonstrate coordinated shear dynamics (~2.5 THz), a mechanism which we have shown to destabilize MOF crystals, in the exact crystallographic direction of the minimum shear modulus (Gmin)."
A southern-sky total intensity source catalogue at 2.3 GHz from S-band Polarisation All-Sky Survey data,"The S-band Polarisation All-Sky Survey (S-PASS) has observed the entire southern sky using the 64-metre Parkes radio telescope at 2.3GHz with an effective bandwidth of 184MHz. The surveyed sky area covers all declinations $\delta\leq 0^\circ$. To analyse compact sources the survey data have been re-processed to produce a set of 107 Stokes $I$ maps with 10.75arcmin resolution and the large scale emission contribution filtered out. In this paper we use these Stokes $I$ images to create a total intensity southern-sky extragalactic source catalogue at 2.3GHz. The source catalogue contains 23,389 sources and covers a sky area of 16,600deg$^2$, excluding the Galactic plane for latitudes $|b|<10^\circ$. Approximately 8% of catalogued sources are resolved. S-PASS source positions are typically accurate to within 35arcsec. At a flux density of 225mJy the S-PASS source catalogue is more than 95% complete, and $\sim$94% of S-PASS sources brighter than 500mJy beam$^{-1}$ have a counterpart at lower frequencies."
Mechanomyography based closed-loop Functional Electrical Stimulation cycling system,"Functional Electrical Stimulation (FES) systems are successful in restoring motor function and supporting paralyzed users. Commercially available FES products are open loop, meaning that the system is unable to adapt to changing conditions with the user and their muscles which results in muscle fatigue and poor stimulation protocols. This is because it is difficult to close the loop between stimulation and monitoring of muscle contraction using adaptive stimulation. FES causes electrical artefacts which make it challenging to monitor muscle contractions with traditional methods such as electromyography (EMG). We look to overcome this limitation by combining FES with novel mechanomyographic (MMG) sensors to be able to monitor muscle activity during stimulation in real time. To provide a meaningful task we built an FES cycling rig with a software interface that enabled us to perform adaptive recording and stimulation, and then combine this with sensors to record forces applied to the pedals using force sensitive resistors (FSRs), crank angle position using a magnetic incremental encoder and inputs from the user using switches and a potentiometer. We illustrated this with a closed-loop stimulation algorithm that used the inputs from the sensors to control the output of a programmable RehaStim 1 FES stimulator (Hasomed) in real-time. This recumbent bicycle rig was used as a testing platform for FES cycling. The algorithm was designed to respond to a change in requested speed (RPM) from the user and change the stimulation power (% of maximum current mA) until this speed was achieved and then maintain it."
QT2S: A System for Monitoring Road Traffic via Fine Grounding of Tweets,"Social media platforms provide continuous access to user generated content that enables real-time monitoring of user behavior and of events. The geographical dimension of such user behavior and events has recently caught a lot of attention in several domains: mobility, humanitarian, or infrastructural. While resolving the location of a user can be straightforward, depending on the affordances of their device and/or of the application they are using, in most cases, locating a user demands a larger effort, such as exploiting textual features. On Twitter for instance, only 2% of all tweets are geo-referenced. In this paper, we present a system for zoomed-in grounding (below city level) for short messages (e.g., tweets). The system combines different natural language processing and machine learning techniques to increase the number of geo-grounded tweets, which is essential to many applications such as disaster response and real-time traffic monitoring."
A Benchmark for Dose Finding Studies with Continuous Outcomes,"An important tool to evaluate the performance of any design is an optimal benchmark proposed by O'Quigley and others (2002, Biostatistics 3(1), 51-56) that provides an upper bound on the performance of a design under a given scenario. The original benchmark can be applied to dose finding studies with a binary endpoint only. However, there is a growing interest in dose finding studies involving continuous outcomes, but no benchmark for such studies has been developed. We show that the original benchmark and its extension by Cheung (2014, Biometrics 70(2), 389-397), when looked at from a different perspective, can be generalised to various settings with several discrete and continuous outcomes. We illustrate and compare the benchmark performance in the setting of a Phase I clinical trial with continuous toxicity endpoint and in the setting of a Phase I/II clinical trial with continuous efficacy outcome. We show that the proposed benchmark provides an accurate upper bound for model-based dose finding methods and serves as a powerful tool for evaluating designs."
Wavefront retrieval through random pupil plane phase probes: Gerchberg-Saxton approach,"A pupil plane wavefront reconstruction procedure is proposed based on analysis of a sequence of focal plane images corresponding to a sequence of random pupil plane phase probes. The developed method provides the unique nontrivial solution of wavefront retrieval problem and shows global convergence to this solution demonstrated using a Gerchberg-Saxton implementation. The method is general and can be used in any optical system that includes deformable mirrors for active/adaptive wavefront correction. The presented numerical simulation and lab experimental results show low noise sensitivity, high reliability and robustness of the proposed approach for high quality optical wavefront restoration. Laboratory experiments have shown $\lambda$/14 rms accuracy in retrieval of a poked DM actuator fiducial pattern with spatial resolution of 20-30$~\mu$m that is comparable with accuracy of direct high-resolution interferometric measurements."
Uplink Non-Orthogonal Multiple Access for 5G Wireless Networks,"  Orthogonal Frequency Division Multiple Access (OFDMA) as well as other orthogonal multiple access techniques fail to achieve the system capacity limit in the uplink due to the exclusivity in resource allocation. This issue is more prominent when fairness among the users is considered in the system. Current Non-Orthogonal Multiple Access (NOMA) techniques introduce redundancy by coding/spreading to facilitate the users' signals separation at the receiver, which degrade the system spectral efficiency. Hence, in order to achieve higher capacity, more efficient NOMA schemes need to be developed. In this paper, we propose a NOMA scheme for uplink that removes the resource allocation exclusivity and allows more than one user to share the same subcarrier without any coding/spreading redundancy. Joint processing is implemented at the receiver to detect the users' signals. However, to control the receiver complexity, an upper limit on the number of users per subcarrier needs to be imposed. In addition, a novel subcarrier and power allocation algorithm is proposed for the new NOMA scheme that maximizes the users' sum-rate. The link-level performance evaluation has shown that the proposed scheme achieves bit error rate close to the single-user case. Numerical results show that the proposed NOMA scheme can significantly improve the system performance in terms of spectral efficiency and fairness comparing to OFDMA. "
Epsilon-approximations and epsilon-nets,  The use of random samples to approximate properties of geometric configurations has been an influential idea for both combinatorial and algorithmic purposes. This chapter considers two related notions---$\epsilon$-approximations and $\epsilon$-nets---that capture the most important quantitative properties that one would expect from a random sample with respect to an underlying geometric configuration. 
A continuous framework for fairness,"  Increasingly, discrimination by algorithms is perceived as a societal and legal problem. As a response, a number of criteria for implementing algorithmic fairness in machine learning have been developed in the literature. This paper proposes the Continuous Fairness Algorithm (CFA$\theta$) which enables a continuous interpolation between different fairness definitions. More specifically, we make three main contributions to the existing literature. First, our approach allows the decision maker to continuously vary between concepts of individual and group fairness. As a consequence, the algorithm enables the decision maker to adopt intermediate ""worldviews"" on the degree of discrimination encoded in algorithmic processes, adding nuance to the extreme cases of ""we're all equal"" (WAE) and ""what you see is what you get"" (WYSIWYG) proposed so far in the literature. Second, we use optimal transport theory, and specifically the concept of the barycenter, to maximize decision maker utility under the chosen fairness constraints. Third, the algorithm is able to handle cases of intersectionality, i.e., of multi-dimensional discrimination of certain groups on grounds of several criteria. We discuss three main examples (college admissions; credit application; insurance contracts) and map out the policy implications of our approach. The explicit formalization of the trade-off between individual and group fairness allows this post-processing approach to be tailored to different situational contexts in which one or the other fairness criterion may take precedence. "
Counting Subwords Occurrences in Base-b Expansions,"We count the number of distinct (scattered) subwords occurring in the base-b expansion of the non-negative integers. More precisely, we consider the sequence $(S_b(n))_{n\ge 0}$ counting the number of positive entries on each row of a generalization of the Pascal triangle to binomial coefficients of base-$b$ expansions. By using a convenient tree structure, we provide recurrence relations for $(S_b(n))_{n\ge 0}$ leading to the $b$-regularity of the latter sequence. Then we deduce the asymptotics of the summatory function of the sequence $(S_b(n))_{n\ge 0}$."
Tensor Renormalization Group with Randomized Singular Value Decomposition,"  An algorithm of the tensor renormalization group is proposed based on a randomized algorithm for singular value decomposition. Our algorithm is applicable to a broad range of two-dimensional classical models. In the case of a square lattice, its computational complexity and memory usage are proportional to the fifth and the third power of the bond dimension, respectively, whereas those of the conventional implementation are of the sixth and the fourth power. The oversampling parameter larger than the bond dimension is sufficient to reproduce the same result as full singular value decomposition even at the critical point of the two-dimensional Ising model. "
Cohomology of symplectic groups and Meyer's signature theorem,"Meyer showed that the signature of a closed oriented surface bundle over a surface is a multiple of $4$, and can be computed using an element of $H^2(\mathsf{Sp}(2g, \mathbb{Z}),\mathbb{Z})$. Denoting by $1 \to \mathbb{Z} \to \widetilde{\mathsf{Sp}(2g,\mathbb{Z})} \to \mathsf{Sp}(2g,\mathbb{Z}) \to 1$ the pullback of the universal cover of $\mathsf{ Sp}(2g,\mathbb{R})$, Deligne proved that every finite index subgroup of $\widetilde{\mathsf {Sp}(2g, \mathbb{Z})}$ contains $2\mathbb{Z}$. As a consequence, a class in the second cohomology of any finite quotient of $\mathsf{Sp}(2g, \mathbb{Z})$ can at most enable us to compute the signature of a surface bundle modulo $8$. We show that this is in fact possible and investigate the smallest quotient of $\mathsf{Sp}(2g, \mathbb{Z})$ that contains this information. This quotient $\mathfrak{H}$ is a non-split extension of $\mathsf {Sp}(2g,2)$ by an elementary abelian group of order $2^{2g+1}$. There is a central extension $1\to \mathbb{Z}/2\to\tilde{\mathfrak{H}}\to\mathfrak{H}\to 1$, and $\tilde{\mathfrak{H}}$ appears as a quotient of the metaplectic double cover $\mathsf{Mp}(2g,\mathbb{Z})=\widetilde{\mathsf{Sp}(2g,\mathbb{Z})}/2\mathbb{Z}$. It is an extension of $\mathsf{Sp}(2g,2)$ by an almost extraspecial group of order $2^{2g+2}$, and has a faithful irreducible complex representation of dimension $2^g$. Provided $g\ge 4$, $\widetilde{\mathfrak{H}}$ is the universal central extension of $\mathfrak{H}$. Putting all this together, we provide a recipe for computing the signature modulo $8$, and indicate some consequences."
The Newman--Shapiro problem,"We give a negative answer to the Newman--Shapiro problem on weighted approximation for entire functions formulated in 1966 and motivated by the theory of operators on the Fock space. There exists a function in the Fock space such that its exponential multiples do not approximate some entire multiples in the space. Furthermore, we establish several positive results under different restrictions on the function in question."
randUTV: A blocked randomized algorithm for computing a rank-revealing UTV factorization,"  This manuscript describes the randomized algorithm randUTV for computing a so called UTV factorization efficiently. Given a matrix $A$, the algorithm computes a factorization $A = UTV^{*}$, where $U$ and $V$ have orthonormal columns, and $T$ is triangular (either upper or lower, whichever is preferred). The algorithm randUTV is developed primarily to be a fast and easily parallelized alternative to algorithms for computing the Singular Value Decomposition (SVD). randUTV provides accuracy very close to that of the SVD for problems such as low-rank approximation, solving ill-conditioned linear systems, determining bases for various subspaces associated with the matrix, etc. Moreover, randUTV produces highly accurate approximations to the singular values of $A$. Unlike the SVD, the randomized algorithm proposed builds a UTV factorization in an incremental, single-stage, and non-iterative way, making it possible to halt the factorization process once a specified tolerance has been met. Numerical experiments comparing the accuracy and speed of randUTV to the SVD are presented. These experiments demonstrate that in comparison to column pivoted QR, which is another factorization that is often used as a relatively economic alternative to the SVD, randUTV compares favorably in terms of speed while providing far higher accuracy. "
"Continuity of nonlinear eigenvalues in $CD(K,\infty)$ spaces with respect to measured Gromov-Hausdorff convergence","  In this note we prove in the nonlinear setting of $CD(K,\infty)$ spaces the stability of the Krasnoselskii spectrum of the Laplace operator $-\Delta$ under measured Gromov-Hausdorff convergence, under an additional compactness assumption satisfied, for instance, by sequences of $CD^*(K,N)$ metric measure spaces with uniformly bounded diameter. Additionally, we show that every element $\lambda$ in the Krasnoselskii spectrum is indeed an eigenvalue, namely there exists a nontrivial $u$ satisfying the eigenvalue equation $- \Delta u = \lambda u$. "
Modeling WiFi Traffic for White Space Prediction in Wireless Sensor Networks,"Cross Technology Interference (CTI) is a prevalent phenomenon in the 2.4 GHz unlicensed spectrum causing packet losses and increased channel contention. In particular, WiFi interference is a severe problem for low-power wireless networks as its presence causes a significant degradation of the overall performance. In this paper, we propose a proactive approach based on WiFi interference modeling for accurately predicting transmission opportunities for low-power wireless networks. We leverage statistical analysis of real-world WiFi traces to learn aggregated traffic characteristics in terms of Inter-Arrival Time (IAT) that, once captured into a specific 2nd order Markov Modulated Poisson Process (MMPP(2)) model, enable accurate estimation of interference. We further use a hidden Markov model (HMM) for channel occupancy prediction. We evaluated the performance of i) the MMPP(2) traffic model w.r.t. real-world traces and an existing Pareto model for accurately characterizing the WiFi traffic and, ii) compared the HMM based white space prediction to random channel access. We report encouraging results for using interference modeling for white space prediction."
Dyson models under renormalization and in weak fields,"We consider one-dimensional long-range spin models (usually called Dyson models), consisting of Ising ferromagnets with slowly decaying long-range pair potentials of the form $\frac{1}{|i-j|^{\alpha}}$ mainly focusing on the range of slow decays $1 < \alpha \leq 2$. We describe two recent results, one about renormalization and one about the effect of external fields at low temperature. The first result states that a decimated long-range Gibbs measure in one dimension becomes non-Gibbsian, in the same vein as comparable results in higher dimensions for short-range models. The second result addresses the behaviour of such models under inhomogeneous fields, in particular external fields which decay to zero polynomially as $(|i|+1)^{- \gamma}$. We study how the critical decay power of the field, $\gamma$, for which the phase transition persists and the decay power $\alpha$ of the Dyson model compare, extending recent results for short-range models on lattices and on trees. We also briefly point out some analogies between these results."
Coupling geometry on binary bipartite networks: hypotheses testing on pattern geometry and nestedness,"  Upon a matrix representation of a binary bipartite network, via the permutation invariance, a coupling geometry is computed to approximate the minimum energy macrostate of a network's system. Such a macrostate is supposed to constitute the intrinsic structures of the system, so that the coupling geometry should be taken as information contents, or even the nonparametric minimum sufficient statistics of the network data. Then pertinent null and alternative hypotheses, such as nestedness, are to be formulated according to the macrostate. That is, any efficient testing statistic needs to be a function of this coupling geometry. These conceptual architectures and mechanisms are by and large still missing in community ecology literature, and rendered misconceptions prevalent in this research area. Here the algorithmically computed coupling geometry is shown consisting of deterministic multiscale block patterns, which are framed by two marginal ultrametric trees on row and column axes, and stochastic uniform randomness within each block found on the finest scale. Functionally a series of increasingly larger ensembles of matrix mimicries is derived by conforming to the multiscale block configurations. Here matrix mimicking is meant to be subject to constraints of row and column sums sequences. Based on such a series of ensembles, a profile of distributions becomes a natural device for checking the validity of testing statistics or structural indexes. An energy based index is used for testing whether network data indeed contains structural geometry. A new version block-based nestedness index is also proposed. Its validity is checked and compared with the existing ones. A computing paradigm, called Data Mechanics, and its application on one real data network are illustrated throughout the developments and discussions in this paper. "
"MUSE-inspired view of the quasar Q2059-360, its Lyman alpha blob, and its neighborhood","The radio-quiet quasar Q2059-360 at redshift $z=3.08$ is known to be close to a small Lyman $\alpha$ blob (LAB) and to be absorbed by a proximate damped Ly$\alpha$ (PDLA) system. Here, we present the Multi Unit Spectroscopic Explorer (MUSE) integral field spectroscopy follow-up of this quasi-stellar object (QSO). Our primary goal is to characterize this LAB in detail by mapping it both spatially and spectrally using the Ly$\alpha$ line, and by looking for high-ionization lines to constrain the emission mechanism. Combining the high sensitivity of the MUSE integral field spectrograph mounted on the Yepun telescope at ESO-VLT with the natural coronagraph provided by the PDLA, we map the LAB down to the QSO position, after robust subtraction of QSO light in the spectral domain. In addition to confirming earlier results for the small bright component of the LAB, we unveil a faint filamentary emission protruding to the south over about 80 pkpc (physical kpc); this results in a total size of about 120 pkpc. We derive the velocity field of the LAB (assuming no transfer effects) and map the Ly$\alpha$ line width. Upper limits are set to the flux of the N V $\lambda 1238-1242$, C IV $\lambda 1548-1551$, He II $\lambda 1640$, and C III] $\lambda 1548-1551$ lines. We have discovered two probable Ly$\alpha$ emitters at the same redshift as the LAB and at projected distances of 265 kpc and 207 kpc from the QSO; their Ly$\alpha$ luminosities might well be enhanced by the QSO radiation. We also find an emission line galaxy at $z=0.33$ near the line of sight to the QSO. This LAB shares the same general characteristics as the 17 others surrounding radio-quiet QSOs presented previously. However, there are indications that it may be centered on the PDLA galaxy rather than on the QSO."
Community Detection on Euclidean Random Graphs,"  We study the problem of community detection (CD) on Euclidean random geometric graphs where each vertex has two latent variables: a binary community label and a $\mathbb{R}^d$ valued location label which forms the support of a Poisson point process of intensity $\lambda$. A random graph is then drawn with edge probabilities dependent on both the community and location labels. In contrast to the stochastic block model (SBM) that has no location labels, the resulting random graph contains many more short loops due to the geometric embedding. We consider the recovery of the community labels, partial and exact, using the random graph and the location labels. We establish phase transitions for both sparse and logarithmic degree regimes, and provide bounds on the location of the thresholds, conjectured to be tight in the case of exact recovery. We also show that the threshold of the distinguishability problem, i.e., the testing between our model and the null model without community labels exhibits no phase-transition and in particular, does not match the weak recovery threshold (in contrast to the SBM). "
Load balancing with heterogeneous schedulers,"Load balancing is a common approach in web server farms or inventory routing problems. An important issue in such systems is to determine the server to which an incoming request should be routed to optimize a given performance criteria. In this paper, we assume the server's scheduling disciplines to be heterogeneous. More precisely, a server implements a scheduling discipline which belongs to the class of limited processor sharing (LPS-$d$) scheduling disciplines. Under LPS-$d$, up to $d$ jobs can be served simultaneously, and hence, includes as special cases First Come First Served ($d=1$) and Processor Sharing ($d=\infty$). In order to obtain efficient heuristics, we model the above load-balancing framework as a multi-armed restless bandit problem. Using the relaxation technique, as first developed in the seminal work of Whittle, we derive Whittle's index policy for general cost functions and obtain a closed-form expression for Whittle's index in terms of the steady-state distribution. Through numerical computations, we investigate the performance of Whittle's index with two different performance criteria: linear cost criterion and a cost criterion that depends on the first and second moment of the throughput. Our results show that \emph{(i)} the structure of Whittle's index policy can strongly depend on the scheduling discipline implemented in the server, i.e., on $d$, and that \emph{(ii)} Whittle's index policy significantly outperforms standard dispatching rules such as Join the Shortest Queue (JSQ), Join the Shortest Expected Workload (JSEW), and Random Server allocation (RSA)."
An Information Theoretic Framework for Active De-anonymization in Social Networks Based on Group Memberships,"  In this paper, a new mathematical formulation for the problem of de-anonymizing social network users by actively querying their membership in social network groups is introduced. In this formulation, the attacker has access to a noisy observation of the group membership of each user in the social network. When an unidentified victim visits a malicious website, the attacker uses browser history sniffing to make queries regarding the victim's social media activity. Particularly, it can make polar queries regarding the victim's group memberships and the victim's identity. The attacker receives noisy responses to her queries. The goal is to de-anonymize the victim with the minimum number of queries. Starting with a rigorous mathematical model for this active de-anonymization problem, an upper bound on the attacker's expected query cost is derived, and new attack algorithms are proposed which achieve this bound. These algorithms vary in computational cost and performance. The results suggest that prior heuristic approaches to this problem provide sub-optimal solutions. "
Geobiodynamics and Roegenian Economic Systems,"  This mathematical essay brings together ideas from Economics, Geobiodynamics and Thermodynamics. Its purpose is to obtain real models of complex evolutionary systems. More specifically, the essay defines Roegenian Economy and links Geobiodynamics and Roegenian Economy. In this context, we discuss the isomorphism between the concepts and techniques of Thermodynamics and Economics. Then we describe a Roegenian economic system like a Carnot group. After we analyse the phase equilibrium for two heterogeneous economic systems. The European Union Economics appears like Cartesian product of Roegenian economic systems and its Balance is analysed in details. A Section at the end describes the ""economic black holes"" as small parts of a a global economic system in which national income is so great that it causes others poor enrichment. These ideas can be used to improve our knowledge and understanding of the nature of development and evolution of thermodynamic-economic systems. "
The effects of the overshooting of the convective core on main-sequence turnoffs of young- and intermediate-age star clusters,"Recent investigations have shown that the extended main-sequence turnoffs (eMSTOs) are a common feature of intermediate-age star clusters in the Magellanic Clouds. The eMSTOs are also found in the color-magnitude diagram (CMD) of young-age star clusters. The origin of the eMSTOs is still an open question. Moreover, asteroseismology shows that the value of the overshooting parameter $\delta_{\rm ov}$ of the convective core is not fixed for the stars with an approximatelly equal mass. Thus the MSTO of star clusters may be affected by the overshooting of the convective core (OVCC). We calculated the effects of the OVCC with different $\delta_{\rm ov}$ on the MSTO of young- and intermediate-age star clusters. \textbf{If $\delta_{\rm ov}$ varies between stars in a cluster,} the observed eMSTOs of young- and intermediate-age star clusters can be explained well by the effects. The equivalent age spreads of MSTO caused by the OVCC are related to the age of star clusters and are in good agreement with observed results of many clusters. Moreover, the observed eMSTOs of NGC 1856 are reproduced by the coeval populations with different $\delta_{\rm ov}$. The eMSTOs of star clusters may be relevant to the effects of the OVCC. The effects of the OVCC \textbf{are similar to that of rotation in some respects. But the effects cannot result in a significant split of main sequence of young star clusters at $m_{U}\lesssim 21$.} The presence of a rapid rotation can make the split of main sequence of young star clusters more significant."
First eigenvalue estimates of Dirichlet-to-Neumann operators on graphs,"Following Escobar [Esc97] and Jammes [Jam15], we introduce two types of isoperimetric constants and give lower bound estimates for the first nontrivial eigenvalues of Dirichlet-to-Neumann operators on finite graphs with boundary respectively."
Air-burst Generated Tsunamis,"  This paper examines the questions of whether smaller asteroids that burst in the air over water can generate tsunamis that could pose a threat to distant locations. Such air burst-generated tsunamis are qualitatively different than the more frequently studied earthquake-generated tsunamis, and differ as well from impact asteroids. Numerical simulations are presented using the shallow water equations in several settings, demonstrating very little tsunami threat from this scenario. A model problem with an explicit solution that demonstrates and explains the same phenomena found in the computations is analyzed. We discuss the question of whether compressibility and dispersion are important effects that should be included, and show results from a more sophisticated model problem using the linearized Euler equations that begins to addresses this. "
A new design principle of robust onion-like networks self-organized in growth,"  Today's economy, production activity, and our life are sustained by social and technological network infrastructures, while new threats of network attacks by destructing loops have been found recently in network science. We inversely take into account the weakness, and propose a new design principle for incrementally growing robust networks. The networks are self-organized by enhancing interwoven long loops. In particular, we consider the range-limited approximation of linking by intermediations in a few hops, and show the strong robustness in the growth without degrading efficiency of paths. Moreover, we demonstrate that the tolerance of connectivity is reformable even from extremely vulnerable real networks according to our proposed growing process with some investment. These results may indicate a prospective direction to the future growth of our network infrastructures. "
Surrogate Losses for Online Learning of Stepsizes in Stochastic Non-Convex Optimization,"  Stochastic Gradient Descent (SGD) has played a central role in machine learning. However, it requires a carefully hand-picked stepsize for fast convergence, which is notoriously tedious and time-consuming to tune. Over the last several years, a plethora of adaptive gradient-based algorithms have emerged to ameliorate this problem. They have proved efficient in reducing the labor of tuning in practice, but many of them lack theoretic guarantees even in the convex setting. In this paper, we propose new surrogate losses to cast the problem of learning the optimal stepsizes for the stochastic optimization of a non-convex smooth objective function onto an online convex optimization problem. This allows the use of no-regret online algorithms to compute optimal stepsizes on the fly. In turn, this results in a SGD algorithm with self-tuned stepsizes that guarantees convergence rates that are automatically adaptive to the level of noise. "
On Categorical Time Series Models With Covariates,"  We study the problem of stationarity and ergodicity for autoregressive multinomial logistic time series models which possibly include a latent process and are defined by a GARCH-type recursive equation. We improve considerably upon the existing results related to stationarity and ergodicity conditions of such models. Proofs are based on theory developed for chains with complete connections. This approach is based on a useful coupling technique which is utilized for studying ergodicity of more general finite-state stochastic processes. Such processes generalize finite-state Markov chains by assuming infinite order models of past values. For finite order Markov chains, we also discuss ergodicity properties when some strongly exogenous covariates are considered in the dynamics of the process. "
Layered Coding for Energy Harvesting Communication Without CSIT,"  Due to stringent constraints on resources, it may be infeasible to acquire the current channel state information at the transmitter in energy harvesting communication systems. In this paper, we optimize an energy harvesting transmitter, communicating over a slow fading channel, using layered coding. The transmitter has access to the channel statistics, but does not know the exact channel state. In layered coding, the codewords are first designed for each of the channel states at different rates, and then the codewords are either time-multiplexed or superimposed before the transmission, leading to two transmission strategies. The receiver then decodes the information adaptively based on the realized channel state. The transmitter is equipped with a finite-capacity battery having non-zero internal resistance. In each of the transmission strategies, we first formulate and study an average rate maximization problem with non-causal knowledge of the harvested power variations. Further, assuming statistical knowledge and causal information of the harvested power variations, we propose a sub-optimal algorithm, and compare with the stochastic dynamic programming based solution and a greedy policy. "
The automorphisms of Petit's algebras,"Let $\sigma$ be an automorphism of a field $K$ with fixed field $F$. We study the automorphisms of nonassociative unital algebras which are canonical generalizations of the associative quotient algebras $K[t;\sigma]/fK[t;\sigma]$ obtained when the twisted polynomial $f\in K[t;\sigma]$ is invariant, and were first defined by Petit. We compute all their automorphisms if $\sigma$ commutes with all automorphisms in ${\rm Aut}_F(K)$ and $n\geq m-1$, where $n$ is the order of $\sigma$ and $m$ the degree of $f$,and obtain partial results for $n<m-1$. In the case where $K/F$ is a finite Galois field extension, we obtain more detailed information on the structure of the automorphism groups of these nonassociative unital algebras over $F$. We also briefly investigate when two such algebras are isomorphic."
A convolutional autoencoder approach for mining features in cellular electron cryo-tomograms and weakly supervised coarse segmentation,"Cellular electron cryo-tomography enables the 3D visualization of cellular organization in the near-native state and at submolecular resolution. However, the contents of cellular tomograms are often complex, making it difficult to automatically isolate different in situ cellular components. In this paper, we propose a convolutional autoencoder-based unsupervised approach to provide a coarse grouping of 3D small subvolumes extracted from tomograms. We demonstrate that the autoencoder can be used for efficient and coarse characterization of features of macromolecular complexes and surfaces, such as membranes. In addition, the autoencoder can be used to detect non-cellular features related to sample preparation and data collection, such as carbon edges from the grid and tomogram boundaries. The autoencoder is also able to detect patterns that may indicate spatial interactions between cellular components. Furthermore, we demonstrate that our autoencoder can be used for weakly supervised semantic segmentation of cellular components, requiring a very small amount of manual annotation."
Probabilistic Rule Realization and Selection,"  Abstraction and realization are bilateral processes that are key in deriving intelligence and creativity. In many domains, the two processes are approached through rules: high-level principles that reveal invariances within similar yet diverse examples. Under a probabilistic setting for discrete input spaces, we focus on the rule realization problem which generates input sample distributions that follow the given rules. More ambitiously, we go beyond a mechanical realization that takes whatever is given, but instead ask for proactively selecting reasonable rules to realize. This goal is demanding in practice, since the initial rule set may not always be consistent and thus intelligent compromises are needed. We formulate both rule realization and selection as two strongly connected components within a single and symmetric bi-convex problem, and derive an efficient algorithm that works at large scale. Taking music compositional rules as the main example throughout the paper, we demonstrate our model's efficiency in not only music realization (composition) but also music interpretation and understanding (analysis). "
Asymptotic and numerical analysis of a porous medium model for transpiration-driven sap flow in trees,"We develop a 3D porous medium model for sap flow within a tree stem, which consists of a nonlinear parabolic partial differential equation with a suitable transpiration source term. Using an asymptotic analysis, we derive approximate series solutions for the liquid saturation and sap velocity for a general class of coefficient functions. Several important non-dimensional parameters are identified that can be used to characterize various flow regimes. We investigate the relative importance of stem aspect ratio versus anisotropy in the sapwood hydraulic conductivity, and how these two effects impact the radial and vertical components of sap velocity. The analytical results are validated by means of a second-order finite volume discretization of the governing equations, and comparisons are drawn to experimental results on Norway spruce trees."
Risk Estimators for Choosing Regularization Parameters in Ill-Posed Problems - Properties and Limitations,"  This paper discusses the properties of certain risk estimators recently proposed to choose regularization parameters in ill-posed problems. A simple approach is Stein's unbiased risk estimator (SURE), which estimates the risk in the data space, while a recent modification (GSURE) estimates the risk in the space of the unknown variable. It seems intuitive that the latter is more appropriate for ill-posed problems, since the properties in the data space do not tell much about the quality of the reconstruction. We provide theoretical studies of both estimators for linear Tikhonov regularization in a finite dimensional setting and estimate the quality of the risk estimators, which also leads to asymptotic convergence results as the dimension of the problem tends to infinity. Unlike previous papers, who studied image processing problems with a very low degree of ill-posedness, we are interested in the behavior of the risk estimators for increasing ill-posedness. Interestingly, our theoretical results indicate that the quality of the GSURE risk can deteriorate asymptotically for ill-posed problems, which is confirmed by a detailed numerical study. The latter shows that in many cases the GSURE estimator leads to extremely small regularization parameters, which obviously cannot stabilize the reconstruction. Similar but less severe issues with respect to robustness also appear for the SURE estimator, which in comparison to the rather conservative discrepancy principle leads to the conclusion that regularization parameter choice based on unbiased risk estimation is not a reliable procedure for ill-posed problems. A similar numerical study for sparsity regularization demonstrates that the same issue appears in nonlinear variational regularization approaches. "
Analyzing Effects of Seasonal Variations in Wind Generation and Load on Voltage Profiles,"This paper presents a methodology for building daily profiles of wind generation and load for different seasons to assess their impacts on voltage violations. The measurement-based wind models showed very high accuracy when validated against several years of actual wind power data. System load modeling was carried out by analyzing the seasonal trends that occur in residential, commercial, and industrial loads. When the proposed approach was implemented on the IEEE 118-bus system, it could identify violations in bus voltage profiles that the season-independent model could not capture. The results of the proposed approach are expected to provide better visualization of the problems that seasonal variations in wind power and load might cause to the electric power grid."
Imitation Learning from Imperfect Demonstration,"Imitation learning (IL) aims to learn an optimal policy from demonstrations. However, such demonstrations are often imperfect since collecting optimal ones is costly. To effectively learn from imperfect demonstrations, we propose a novel approach that utilizes confidence scores, which describe the quality of demonstrations. More specifically, we propose two confidence-based IL methods, namely two-step importance weighting IL (2IWIL) and generative adversarial IL with imperfect demonstration and confidence (IC-GAIL). We show that confidence scores given only to a small portion of sub-optimal demonstrations significantly improve the performance of IL both theoretically and empirically."
Some Bounds on Binary LCD Codes,"A linear code with a complementary dual (or LCD code) is defined to be a linear code $C$ whose dual code $C^{\perp}$ satisfies $C \cap C^{\perp}$= $\left\{ \mathbf{0}\right\} $. Let $LCD{[}n,k{]}$ denote the maximum of possible values of $d$ among $[n,k,d]$ binary LCD codes. We give exact values of $LCD{[}n,k{]}$ for $1 \le k \le n \le 12$. We also show that $LCD[n,n-i]=2$ for any $i\geq2$ and $n\geq2^{i}$. Furthermore, we show that $LCD[n,k]\leq LCD[n,k-1]$ for $k$ odd and $LCD[n,k]\leq LCD[n,k-2]$ for $k$ even."
Control-Oriented Learning on the Fly,"  This paper focuses on developing a strategy for control of systems whose dynamics are almost entirely unknown. This situation arises naturally in a scenario where a system undergoes a critical failure. In that case, it is imperative to retain the ability to satisfy basic control objectives in order to avert an imminent catastrophe. A prime example of such an objective is the reach-avoid problem, where a system needs to move to a certain state in a constrained state space. To deal with limitations on our knowledge of system dynamics, we develop a theory of myopic control. The primary goal of myopic control is to, at any given time, optimize the current direction of the system trajectory, given solely the information obtained about the system until that time. We propose an algorithm that uses small perturbations in the control effort to learn local dynamics while simultaneously ensuring that the system moves in a direction that appears to be nearly optimal, and provide hard bounds for its suboptimality. We additionally verify the usefulness of the algorithm on a simulation of a damaged aircraft seeking to avoid a crash, as well as on an example of a Van der Pol oscillator. "
Multi Agent Driven Data Mining For Knowledge Discovery in Cloud Computing,"  Today, huge amount of data is available on the web. Now there is a need to convert that data in knowledge which can be useful for different purposes. This paper depicts the use of data mining process, OLAP with the combination of multi agent system to find the knowledge from data in cloud computing. For this, I am also trying to explain one case study of online shopping of one Bakery Shop. May be we can increase the sale of items by using the model, which I am trying to represent. "
Matrix Completion from $O(n)$ Samples in Linear Time,"We consider the problem of reconstructing a rank-$k$ $n \times n$ matrix $M$ from a sampling of its entries. Under a certain incoherence assumption on $M$ and for the case when both the rank and the condition number of $M$ are bounded, it was shown in \cite{CandesRecht2009, CandesTao2010, keshavan2010, Recht2011, Jain2012, Hardt2014} that $M$ can be recovered exactly or approximately (depending on some trade-off between accuracy and computational complexity) using $O(n \, \text{poly}(\log n))$ samples in super-linear time $O(n^{a} \, \text{poly}(\log n))$ for some constant $a \geq 1$. In this paper, we propose a new matrix completion algorithm using a novel sampling scheme based on a union of independent sparse random regular bipartite graphs. We show that under the same conditions w.h.p. our algorithm recovers an $\epsilon$-approximation of $M$ in terms of the Frobenius norm using $O(n \log^2(1/\epsilon))$ samples and in linear time $O(n \log^2(1/\epsilon))$. This provides the best known bounds both on the sample complexity and computational complexity for reconstructing (approximately) an unknown low-rank matrix. The novelty of our algorithm is two new steps of thresholding singular values and rescaling singular vectors in the application of the ""vanilla"" alternating minimization algorithm. The structure of sparse random regular graphs is used heavily for controlling the impact of these regularization steps."
"Ingestion, Indexing and Retrieval of High-Velocity Multidimensional Sensor Data on a Single Node","  Multidimensional data are becoming more prevalent, partly due to the rise of the Internet of Things (IoT), and with that the need to ingest and analyze data streams at rates higher than before. Some industrial IoT applications require ingesting millions of records per second, while processing queries on recently ingested and historical data. Unfortunately, existing database systems suited to multidimensional data exhibit low per-node ingestion performance, and even if they can scale horizontally in distributed settings, they require large number of nodes to meet such ingest demands. For this reason, in this paper we evaluate a single-node multidimensional data store for high-velocity sensor data. Its design centers around a two-level indexing structure, wherein the global index is an in-memory R*-tree and the local indices are serialized kd-trees. This study is confined to records with numerical indexing fields and range queries, and covers ingest throughput, query response time, and storage footprint. We show that the adopted design streamlines data ingestion and offers ingress rates two orders of magnitude higher than those of Percona Server, SQLite, and Druid. Our prototype also reports query response times comparable to or better than those of Percona Server and Druid, and compares favorably in terms of storage footprint. In addition, we evaluate a kd-tree partitioning based scheme for grouping incoming streamed data records. Compared to a random scheme, this scheme produces less overlap between groups of streamed records, but contrary to what we expected, such reduced overlap does not translate into better query performance. By contrast, the local indices prove much more beneficial to query performance. We believe the experience reported in this paper is valuable to practitioners and researchers alike interested in building database systems for high-velocity multidimensional data. "
Electron and thermal transport via Variable Range Hopping in MoSe$_{2}$ single crystals,"Bulk single crystal Molybdenum diselenide has been studied for its electronic and thermal transport properties. We perform resistivity measurements with current in-plane (CIP) and current perpendicular to plane (CPP) as a function of temperature. The CIP measurements exhibit metal to semiconductor transition at $\simeq 31$ K. In the semiconducting phase ($T > 31$ K), the transport is best explained by variable range hopping (VRH) model. Large magnitude of resistivity in CPP mode indicates strong structural anisotropy. Seebeck coefficient as a function of temperature measured in the range $90 - 300$ K, also agrees well with the VRH model. The room temperature Seebeck coefficient is found to be $139$ $\mu$V/K. VRH fittings of the resistivity and Seebeck coefficient data indicate high degree of localization."
Ball in double hoop: demonstration model for numerical optimal control,"  Ball and hoop system is a well-known model for the education of linear control systems. In this paper, we have a look at this system from another perspective and show that it is also suitable for demonstration of more advanced control techniques. In contrast to the standard use, we describe the dynamics of the system at full length; in addition to the mode where the ball rolls on the (outer) hoop we also consider the mode where the ball drops out of the hoop and enters a free-fall mode. Furthermore, we add another (inner) hoop in the center upon which the ball can land from the free-fall mode. This constitutes another mode of the hybrid description of the system. We present two challenging tasks for this model and show how they can be solved by trajectory generation and stabilization. We also describe how such a model can be built and experimentally verify the validity of our approach solving the proposed tasks. "
Forecasting market states,"  We propose a novel methodology to define, analyse and forecast market states. In our approach market states are identified by a reference sparse precision matrix and a vector of expectation values. In our procedure each multivariate observation is associated to a given market state accordingly to a penalized likelihood maximization. The procedure is made computationally very efficient and can be used with a large number of assets. We demonstrate that this procedure successfully classifies different states of the markets in an unsupervised manner. In particular, we describe an experiment with one hundred log-returns and two states in which the methodology automatically associates one state to periods with average positive returns and the other state to periods with average negative returns, therefore discovering spontaneously the common classification of `bull' and `bear' markets. In another experiment, with again one hundred log-returns and two states, we demonstrate that this procedure can be efficiently used to forecast off-sample future market states with significant prediction accuracy. This methodology opens the way to a range of applications in risk management and trading strategies where the correlation structure plays a central role. "
Planetary Ring Dynamics -- The Streamline Formalism -- 2. Theory of Narrow Rings and Sharp Edges,"The present material covers the features of large scale ring dynamics in perturbed flows that were not addressed in part 1 (astro-ph/1606.00759); this includes an extensive coverage of all kinds of ring modes dynamics (except density waves which have been covered in part 1), the origin of ring eccentricities and mode amplitudes, and the issue of ring/gap confinement. This still leaves aside a number of important dynamical issues relating to the ring small scale structure, most notably the dynamics of self-gravitational wakes, of local viscous overstabilities and of ballistic transport processes. As this material is designed to be self-contained, there is some 30% overlap with part 1. This work constitutes a preprint of Chapter 11 of the forthcoming Cambridge University book on rings (Planetary Ring Systems, Matt Tiscareno and Carl Murray, eds)."
Parallel Streaming Wasserstein Barycenters,"  Efficiently aggregating data from different sources is a challenging problem, particularly when samples from each source are distributed differently. These differences can be inherent to the inference task or present for other reasons: sensors in a sensor network may be placed far apart, affecting their individual measurements. Conversely, it is computationally advantageous to split Bayesian inference tasks across subsets of data, but data need not be identically distributed across subsets. One principled way to fuse probability distributions is via the lens of optimal transport: the Wasserstein barycenter is a single distribution that summarizes a collection of input measures while respecting their geometry. However, computing the barycenter scales poorly and requires discretization of all input distributions and the barycenter itself. Improving on this situation, we present a scalable, communication-efficient, parallel algorithm for computing the Wasserstein barycenter of arbitrary distributions. Our algorithm can operate directly on continuous input distributions and is optimized for streaming data. Our method is even robust to nonstationary input distributions and produces a barycenter estimate that tracks the input measures over time. The algorithm is semi-discrete, needing to discretize only the barycenter estimate. To the best of our knowledge, we also provide the first bounds on the quality of the approximate barycenter as the discretization becomes finer. Finally, we demonstrate the practical effectiveness of our method, both in tracking moving distributions on a sphere, as well as in a large-scale Bayesian inference task. "
Lecar's visual comparison method to assess the randomness of Bode's law: an answer,The usual main objection against any attempt in finding a physical cause for the planet distance distribution is based on the assumption that similar distance distribution could be obtained by sequences of random numbers. This assumption was stated by Lecar in an old paper (1973). We show here how this assumption is incorrect and how his visual comparison method is inappropriate.
GoDP: Globally optimized dual pathway system for facial landmark localization in-the-wild,"Facial landmark localization is a fundamental module for pose-invariant face recognition. The most common approach for facial landmark detection is cascaded regression, which is composed of two steps: feature extraction and facial shape regression. Recent methods employ deep convolutional networks to extract robust features for each step, while the whole system could be regarded as a deep cascaded regression architecture. In this work, instead of employing a deep regression network, a Globally Optimized Dual-Pathway (GoDP) deep architecture is proposed to identify the target pixels through solving a cascaded pixel labeling problem without resorting to high-level inference models or complex stacked architecture. The proposed end-to-end system relies on distance-aware softmax functions and dual-pathway proposal-refinement architecture. Results show that it outperforms the state-of-the-art cascaded regression-based methods on multiple in-the-wild face alignment databases. The model achieves 1.84 normalized mean error (NME) on the AFLW database, which outperforms 3DDFA by 61.8%. Experiments on face identification demonstrate that GoDP, coupled with DPM-headhunter, is able to improve rank-1 identification rate by 44.2% compared to Dlib toolbox on a challenging database."
"A function with support of finite measure and ""small"" spectrum",  We construct a function on the real line supported on a set of finite measure whose spectrum has density zero. 
Cell growth rate dictates the onset of glass to fluid-like transition and long time super-diffusion in an evolving cell colony,"Collective migration dominates many phenomena, from cell movement in living systems to abiotic self-propelling particles. Focusing on the early stages of tumor evolution, we enunciate the principles involved in cell dynamics and highlight their implications in understanding similar behavior in seemingly unrelated soft glassy materials and possibly chemokine-induced migration of CD8$^{+}$ T cells. We performed simulations of tumor invasion using a minimal three dimensional model, accounting for cell elasticity and adhesive cell-cell interactions as well as cell birth and death to establish that cell growth rate-dependent tumor expansion results in the emergence of distinct topological niches. Cells at the periphery move with higher velocity perpendicular to the tumor boundary, while motion of interior cells is slower and isotropic. The mean square displacement, $\Delta(t)$, of cells exhibits glassy behavior at times comparable to the cell cycle time, while exhibiting super-diffusive behavior, $\Delta (t) \approx t^{\alpha}$ ($\alpha > 1$), at longer times. We derive the value of $\alpha \approx 1.33$ using a field theoretic approach based on stochastic quantization. In the process we establish the universality of super-diffusion in a class of seemingly unrelated non-equilibrium systems. Super diffusion at long times arises only if there is an imbalance between cell birth and death rates. Our findings for the collective migration, which also suggests that tumor evolution occurs in a polarized manner, are in quantitative agreement with {\it in vitro} experiments. Although set in the context of tumor invasion the findings should also hold in describing collective motion in growing cells and in active systems where creation and annihilation of particles play a role."
Distributed Average Tracking for Lipschitz-Type Nonlinear Dynamical Systems,"  In this paper, a distributed average tracking problem is studied for Lipschitz-type nonlinear dynamical systems. The objective is to design distributed average tracking algorithms for locally interactive agents to track the average of multiple reference signals. Here, in both the agents' and the reference signals' dynamics, there is a nonlinear term satisfying the Lipschitz-type condition. Three types of distributed average tracking algorithms are designed. First, based on state-dependent-gain designing approaches, a robust distributed average tracking algorithm is developed to solve distributed average tracking problems without requiring the same initial condition. Second, by using a gain adaption scheme, an adaptive distributed average tracking algorithm is proposed in this paper to remove the requirement that the Lipschitz constant is known for agents. Third, to reduce chattering and make the algorithms easier to implement, a continuous distributed average tracking algorithm based on a time-varying boundary layer is further designed as a continuous approximation of the previous discontinuous distributed average tracking algorithms. "
On the Möbius Function and Topology of General Pattern Posets,"  We introduce a formal definition of a pattern poset which encompasses several previously studied posets in the literature. Using this definition we present some general results on the Möbius function and topology of such pattern posets. We prove our results using a poset fibration based on the embeddings of the poset, where embeddings are representations of occurrences. We show that the Möbius function of these posets is intrinsically linked to the number of embeddings, and in particular to so called normal embeddings. We present results on when topological properties such as Cohen-Macaulayness and shellability are preserved by this fibration. Furthermore, we apply these results to some pattern posets and derive alternative proofs of existing results, such as Björner's results on subword order. "
A Potapov-type approach to a truncated matricial Stieltjes-type power moment problem,  The paper gives a parametrization of the solution set of a matricial Stieltjes-type truncated power moment problem in the non-degenerate and degenerate cases. The key role plays the solution of the corresponding system of Potapov's fundamental matrix inequalities. 
The scaling limit of the KPZ equation in space dimension 3 and higher,"We study in the present article the Kardar-Parisi-Zhang (KPZ) equation $$ \partial_t h(t,x)=\nu\Delta h(t,x)+\lambda |\nabla h(t,x)|^2 +\sqrt{D}\, \eta(t,x), \qquad (t,x)\in\mathbb{R}_+\times\mathbb{R}^d $$ in $d\ge 3$ dimensions in the perturbative regime, i.e. for $\lambda>0$ small enough and a smooth, bounded, integrable initial condition $h_0=h(t=0,\cdot)$. The forcing term $\eta$ in the right-hand side is a regularized space-time white noise. The exponential of $h$ -- its so-called Cole-Hopf transform -- is known to satisfy a linear PDE with multiplicative noise. We prove a large-scale diffusive limit for the solution, in particular a time-integrated heat-kernel behavior for the covariance in a parabolic scaling. The proof is based on a rigorous implementation of K. Wilson's renormalization group scheme. A double cluster/momentum-decoupling expansion allows for perturbative estimates of the bare resolvent of the Cole-Hopf linear PDE in the small-field region where the noise is not too large, following the broad lines of Iagolnitzer-Magnen. Standard large deviation estimates for $\eta$ make it possible to extend the above estimates to the large-field region. Finally, we show, by resumming all the by-products of the expansion, that the solution $h$ may be written in the large-scale limit (after a suitable Galilei transformation) as a small perturbation of the solution of the underlying linear Edwards-Wilkinson model ($\lambda=0$) with renormalized coefficients $\nu_{eff}=\nu+O(\lambda^2),D_{eff}=D+O(\lambda^2)$."
Beyond Free Riding: Quality of Indicators for Assessing Participation in Information Sharing for Threat Intelligence,"  Threat intelligence sharing has become a growing concept, whereby entities can exchange patterns of threats with each other, in the form of indicators, to a community of trust for threat analysis and incident response. However, sharing threat-related information have posed various risks to an organization that pertains to its security, privacy, and competitiveness. Given the coinciding benefits and risks of threat information sharing, some entities have adopted an elusive behavior of ""free-riding"" so that they can acquire the benefits of sharing without contributing much to the community. So far, understanding the effectiveness of sharing has been viewed from the perspective of the amount of information exchanged as opposed to its quality. In this paper, we introduce the notion of quality of indicators (\qoi) for the assessment of the level of contribution by participants in information sharing for threat intelligence. We exemplify this notion through various metrics, including correctness, relevance, utility, and uniqueness of indicators. In order to realize the notion of \qoi, we conducted an empirical study and taken a benchmark approach to define quality metrics, then we obtained a reference dataset and utilized tools from the machine learning literature for quality assessment. We compared these results against a model that only considers the volume of information as a metric for contribution, and unveiled various interesting observations, including the ability to spot low quality contributions that are synonym to free riding in threat information sharing. "
On the Design of LQR Kernels for Efficient Controller Learning,"  Finding optimal feedback controllers for nonlinear dynamic systems from data is hard. Recently, Bayesian optimization (BO) has been proposed as a powerful framework for direct controller tuning from experimental trials. For selecting the next query point and finding the global optimum, BO relies on a probabilistic description of the latent objective function, typically a Gaussian process (GP). As is shown herein, GPs with a common kernel choice can, however, lead to poor learning outcomes on standard quadratic control problems. For a first-order system, we construct two kernels that specifically leverage the structure of the well-known Linear Quadratic Regulator (LQR), yet retain the flexibility of Bayesian nonparametric learning. Simulations of uncertain linear and nonlinear systems demonstrate that the LQR kernels yield superior learning performance. "
How does propaganda influence the opinion dynamics of a population ?,"  The evolution of opinions in a population of individuals who constantly interact with a common source of user-generated content (i.e. the internet) and are also subject to propaganda is analyzed using computer simulations. The model is based on the bounded confidence approach. In the absence of propaganda, computer simulations show that the online population as a whole is either fragmented, polarized or in perfect harmony on a certain issue or ideology depending on the uncertainty of individuals in accepting opinions not closer to theirs. On applying the model to simulate radicalization, a proportion of the online population, subject to extremist propaganda radicalize depending on their pre-conceived opinions and opinion uncertainty. It is found that an optimal counter propaganda that prevents radicalization is not necessarily centrist. "
Explicit Solution for Constrained Stochastic Linear-Quadratic Control with Multiplicative Noise,"  We study in this paper a class of constrained linear-quadratic (LQ) optimal control problem formulations for the scalar-state stochastic system with multiplicative noise, which has various applications, especially in the financial risk management. The linear constraint on both the control and state variables considered in our model destroys the elegant structure of the conventional LQ formulation and has blocked the derivation of an explicit control policy so far in the literature. We successfully derive in this paper the analytical control policy for such a class of problems by utilizing the state separation property induced from its structure. We reveal that the optimal control policy is a piece-wise affine function of the state and can be computed off-line efficiently by solving two coupled Riccati equations. Under some mild conditions, we also obtain the stationary control policy for infinite time horizon. We demonstrate the implementation of our method via some illustrative examples and show how to calibrate our model to solve dynamic constrained portfolio optimization problems. "
Viewpoint Selection for Photographing Architectures,"This paper studies the problem of how to choose good viewpoints for taking photographs of architectures. We achieve this by learning from professional photographs of world famous landmarks that are available on the Internet. Unlike previous efforts devoted to photo quality assessment which mainly rely on 2D image features, we show in this paper combining 2D image features extracted from images with 3D geometric features computed on the 3D models can result in more reliable evaluation of viewpoint quality. Specifically, we collect a set of photographs for each of 15 world famous architectures as well as their 3D models from the Internet. Viewpoint recovery for images is carried out through an image-model registration process, after which a newly proposed viewpoint clustering strategy is exploited to validate users' viewpoint preferences when photographing landmarks. Finally, we extract a number of 2D and 3D features for each image based on multiple visual and geometric cues and perform viewpoint recommendation by learning from both 2D and 3D features using a specifically designed SVM-2K multi-view learner, achieving superior performance over using solely 2D or 3D features. We show the effectiveness of the proposed approach through extensive experiments. The experiments also demonstrate that our system can be used to recommend viewpoints for rendering textured 3D models of buildings for the use of architectural design, in addition to viewpoint evaluation of photographs and recommendation of viewpoints for photographing architectures in practice."
Geophysical tests for habitability in ice-covered ocean worlds,"Geophysical measurements can reveal the structure of icy ocean worlds and cycling of volatiles. The associated density, temperature, sound speed, and electrical conductivity of such worlds thus characterizes their habitability. To explore the variability and correlation of these parameters, and to provide tools for planning and data analyses, we develop 1-D calculations of internal structure, which use available constraints on the thermodynamics of aqueous MgSO$_4$, NaCl (as seawater), and NH$_3$, water ices, and silicate content. Limits in available thermodynamic data narrow the parameter space that can be explored: insufficient coverage in pressure, temperature, and composition for end-member salinities of MgSO$_4$ and NaCl, and for relevant water ices; and a dearth of suitable data for aqueous mixtures of Na-Mg-Cl-SO$_4$-NH$_3$. For Europa, ocean compositions that are oxidized and dominated by MgSO$_4$, vs reduced (NaCl), illustrate these gaps, but also show the potential for diagnostic and measurable combinations of geophysical parameters. The low-density rocky core of Enceladus may comprise hydrated minerals, or anydrous minerals with high porosity comparable to Earth's upper mantle. Titan's ocean must be dense, but not necessarily saline, as previously noted, and may have little or no high-pressure ice at its base. Ganymede's silicious interior is deepest among all known ocean worlds, and may contain multiple phases of high-pressure ice, which will become buoyant if the ocean is sufficiently salty. Callisto's likely near-eutectic ocean cannot be adequately modeled using available data. Callisto may also lack high-pressure ices, but this cannot be confirmed due to uncertainty in its moment of inertia."
A fast speed planning algorithm for robotic manipulators,"  We consider the speed planning problem for a robotic manipulator. In particular, we present an algorithm for finding the time-optimal speed law along an assigned path that satisfies velocity and acceleration constraints and respects the maximum forces and torques allowed by the actuators. The addressed optimization problem is a finite dimensional reformulation of the continuous-time speed optimization problem, obtained by discretizing the speed profile with N points. The proposed algorithm has linear complexity with respect to N and to the number of degrees of freedom. Such complexity is the best possible for this problem. Numerical tests show that the proposed algorithm is significantly faster than algorithms already existing in literature. "
Linearly convergent stochastic heavy ball method for minimizing generalization error,"  In this work we establish the first linear convergence result for the stochastic heavy ball method. The method performs SGD steps with a fixed stepsize, amended by a heavy ball momentum term. In the analysis, we focus on minimizing the expected loss and not on finite-sum minimization, which is typically a much harder problem. While in the analysis we constrain ourselves to quadratic loss, the overall objective is not necessarily strongly convex. "
Applying the Spacecraft with a Solar Sail to Form the Climate on a Mars Base,"  This article is devoted to research the application of the spacecraft with a solar sail to support the certain climatic conditions in an area of the Mars surface. Authors propose principles of functioning of the spacecraft, intended to create a light and thermal light spot in a predetermined area of the Martian surface. The mathematical motion model in such condition of the solar sail's orientation is considered and used for motion simulation session. Moreover, the analysis of this motion is performed. Thus, were obtained parameters of the stationary orbit of the spacecraft with a solar sail and were given recommendations for further applying spacecrafts to reflect the sunlight on a planet's surface. "
Data processing pipeline for Herschel HIFI,"{Context}. The HIFI instrument on the Herschel Space Observatory performed over 9100 astronomical observations, almost 900 of which were calibration observations in the course of the nearly four-year Herschel mission. The data from each observation had to be converted from raw telemetry into calibrated products and were included in the Herschel Science Archive. {Aims}. The HIFI pipeline was designed to provide robust conversion from raw telemetry into calibrated data throughout all phases of the HIFI missions. Pre-launch laboratory testing was supported as were routine mission operations. {Methods}. A modular software design allowed components to be easily added, removed, amended and/or extended as the understanding of the HIFI data developed during and after mission operations. {Results}. The HIFI pipeline processed data from all HIFI observing modes within the Herschel automated processing environment as well as within an interactive environment. The same software can be used by the general astronomical community to reprocess any standard HIFI observation. The pipeline also recorded the consistency of processing results and provided automated quality reports. Many pipeline modules were in use since the HIFI pre-launch instrument level testing. {Conclusions}. Processing in steps facilitated data analysis to discover and address instrument artefacts and uncertainties. The availability of the same pipeline components from pre-launch throughout the mission made for well-understood, tested, and stable processing. A smooth transition from one phase to the next significantly enhanced processing reliability and robustness."
Regret Analysis for Continuous Dueling Bandit,"  The dueling bandit is a learning framework wherein the feedback information in the learning process is restricted to a noisy comparison between a pair of actions. In this research, we address a dueling bandit problem based on a cost function over a continuous space. We propose a stochastic mirror descent algorithm and show that the algorithm achieves an $O(\sqrt{T\log T})$-regret bound under strong convexity and smoothness assumptions for the cost function. Subsequently, we clarify the equivalence between regret minimization in dueling bandit and convex optimization for the cost function. Moreover, when considering a lower bound in convex optimization, our algorithm is shown to achieve the optimal convergence rate in convex optimization and the optimal regret in dueling bandit except for a logarithmic factor. "
X-ray induced deuterium enrichment on N-rich organics in protoplanetary disks: an experimental investigation using synchrotron light,"The deuterium enrichment of organics in the interstellar medium, protoplanetary disks and meteorites has been proposed to be the result of ionizing radiation. The goal of this study is to quantify the effects of soft X-rays (0.1 - 2 keV), a component of stellar radiation fields illuminating protoplanetary disks, on the refractory organics present in the disks. We prepared tholins, nitrogen-rich complex organics, via plasma deposition and used synchrotron radiation to simulate X-ray fluences in protoplanetary disks. Controlled irradiation experiments at 0.5 and 1.3 keV were performed at the SEXTANTS beam line of the SOLEIL synchrotron, and were followed by ex-situ infrared, Raman and isotopic diagnostics. Infrared spectroscopy revealed the loss of singly-bonded groups (N-H, C-H and R-N$\equiv$C) and the formation of sp$^3$ carbon defects. Raman analysis revealed the introduction of defects and structural amorphization. Finally, tholins were measured via secondary ion mass spectrometry (SIMS), revealing that significant D-enrichment is induced by X-ray irradiation. Our results are compared to previous experimental studies involving the thermal degradation and electron irradiation of organics. The penetration depth of soft X-rays in $\mu$m-sized tholins leads to volume rather than surface modifications: lower energy X-rays (0.5 keV) induce a larger D-enrichment than 1.3 keV X-rays, reaching a plateau for doses larger than 5 $\times$ 10$^{27}$ eV cm$^{-3}$. Our work provides experimental evidence of a new non-thermal pathway to deuterium fractionation of organic matter."
Parisian ruin of Brownian motion risk model over an infinite-time horizon,"Let $B(t), t\in \mathbb{R}$ be a standard Brownian motion. In this paper, we derive the exact asymptotics of the probability of Parisian ruin on infinite time horizon for the following risk process \begin{align}\label{Rudef} R_u^{\delta}(t)=e^{\delta t}\left(u+c\int^{t}_{0}e^{-\delta v}d v-\sigma\int_{0}^{t}e^{-\delta v}d B(v)\right),\quad t\geq0, \end{align} where $u\geq 0$ is the initial reserve, $\delta\geq0$ is the force of interest, $c>0$ is the rate of premium and $\sigma>0$ is a volatility factor. Further, we show the asymptotics of the Parisian ruin time of this risk process."
"Exogeneity tests, incomplete models, weak identification and non-Gaussian distributions: invariance and finite-sample distributional theory","We study the distribution of Durbin-Wu-Hausman (DWH) and Revankar-Hartley (RH) tests for exogeneity from a finite-sample viewpoint, under the null and alternative hypotheses. We consider linear structural models with possibly non-Gaussian errors, where structural parameters may not be identified and where reduced forms can be incompletely specified (or nonparametric). On level control, we characterize the null distributions of all the test statistics. Through conditioning and invariance arguments, we show that these distributions do not involve nuisance parameters. In particular, this applies to several test statistics for which no finite-sample distributional theory is yet available, such as the standard statistic proposed by Hausman (1978). The distributions of the test statistics may be non-standard -- so corrections to usual asymptotic critical values are needed -- but the characterizations are sufficiently explicit to yield finite-sample (Monte-Carlo) tests of the exogeneity hypothesis. The procedures so obtained are robust to weak identification, missing instruments or misspecified reduced forms, and can easily be adapted to allow for parametric non-Gaussian error distributions. We give a general invariance result (block triangular invariance) for exogeneity test statistics. This property yields a convenient exogeneity canonical form and a parsimonious reduction of the parameters on which power depends. In the extreme case where no structural parameter is identified, the distributions under the alternative hypothesis and the null hypothesis are identical, so the power function is flat, for all the exogeneity statistics. However, as soon as identification does not fail completely, this phenomenon typically disappears."
Pair formation of hard core bosons in flat band systems,"  Hard core bosons in a large class of one or two dimensional flat band systems have an upper critical density, below which the ground states can be described completely. At the critical density, the ground states are Wigner crystals. If one adds a particle to the system at the critical density, the ground state and the low lying multi particle states of the system can be described as a Wigner crystal with an additional pair of particles. The energy band for the pair is separated from the rest of the multi-particle spectrum. The proofs use a Gerschgorin type of argument for block diagonally dominant matrices. In certain one-dimensional or tree-like structures one can show that the pair is localised, for example in the chequerboard chain. For this one-dimensional system with periodic boundary condition the energy band for the pair is flat, the pair is localised. "
"A Note on the Relationship Between Conditional and Unconditional Independence, and its Extensions for Markov Kernels","  Two known results on the relationship between conditional and unconditional independence are obtained as a consequence of the main result of this paper, a theorem that uses independence of Markov kernels to obtain a minimal condition which added to conditional independence implies independence. Some counterexamples and representation results are provided to clarify the concepts introduced and the propositions of the statement of the main theorem. Moreover, conditional independence and the mentioned results are extended to the framework of Markov kernels. "
The Effect of Temperature on Amdahl Law in 3D Multicore Era,"This work studies the influence of temperature on performance and scalability of 3D Chip Multiprocessors (CMP) from Amdahl law perspective. We find that 3D CMP may reach its thermal limit before reaching its maximum power. We show that a high level of parallelism may lead to high peak temperatures even in small scale 3D CMPs, thus limiting 3D CMP scalability and calling for different, in-memory computing architectures."
Video Object Segmentation using Supervoxel-Based Gerrymandering,"  Pixels operate locally. Superpixels have some potential to collect information across many pixels; supervoxels have more potential by implicitly operating across time. In this paper, we explore this well established notion thoroughly analyzing how supervoxels can be used in place of and in conjunction with other means of aggregating information across space-time. Focusing on the problem of strictly unsupervised video object segmentation, we devise a method called supervoxel gerrymandering that links masks of foregroundness and backgroundness via local and non-local consensus measures. We pose and answer a series of critical questions about the ability of supervoxels to adequately sway local voting; the questions regard type and scale of supervoxels as well as local versus non-local consensus, and the questions are posed in a general way so as to impact the broader knowledge of the use of supervoxels in video understanding. We work with the DAVIS dataset and find that our analysis yields an unsupervised method that outperforms all other known unsupervised methods and even many supervised ones. "
The Absent-Minded Driver Problem Redux,"  This paper reconsiders the problem of the absent-minded driver who must choose between alternatives with different payoff with imperfect recall and varying degrees of knowledge of the system. The classical absent-minded driver problem represents the case with limited information and it has bearing on the general area of communication and learning, social choice, mechanism design, auctions, theories of knowledge, belief, and rational agency. Within the framework of extensive games, this problem has applications to many artificial intelligence scenarios. It is obvious that the performance of the agent improves as information available increases. It is shown that a non-uniform assignment strategy for successive choices does better than a fixed probability strategy. We consider both classical and quantum approaches to the problem. We argue that the superior performance of quantum decisions with access to entanglement cannot be fairly compared to a classical algorithm. If the cognitive systems of agents are taken to have access to quantum resources, or have a quantum mechanical basis, then that can be leveraged into superior performance. "
Real-time Acceleration-continuous Path-constrained Trajectory Planning With Built-in Tradability Between Cruise and Time-optimal Motions,"  In this paper, a novel real-time acceleration-continuous path-constrained trajectory planning algorithm is proposed with an appealing built-in tradability mechanism between cruise motion and time-optimal motion. Different from existing approaches, the proposed approach smoothens time-optimal trajectories with bang-bang input structures to generate acceleration-continuous trajectories while preserving the completeness property. More importantly, a novel built-in tradability mechanism is proposed and embedded into the trajectory planning framework, so that the proportion of the cruise motion and time-optimal motion can be flexibly adjusted by changing a user-specified functional parameter. Thus, the user can easily apply the trajectory planning algorithm for various tasks with different requirements on motion efficiency and cruise proportion. Moreover, it is shown that feasible trajectories are computed more quickly than optimal trajectories. Rigorous mathematical analysis and proofs are provided for these aforementioned results. Comparative simulation and experimental results on omnidirectional wheeled mobile robots demonstrate the capability of the proposed algorithm in terms of flexible tunning between cruise and time-optimal motions, as well as higher computational efficiency. "
Multi-locus data distinguishes between population growth and multiple merger coalescents,"  We introduce a low dimensional function of the site frequency spectrum that is tailor-made for distinguishing coalescent models with multiple mergers from Kingman coalescent models with population growth, and use this function to construct a hypothesis test between these model classes. The null and alternative sampling distributions of the statistic are intractable, but its low dimensionality renders them amenable to Monte Carlo estimation. We construct kernel density estimates of the sampling distributions based on simulated data, and show that the resulting hypothesis test dramatically improves on the statistical power of a current state-of-the-art method. A key reason for this improvement is the use of multi-locus data, in particular averaging observed site frequency spectra across unlinked loci to reduce sampling variance. We also demonstrate the robustness of our method to nuisance and tuning parameters. Finally we show that the same kernel density estimates can be used to conduct parameter estimation, and argue that our method is readily generalisable for applications in model selection, parameter inference and experimental design. "
Ramsey interferometry of Rydberg ensembles inside microwave cavities,"We study ensembles of Rydberg atoms in a confined electromagnetic environment such as provided by a microwave cavity. The competition between standard free space Ising type and cavity-mediated interactions leads to the emergence of different regimes where the particle-particle couplings range from the typical van der Waals $r^{-6}$ behavior to $r^{-3}$ and to $r$-independence. We apply a Ramsey spectroscopic technique to map the two-body interactions into a characteristic signal such as intensity and contrast decay curves. As opposed to previous treatments requiring high-densities for considerable contrast and phase decay, the cavity scenario can exhibit similar behavior at much lower densities."
Impurity self-energy in the strongly-correlated Bose systems,We proposed the non-perturbative scheme for calculation of the impurity spectrum in the Bose system at zero temperature. The method is based on the path-integral formulation and describes an impurity as a zero-density ideal Fermi gas interacting with Bose system for which the action is written in terms of density fluctuations. On the example of the $^3$He atom immersed in the liquid helium-4 a good consistency with experimental data and results of Monte Carlo simulations is shown.
$k$-step correction for mixed integer linear programming: a new approach for instrumental variable quantile regressions and related problems,"This paper proposes a new framework for estimating instrumental variable (IV) quantile models. The first part of our proposal can be cast as a mixed integer linear program (MILP), which allows us to capitalize on recent progress in mixed integer optimization. The computational advantage of the proposed method makes it an attractive alternative to existing estimators in the presence of multiple endogenous regressors. This is a situation that arises naturally when one endogenous variable is interacted with several other variables in a regression equation. In our simulations, the proposed method using MILP with a random starting point can reliably estimate regressions for a sample size of 500 with 20 endogenous variables in 5 seconds. Theoretical results for early termination of MILP are also provided. The second part of our proposal is a $k$-step correction framework, which is proved to be able to convert any point within a small but fixed neighborhood of the true parameter value into an estimate that is asymptotically equivalent to GMM. Our result does not require the initial estimate to be consistent and only $2\log n$ iterations are needed. Since the $k$-step correction does not require any optimization, applying the $k$-step correction to MILP estimate provides a computationally attractive way of obtaining efficient estimators. When dealing with very large data sets, we can run the MILP algorithm on only a small subsample and our theoretical results guarantee that the resulting estimator from the $k$-step correction is equivalent to computing GMM on the full sample. As a result, we can handle massive datasets of millions of observations within seconds. As an empirical illustration, we examine the heterogeneous treatment effect of Job Training Partnership Act (JTPA) using a regression with 13 interaction terms of the treatment variable."
An MCMC Algorithm for Estimating the Reduced RUM,"The RRUM is a model that is frequently seen in language assessment studies. The objective of this research is to advance an MCMC algorithm for the Bayesian RRUM. The algorithm starts with estimating correlated attributes. Using a saturated model and a binary decimal conversion, the algorithm transforms possible attribute patterns to a Multinomial distribution. Along with the likelihood of an attribute pattern, a Dirichlet distribution is used as the prior to sample from the posterior. The Dirichlet distribution is constructed using Gamma distributions. Correlated attributes of examinees are generated using the inverse transform sampling. Model parameters are estimated using the Metropolis within Gibbs sampler sequentially. Two simulation studies are conducted to evaluate the performance of the algorithm. The first simulation uses a complete and balanced Q-matrix that measures 5 attributes. Comprised of 28 items and 9 attributes, the Q-matrix for the second simulation is incomplete and imbalanced. The empirical study uses the ECPE data obtained from the CDM R package. Parameter estimates from the MCMC algorithm and from the CDM R package are presented and compared. The algorithm developed in this research is implemented in R."
Stretching p-wave molecules by transverse confinements,"We revisit the confinement-induced p-wave resonance in quasi-one-dimensional (quasi-1D) atomic gases and study the induced molecules near resonance. We derive the reduced 1D interaction parameters and show that they can well predict the binding energy of shallow molecules in quasi-1D system. Importantly, these shallow molecules are found to be much more spatially extended compared to those in three dimensions (3D) without transverse confinement. Our results strongly indicate that a p-wave interacting atomic gas can be much more stable in quasi-1D near the induced p-wave resonance, where most weight of the molecule lies outside the short-range regime and thus the atom loss could be suppressed."
A Conceptual Framework for Supporting a Rapid Design of Web Applications for Data Analysis of Electrical Quality Assurance Data for the LHC,"  The Large Hadron Collider (LHC) is one of the most complex machines ever build. It is composed of many components which constitute a large system. The tunnel and the accelerator is just one of a very critical fraction of the whole LHC infrastructure. Hardware comissioning as one of the critical processes before running the LHC is implemented during the Long Shutdown (LS) states of the macine, where Electrical Quality Assurance (ELQA) is one of its key components. Here a huge data is collected when implementing various ELQA electrical tests. In this paper we present a conceptual framework for supporting a rapid design of web applications for ELQA data analysis. We show a framework's main components, their possible integration with other systems and machine learning algorithms and a simple use case of prototyping an application for Electrical Quality Assurance of the LHC. "
The Parallel Meaning Bank: Towards a Multilingual Corpus of Translations Annotated with Compositional Meaning Representations,"The Parallel Meaning Bank is a corpus of translations annotated with shared, formal meaning representations comprising over 11 million words divided over four languages (English, German, Italian, and Dutch). Our approach is based on cross-lingual projection: automatically produced (and manually corrected) semantic annotations for English sentences are mapped onto their word-aligned translations, assuming that the translations are meaning-preserving. The semantic annotation consists of five main steps: (i) segmentation of the text in sentences and lexical items; (ii) syntactic parsing with Combinatory Categorial Grammar; (iii) universal semantic tagging; (iv) symbolization; and (v) compositional semantic analysis based on Discourse Representation Theory. These steps are performed using statistical models trained in a semi-supervised manner. The employed annotation models are all language-neutral. Our first results are promising."
A nonparametric copula approach to conditional Value-at-Risk,"  Value-at-Risk and its conditional allegory, which takes into account the available information about the economic environment, form the centrepiece of the Basel framework for the evaluation of market risk in the banking sector. In this paper, a new nonparametric framework for estimating this conditional Value-at-Risk is presented. A nonparametric approach is particularly pertinent as the traditionally used parametric distributions have been shown to be insufficiently robust and flexible in most of the equity-return data sets observed in practice. The method extracts the quantile of the conditional distribution of interest, whose estimation is based on a novel estimator of the density of the copula describing the dynamic dependence observed in the series of returns. Real-world back-testing analyses demonstrate the potential of the approach, whose performance may be superior to its industry counterparts. "
Towards High-quality Visualization of Superfluid Vortices,"  Superfluidity is a special state of matter exhibiting macroscopic quantum phenomena and acting like a fluid with zero viscosity. In such a state, superfluid vortices exist as phase singularities of the model equation with unique distributions. This paper presents novel techniques to aid the visual understanding of superfluid vortices based on the state-of-the-art non-linear Klein-Gordon equation, which evolves a complex scalar field, giving rise to special vortex lattice/ring structures with dynamic vortex formation, reconnection, and Kelvin waves, etc. By formulating a numerical model with theoretical physicists in superfluid research, we obtain high-quality superfluid flow data sets without noise-like waves, suitable for vortex visualization. By further exploring superfluid vortex properties, we develop a new vortex identification and visualization method: a novel mechanism with velocity circulation to overcome phase singularity and an orthogonal-plane strategy to avoid ambiguity. Hence, our visualizations can help reveal various superfluid vortex structures and enable domain experts for related visual analysis, such as the steady vortex lattice/ring structures, dynamic vortex string interactions with reconnections and energy radiations, where the famous Kelvin waves and decaying vortex tangle were clearly observed. These visualizations have assisted physicists to verify the superfluid model, and further explore its dynamic behavior more intuitively. "
Nonrepetitive edge-colorings of trees,"A repetition is a sequence of symbols in which the first half is the same as the second half. An edge-coloring of a graph is repetition-free or nonrepetitive if there is no path with a color pattern that is a repetition. The minimum number of colors so that a graph has a nonrepetitive edge-coloring is called its Thue edge-chromatic number. We improve on the best known general upper bound of $4\Delta-4$ for the Thue edge-chromatic number of trees of maximum degree $\Delta$ due to Alon, Grytczuk, Ha{\l}uszczak and Riordan (2002) by providing a simple nonrepetitive edge-coloring with $3\Delta-2$ colors."
On the new wave behavior to the longitudinal wave quation in a magneto-electro-elastic circular rod,"With the aid of the symbolic computations software; Wolfram Mathematica 9, the powerful sine-Gordon expansion method is used in examining the analytical solution of the longitudinal wave equation in a magneto-electro-elastic circular rod. Sine-Gordon expansion method is based on the well-known sine-Gordon equation and a wave transformation. The longitudinal wave equation is an equation that arises in mathematical physics with dispersion caused by the transverse Poisson's effect in a magneto-electro-elastic circular rod. We successfully get some solutions with the complex, trigonometric and hyperbolic function structure. We present the numerical simulations of all the obtained solutions by choosing appropriate values of the parameters. We give the physical meanings of some of the obtained analytical solutions which significantly explain some practical physical problems."
Formulation of Deep Reinforcement Learning Architecture Toward Autonomous Driving for On-Ramp Merge,"  Multiple automakers have in development or in production automated driving systems (ADS) that offer freeway-pilot functions. This type of ADS is typically limited to restricted-access freeways only, that is, the transition from manual to automated modes takes place only after the ramp merging process is completed manually. One major challenge to extend the automation to ramp merging is that the automated vehicle needs to incorporate and optimize long-term objectives (e.g. successful and smooth merge) when near-term actions must be safely executed. Moreover, the merging process involves interactions with other vehicles whose behaviors are sometimes hard to predict but may influence the merging vehicle optimal actions. To tackle such a complicated control problem, we propose to apply Deep Reinforcement Learning (DRL) techniques for finding an optimal driving policy by maximizing the long-term reward in an interactive environment. Specifically, we apply a Long Short-Term Memory (LSTM) architecture to model the interactive environment, from which an internal state containing historical driving information is conveyed to a Deep Q-Network (DQN). The DQN is used to approximate the Q-function, which takes the internal state as input and generates Q-values as output for action selection. With this DRL architecture, the historical impact of interactive environment on the long-term reward can be captured and taken into account for deciding the optimal control policy. The proposed architecture has the potential to be extended and applied to other autonomous driving scenarios such as driving through a complex intersection or changing lanes under varying traffic flow conditions. "
CDS Rate Construction Methods by Machine Learning Techniques,"Regulators require financial institutions to estimate counterparty default risks from liquid CDS quotes for the valuation and risk management of OTC derivatives. However, the vast majority of counterparties do not have liquid CDS quotes and need proxy CDS rates. Existing methods cannot account for counterparty-specific default risks; we propose to construct proxy CDS rates by associating to illiquid counterparty liquid CDS Proxy based on Machine Learning Techniques. After testing 156 classifiers from 8 most popular classifier families, we found that some classifiers achieve highly satisfactory accuracy rates. Furthermore, we have rank-ordered the performances and investigated performance variations amongst and within the 8 classifier families. This paper is, to the best of our knowledge, the first systematic study of CDS Proxy construction by Machine Learning techniques, and the first systematic classifier comparison study based entirely on financial market data. Its findings both confirm and contrast existing classifier performance literature. Given the typically highly correlated nature of financial data, we investigated the impact of correlation on classifier performance. The techniques used in this paper should be of interest for financial institutions seeking a CDS Proxy method, and can serve for proxy construction for other financial variables. Some directions for future research are indicated."
Eigentriads and Eigenprogressions on the Tonnetz,"  We introduce a new multidimensional representation, named eigenprogression transform, that characterizes some essential patterns of Western tonal harmony while being equivariant to time shifts and pitch transpositions. This representation is deep, multiscale, and convolutional in the piano-roll domain, yet incurs no prior training, and is thus suited to both supervised and unsupervised MIR tasks. The eigenprogression transform combines ideas from the spiral scattering transform, spectral graph theory, and wavelet shrinkage denoising. We report state-of-the-art results on a task of supervised composer recognition (Haydn vs. Mozart) from polyphonic music pieces in MIDI format. "
A generalization of the Log Lindley distribution -- its properties and applications,"An extension of the two-parameter Log-Lindley distribution of Gomez et al. (2014) with support in (0, 1) is proposed. Its important properties like cumulative distribution function, moments, survival function, hazard rate function, Shannon entropy, stochastic n ordering and convexity (concavity) conditions are derived. An application in distorted premium principal is outlined and parameter estimation by method of maximum likelihood is also presented. We also consider use of a re-parameterized form of the proposed distribution in regression modeling for bounded responses by considering a modeling of real life data in comparison with beta regression and log-Lindley regression models."
A Flux Conserving Meshfree Method for Conservation Laws,"  Lack of conservation has been the biggest drawback in meshfree generalized finite difference methods (GFDMs). In this paper, we present a novel modification of classical meshfree GFDMs to include local balances which produce an approximate conservation of numerical fluxes. This numerical flux conservation is done within the usual moving least squares framework. Unlike Finite Volume Methods, it is based on locally defined control cells, rather than a globally defined mesh. We present the application of this method to an advection diffusion equation and the incompressible Navier - Stokes equations. Our simulations show that the introduction of flux conservation significantly reduces the errors in conservation in meshfree GFDMs. "
Anomaly detection in the dynamics of web and social networks,"  In this work, we propose a new, fast and scalable method for anomaly detection in large time-evolving graphs. It may be a static graph with dynamic node attributes (e.g. time-series), or a graph evolving in time, such as a temporal network. We define an anomaly as a localized increase in temporal activity in a cluster of nodes. The algorithm is unsupervised. It is able to detect and track anomalous activity in a dynamic network despite the noise from multiple interfering sources. We use the Hopfield network model of memory to combine the graph and time information. We show that anomalies can be spotted with a good precision using a memory network. The presented approach is scalable and we provide a distributed implementation of the algorithm. To demonstrate its efficiency, we apply it to two datasets: Enron Email dataset and Wikipedia page views. We show that the anomalous spikes are triggered by the real-world events that impact the network dynamics. Besides, the structure of the clusters and the analysis of the time evolution associated with the detected events reveals interesting facts on how humans interact, exchange and search for information, opening the door to new quantitative studies on collective and social behavior on large and dynamic datasets. "
Local coefficients and the Herbert Formula,"  We discuss a generalisation of the Herbert formula for double points, when the normal bundle of an immersion admits an additional structure, and an application. "
Emission line ratios of Fe III as astrophysical plasma diagnostics,"Recent state-of-the-art calculations of A-values and electron impact excitation rates for Fe III are used in conjunction with the Cloudy modeling code to derive emission line intensity ratios for optical transitions among the fine-structure levels of the 3d$^6$ configuration. A comparison of these with high resolution, high signal-to-noise spectra of gaseous nebulae reveals that previous discrepancies found between theory and observation are not fully resolved by the latest atomic data. Blending is ruled out as a likely cause of the discrepancies, because temperature- and density-independent ratios (arising from lines with common upper levels) match well with those predicted by theory. For a typical nebular plasma with electron temperature $T_{\rm e} = 9000$ K and electron density $\rm N_{e}=10^4 \, cm^{-3}$, cascading of electrons from the levels $\rm ^3G_5$, $\rm ^3G_4$ and $\rm ^3G_3$ plays an important role in determining the populations of lower levels, such as $\rm ^3F_4$, which provide the density diagnostic emission lines of Fe III, such as $\rm ^5D_4$ - $\rm ^3F_4$ at 4658 \AA. Hence further work on the A-values for these transitions is recommended, ideally including measurements if possible. However, some Fe III ratios do provide reliable $N_{\rm e}$-diagnostics, such as 4986/4658. The Fe III cooling function calculated with Cloudy using the most recent atomic data is found to be significantly greater at $T_e$ $\simeq$ 30000 K than predicted with the existing Cloudy model. This is due to the presence of additional emission lines with the new data, particularly in the 1000--4000 \AA\ wavelength region."
"Warped cones, (non-)rigidity, and piecewise properties, with a joint appendix with Dawid Kielak","  We prove that if a quasi-isometry of warped cones is induced by a map between the base spaces of the cones, the actions must be conjugate by this map. The converse is false in general, conjugacy of actions is not sufficient for quasi-isometry of the respective warped cones. For a general quasi-isometry of warped cones, using the asymptotically faithful covering constructed in a previous work with Jianchao Wu, we deduce that the two groups are quasi-isometric after taking Cartesian products with suitable powers of the integers. Secondly, we characterise geometric properties of a group (coarse embeddability into Banach spaces, asymptotic dimension, property A) by properties of the warped cone over an action of this group. These results apply to arbitrary asymptotically faithful coverings, in particular to box spaces. As an application, we calculate the asymptotic dimension of a warped cone, improve bounds by Szabó, Wu, and Zacharias and by Bartels on the amenability dimension of actions of virtually nilpotent groups, and give a partial answer to a question of Willett about dynamic asymptotic dimension. In the appendix, we justify optimality of the aforementioned result on general quasi-isometries by showing that quasi-isometric warped cones need not come from quasi-isometric groups, contrary to the case of box spaces. "
Enhancing MapReduce Fault Recovery Through Binocular Speculation,"  MapReduce speculation plays an important role in finding potential task stragglers and failures. But a tacit dichotomy exists in MapReduce due to its inherent two-phase (map and reduce) management scheme in which map tasks and reduce tasks have distinctly different execution behaviors, yet reduce tasks are dependent on the results of map tasks. We reveal that speculation policies for fault handling in MapReduce do not recognize this dichotomy between map and reduce tasks, which leads to an issue of speculation myopia for MapReduce fault recovery. These issues cause significant performance degradation upon network and node failures. To address the speculation myopia caused by MapReduce dichotomy, we introduce a new scheme called binocular speculation to help MapReduce increase its assessment scope for speculation. As part of the scheme, we also design three component techniques including neighborhood glance, collective speculation and speculative rollback. Our evaluation shows that, with these techniques, binocular speculation can increase the coordination of map and reduce phases, and enhance the efficiency of MapReduce fault recovery. "
Generalized classes of continuous symmetries in two-mode Dicke models,"As recently realized experimentally [Léonard et al., Nature 543, 87 (2017)], one can engineer models with continuous symmetries by coupling two cavity modes to trapped atoms, via a Raman pumping geometry. Considering specifically cases where internal states of the atoms couple to the cavity, we show an extended range of parameters for which continuous symmetry breaking can occur, and we classify the distinct steady states and time-dependent states that arise for different points in this extended parameter regime."
The strong Prikry property,"  I isolate a combinatorial property of a poset $\mathbb{P}$ that I call the strong Prikry property, which implies the existence of an ultrafilter on the complete Boolean algebra $\mathbb{B}$ of $\mathbb{P}$ such that one inclusion of the Boolean ultrapower version of the so-called \Bukovsky-Dehornoy phenomenon holds with respect to $\mathbb{B}$ and $U$. I show that in all cases that were previously studied, and for which it was shown that they come with a canonical iterated ultrapower construction whose limit can be described as a single Boolean ultrapower, the posets in question satisfy this property: Prikry forcing, Magidor forcing and generalized Prikry forcing. "
Accelerated Optimization in the PDE Framework: Formulations for the Manifold of Diffeomorphisms,"  We consider the problem of optimization of cost functionals on the infinite-dimensional manifold of diffeomorphisms. We present a new class of optimization methods, valid for any optimization problem setup on the space of diffeomorphisms by generalizing Nesterov accelerated optimization to the manifold of diffeomorphisms. While our framework is general for infinite dimensional manifolds, we specifically treat the case of diffeomorphisms, motivated by optical flow problems in computer vision. This is accomplished by building on a recent variational approach to a general class of accelerated optimization methods by Wibisono, Wilson and Jordan, which applies in finite dimensions. We generalize that approach to infinite dimensional manifolds. We derive the surprisingly simple continuum evolution equations, which are partial differential equations, for accelerated gradient descent, and relate it to simple mechanical principles from fluid mechanics. Our approach has natural connections to the optimal mass transport problem. This is because one can think of our approach as an evolution of an infinite number of particles endowed with mass (represented with a mass density) that moves in an energy landscape. The mass evolves with the optimization variable, and endows the particles with dynamics. This is different than the finite dimensional case where only a single particle moves and hence the dynamics does not depend on the mass. We derive the theory, compute the PDEs for accelerated optimization, and illustrate the behavior of these new accelerated optimization schemes. "
Learning Generalizable Robot Skills from Demonstrations in Cluttered Environments,"Learning from Demonstration (LfD) is a popular approach to endowing robots with skills without having to program them by hand. Typically, LfD relies on human demonstrations in clutter-free environments. This prevents the demonstrations from being affected by irrelevant objects, whose influence can obfuscate the true intention of the human or the constraints of the desired skill. However, it is unrealistic to assume that the robot's environment can always be restructured to remove clutter when capturing human demonstrations. To contend with this problem, we develop an importance weighted batch and incremental skill learning approach, building on a recent inference-based technique for skill representation and reproduction. Our approach reduces unwanted environmental influences on the learned skill, while still capturing the salient human behavior. We provide both batch and incremental versions of our approach and validate our algorithms on a 7-DOF JACO2 manipulator with reaching and placing skills."
Women are slightly more cooperative than men (in one-shot Prisoner's dilemma games played online),"Differences between men and women have intrigued generations of social scientists, who have found that the two sexes behave differently in settings requiring competition, risk taking, altruism, honesty, as well as many others. Yet, little is known about whether there are gender differences in cooperative behavior. Previous evidence is mixed and inconclusive. Here I shed light on this topic by analyzing the totality of studies that my research group has conducted since 2013. This is a dataset of 10,951 observations coming from 7,322 men and women living in the US, recruited through Amazon Mechanical Turk, and who passed four comprehension questions to make sure they understand the cooperation problem (a one-shot prisoner's dilemma). The analysis demonstrates that women are more cooperative than men. The effect size is small (about 4 percentage points, and this might explain why previous studies failed to detect it) but highly significant (p<.0001)."
Relaxation of Radiation-Driven Two-Level Systems Interacting with a Bose-Einstein Condensate Bath,"  We develop a microscopic theory for the relaxation dynamics of an optically pumped two-level system (TLS) coupled to a bath of weakly interacting Bose gas. Using Keldysh formalism and diagrammatic perturbation theory, expressions for the relaxation times of the TLS Rabi oscillations are derived when the boson bath is in the normal state and the Bose-Einstein condensate (BEC) state. We apply our general theory to consider an irradiated quantum dot coupled with a boson bath consisting of a two-dimensional dipolar exciton gas. When the bath is in the BEC regime, relaxation of the Rabi oscillations is due to both condensate and non-condensate fractions of the bath bosons for weak TLS-light coupling and dominantly due to the non-condensate fraction for strong TLS-light coupling. Our theory also shows that a phase transition of the bath from the normal to the BEC state strongly influences the relaxation rate of the TLS Rabi oscillations. The TLS relaxation rate is approximately independent of the pump field frequency and monotonically dependent on the field strength when the bath is in the low-temperature regime of the normal phase. Phase transition of the dipolar exciton gas leads to a non-monotonic dependence of the TLS relaxation rate on both the pump field frequency and field strength, providing a characteristic signature for the detection of BEC phase transition of the coupled dipolar exciton gas. "
On the approximation of convex bodies by ellipses with respect to the symmetric difference metric,"Given a centrally symmetric convex body $K \subset \mathbb{R}^d$ and a positive number $\lambda$, we consider, among all ellipsoids $E \subset \mathbb{R}^d$ of volume $\lambda$, those that best approximate $K$ with respect to the symmetric difference metric, or equivalently that maximize the volume of $E\cap K$: these are the maximal intersection (MI) ellipsoids introduced by Artstein-Avidan and Katzin. The question of uniqueness of MI ellipsoids (under the obviously necessary assumption that $\lambda$ is between the volumes of the John and the Loewner ellipsoids of $K$) is open in general. We provide a positive answer to this question in dimension $d=2$. Therefore we obtain a continuous $1$-parameter family of ellipses interpolating between the John and the Loewner ellipses of $K$. In order to prove uniqueness, we show that the area $I_K(E)$ of the intersection $K \cap E$ is a strictly quasiconcave function of the ellipse $E$, with respect to the natural affine structure on the set of ellipses of area $\lambda$. The proof relies on smoothening $K$, putting it in general position, and obtaining uniform estimates for certain derivatives of the function $I_K(.)$. Finally, we provide a characterization of maximal intersection positions, that is, the situation where the MI ellipse of $K$ is the unit disk, under the assumption that the two boundaries are transverse."
POLAMI: Polarimetric Monitoring of Active Galactic Nuclei at Millimetre Wavelengths. II. Widespread circular polarisation,"We analyse the circular polarisation data accumulated in the first 7 years of the POLAMI project introduced in an accompanying paper (Agudo et al.). In the 3mm wavelength band, we acquired more than 2600 observations, and all but one of our 37 sample sources were detected, most of them several times. For most sources, the observed distribution of the degree of circular polarisation is broader than that of unpolarised calibrators, indicating that weak (<0.5%) circular polarisation is present most of the time. Our detection rate and the maximum degree of polarisation found, 2.0%, are comparable to previous surveys, all made at much longer wavelengths. We argue that the process generating circular polarisation must not be strongly wavelength dependent, and we propose that the widespread presence of circular polarisation in our short wavelength sample dominated by blazars is mostly due to Faraday conversion of the linearly polarised synchrotron radiation in the helical magnetic field of the jet. Circular polarisation is variable, most notably on time scales comparable to or shorter than our median sampling interval <1 month. Longer time scales of about one year are occasionally detected, but severely limited by the weakness of the signal. At variance with some longer wavelength investigations we find that the sign of circular polarisation changes in most sources, while only 7 sources, including 3 already known, have a strong preference for one sign. The degrees of circular and linear polarisation do not show any systematic correlation. We do find however one particular event where the two polarisation degrees vary in synchronism during a time span of 0.9 years. The paper also describes a novel method for calibrating the sign of circular polarisation observations."
Single Classifier-based Passive System for Source Printer Classification using Local Texture Features,"An important aspect of examining printed documents for potential forgeries and copyright infringement is the identification of source printer as it can be helpful for ascertaining the leak and detecting forged documents. This paper proposes a system for classification of source printer from scanned images of printed documents using all the printed letters simultaneously. This system uses local texture patterns based features and a single classifier for classifying all the printed letters. Letters are extracted from scanned images using connected component analysis followed by morphological filtering without the need of using an OCR. Each letter is sub-divided into a flat region and an edge region, and local tetra patterns are estimated separately for these two regions. A strategically constructed pooling technique is used to extract the final feature vectors. The proposed method has been tested on both a publicly available dataset of 10 printers and a new dataset of 18 printers scanned at a resolution of 600 dpi as well as 300 dpi printed in four different fonts. The results indicate shape independence property in the proposed method as using a single classifier it outperforms existing handcrafted feature-based methods and needs much smaller number of training pages by using all the printed letters."
Heat spreader with parallel microchannel configurations employing nanofluids for near active cooling of MEMS,"  While parallel microchannel based cooling systems have been around for quite a period of time, employing the same and incorporating them for near active cooling of microelectronic devices is yet to be implemented and the implications of the same on thermal mitigation to be understood. The present article focusses on a specific design of the PMCS such that it can be implemented at ease on the heat spreader of a modern microprocessor to obtain near active cooling. Extensive experimental and numerical studies have been carried out to comprehend the same and three different flow configurations of PMCS have been adopted for the present investigations. Additional to focussing on the thermofluidics due to flow configuration, nanofluids have also been employed to achieve the desired essentials of mitigation of overshoot temperatures and improving uniformity of cooling. Two modelling methods, Discrete Phase Modelling and Effective Property Modelling have been employed for numerical study to model nanofluids as working fluid in micro flow paths and the DPM predictions have been observed to match accurately with experiments. To quantify the thermal performance of PMCS, an appropriate Figure of Merit has been proposed. From the FoM It has been perceived that the Z configuration employing nanofluid is the best suitable solutions for uniform thermal loads to achieve uniform cooling as well as reducing maximum temperature produced with in the device. The present results are very promising and viable approach for futuristic thermal mitigation of microprocessor systems. "
A review of asymptotic theory of estimating functions,"  Asymptotic statistical theory for estimating functions is reviewed in a generality suitable for stochastic processes. Conditions concerning existence of a consistent estimator, uniqueness, rate of convergence, and the asymptotic distribution are treated separately. Our conditions are not minimal, but can be verified for many interesting stochastic process models. Several examples illustrate the wide applicability of the theory and why the generality is needed. "
Multiview Deep Learning for Predicting Twitter Users' Location,"  The problem of predicting the location of users on large social networks like Twitter has emerged from real-life applications such as social unrest detection and online marketing. Twitter user geolocation is a difficult and active research topic with a vast literature. Most of the proposed methods follow either a content-based or a network-based approach. The former exploits user-generated content while the latter utilizes the connection or interaction between Twitter users. In this paper, we introduce a novel method combining the strength of both approaches. Concretely, we propose a multi-entry neural network architecture named MENET leveraging the advances in deep learning and multiview learning. The generalizability of MENET enables the integration of multiple data representations. In the context of Twitter user geolocation, we realize MENET with textual, network, and metadata features. Considering the natural distribution of Twitter users across the concerned geographical area, we subdivide the surface of the earth into multi-scale cells and train MENET with the labels of the cells. We show that our method outperforms the state of the art by a large margin on three benchmark datasets. "
"Designing the Optimal Bit: Balancing Energetic Cost, Speed and Reliability","  We consider the technologically relevant costs of operating a reliable bit that can be erased rapidly. We find that both erasing and reliability times are non-monotonic in the underlying friction, leading to a trade-off between erasing speed and bit reliability. Fast erasure is possible at the expense of low reliability at moderate friction, and high reliability comes at the expense of slow erasure in the underdamped and overdamped limits. Within a given class of bit parameters and control strategies, we define ""optimal"" designs of bits that meet the desired reliability and erasing time requirements with the lowest operational work cost. We find that optimal designs always saturate the bound on the erasing time requirement, but can exceed the required reliability time if critically damped. The non-trivial geometry of the reliability and erasing time-scales allows us to exclude large regions of parameter space as sub-optimal. We find that optimal designs are either critically damped or close to critical damping under the erasing procedure. "
Economic Factors of Vulnerability Trade and Exploitation,"  Cybercrime markets support the development and diffusion of new attack technologies, vulnerability exploits, and malware. Whereas the revenue streams of cyber attackers have been studied multiple times in the literature, no quantitative account currently exists on the economics of attack acquisition and deployment. Yet, this understanding is critical to characterize the production of (traded) exploits, the economy that drives it, and its effects on the overall attack scenario. In this paper we provide an empirical investigation of the economics of vulnerability exploitation, and the effects of market factors on likelihood of exploit. Our data is collected first-handedly from a prominent Russian cybercrime market where the trading of the most active attack tools reported by the security industry happens. Our findings reveal that exploits in the underground are priced similarly or above vulnerabilities in legitimate bug-hunting programs, and that the refresh cycle of exploits is slower than currently often assumed. On the other hand, cybercriminals are becoming faster at introducing selected vulnerabilities, and the market is in clear expansion both in terms of players, traded exploits, and exploit pricing. We then evaluate the effects of these market variables on likelihood of attack realization, and find strong evidence of the correlation between market activity and exploit deployment. We discuss implications on vulnerability metrics, economics, and exploit measurement. "
Complete classification of generalized crossing changes between GOF-knots,"  We show that the monodromy for a genus one, fibered knot can have at most two monodromy equivalence classes of once-unclean arcs. We use this to classify all monodromies of genus one, fibered knots that possess once-unclean arcs, all manifolds containing genus one fibered knots with generalized crossing changes resulting in another genus one fibered knot, and all generalized crossing changes between two genus one, fibered knots. "
Strain Mode of General Flow: Characterization and Implications for Flow Pattern Structures,"  Understanding the mixing capability of mixing devices based on their geometric shape is an important issue both for predicting mixing processes and for designing new mixers. The flow patterns in mixers are directly connected with the modes of the local strain rate, which is generally a combination of elongational flow and planar shear flow. We develop a measure to characterize the modes of the strain rate for general flow occurring in mixers. The spatial distribution of the volumetric strain rate (or non-planar strain rate) in connection with the flow pattern plays an essential role in understanding distributive mixing. With our measure, flows with different types of screw elements in a twin-screw extruder are numerically analyzed. The difference in flow pattern structure between conveying screws and kneading disks is successfully characterized by the distribution of the volumetric strain rate. The results suggest that the distribution of the strain rate mode offers an essential and convenient way for characterization of the relation between flow pattern structure and the mixer geometry. "
$\mathbb{Z}^2$-algebras as noncommutative blow-ups,"The goal of this note is to first prove that for a well behaved $\mathbb{Z}^2$-algebra $R$, the category $QGr(R) := Gr(R)/Tors(R)$ is equivalent to $QGr(R_\Delta)$ where $R_\Delta$ is a diagonal-like sub-$\mathbb{Z}$-algebra of $R$. Afterwards we use this result to prove that the $\mathbb{Z}^2$-algebras as introduced in [arXiv:1607.08383] are QGr-equivalent to a diagonal-like sub-$\mathbb{Z}$-algebra which is a simultaneous noncommutative blow-up of a quadratic and a cubic Sklyanin algebra. As such we link the noncommutative birational transformation and the associated $\mathbb{Z}^2$-algebras as appearing in the work of Van den Bergh and Presotto with the noncommutative blowups appearing in the work of Rogalski, Sierra and Stafford."
Deep Convolutional Neural Networks for Anomaly Event Classification on Distributed Systems,"The increasing popularity of server usage has brought a plenty of anomaly log events, which have threatened a vast collection of machines. Recognizing and categorizing the anomalous events thereby is a much salient work for our systems, especially the ones generate the massive amount of data and harness it for technology value creation and business development. To assist in focusing on the classification and the prediction of anomaly events, and gaining critical insights from system event records, we propose a novel log preprocessing method which is very effective to filter abundant information and retain critical characteristics. Additionally, a competitive approach for automated classification of anomalous events detected from the distributed system logs with the state-of-the-art deep (Convolutional Neural Network) architectures is proposed in this paper. We measure a series of deep CNN algorithms with varied hyper-parameter combinations by using standard evaluation metrics, the results of our study reveals the advantages and potential capabilities of the proposed deep CNN models for anomaly event classification tasks on real-world systems. The optimal classification precision of our approach is 98.14%, which surpasses the popular traditional machine learning methods."
Group-sparse block PCA and explained variance,"  The paper addresses the simultneous determination of goup-sparse loadings by block optimization, and the correlated problem of defining explained variance for a set of non orthogonal components. We give in both cases a comprehensive mathematical presentation of the problem, which leads to propose i) a new formulation/algorithm for group-sparse block PCA and ii) a framework for the definition of explained variance with the analysis of five definitions. The numerical results i) confirm the superiority of block optimization over deflation for the determination of group-sparse loadings, and the importance of group information when available, and ii) show that ranking of algorithms according to explained variance is essentially independant of the definition of explained variance. These results lead to propose a new optimal variance as the definition of choice for explained variance. "
Multi-Frequency Phase Synchronization,"  We propose a novel formulation for phase synchronization -- the statistical problem of jointly estimating alignment angles from noisy pairwise comparisons -- as a nonconvex optimization problem that enforces consistency among the pairwise comparisons in multiple frequency channels. Inspired by harmonic retrieval in signal processing, we develop a simple yet efficient two-stage algorithm that leverages the multi-frequency information. We demonstrate in theory and practice that the proposed algorithm significantly outperforms state-of-the-art phase synchronization algorithms, at a mild computational costs incurred by using the extra frequency channels. We also extend our algorithmic framework to general synchronization problems over compact Lie groups. "
A Bennequin-type inequality and combinatorial bounds,"  In this paper we provide a new Bennequin-type inequality for the Rasmussen- Beliakova-Wehrli invariant, featuring the numerical transverse braid invariants (the c-invariants) introduced by the author. From the Bennequin type-inequality, and a combinatorial bound on the value of the c-invariants, we deduce a new computable bound on the Rasmussen invariant. "
Thermal field theory of bosonic gases with finite-range effective interaction,"  We study a dilute and ultracold Bose gas of interacting atoms by using an effective field theory which takes account finite-range effects of the inter-atomic potential. Within the formalism of functional integration from the grand canonical partition function we derive beyond-mean-field analytical results which depend on both scattering length and effective range of the interaction. In particular, we calculate the equation of state of the bosonic system as a function of these interaction parameters both at zero and finite temperature including one-loop Gaussian fluctuation. In the case of zero-range effective interaction we explicitly show that, due to quantum fluctuations, the bosonic system is thermodynamically stable only for very small values of the gas parameter. We find that a positive effective range above a critical threshold is necessary to remove the thermodynamical instability of the uniform configuration. Remarkably, also for relatively large values of the gas parameter, our finite-range results are in quite good agreement with recent zero-temperature Monte Carlo calculations obtained with hard-sphere bosons. "
LtFi: Cross-technology Communication for RRM between LTE-U and IEEE 802.11,"Cross-technology communication (CTC) was proposed in recent literature as a way to exploit the opportunities of collaboration between heterogeneous wireless technologies. This paper presents LtFi, a system which enables to set-up a CTC between nodes of co-located LTE-U and WiFi networks. LtFi follows a two-step approach: using the air-interface LTE-U BSs are broadcasting connection and identification data to adjacent WiFi nodes, which is used to create a bi-directional control channel over the wired Internet. This way LtFi enables the development of advanced cross-technology interference and radio resource management schemes between heterogeneous WiFi and LTE-U networks. LtFi is of low complexity and fully compliant with LTE-U technology and works on WiFi side with COTS hardware. It was prototypically implemented and evaluated. Experimental results reveal that LtFi is able to reliably decoded the data transmitted over the LtFi air-interface in a crowded wireless environment at even very low LTE-U receive power levels of -92dBm. Moreover, results from system-level simulations show that LtFi is able to accurately estimate the set of interfering LTE-U BSs in a typical LTE-U multi-cell environment."
Multi-step Off-policy Learning Without Importance Sampling Ratios,"  To estimate the value functions of policies from exploratory data, most model-free off-policy algorithms rely on importance sampling, where the use of importance sampling ratios often leads to estimates with severe variance. It is thus desirable to learn off-policy without using the ratios. However, such an algorithm does not exist for multi-step learning with function approximation. In this paper, we introduce the first such algorithm based on temporal-difference (TD) learning updates. We show that an explicit use of importance sampling ratios can be eliminated by varying the amount of bootstrapping in TD updates in an action-dependent manner. Our new algorithm achieves stability using a two-timescale gradient-based TD update. A prior algorithm based on lookup table representation called Tree Backup can also be retrieved using action-dependent bootstrapping, becoming a special case of our algorithm. In two challenging off-policy tasks, we demonstrate that our algorithm is stable, effectively avoids the large variance issue, and can perform substantially better than its state-of-the-art counterpart. "
Atomic Ferris wheel beams,"  We study the generation of atom vortex beams in the case where an atomic wave-packet, moving in free space, is diffracted from a properly tailored light mask with a spiral transverse profile. We show how such a diffraction scheme could lead to the production of an atomic Ferris wheel beam. "
On the Limits of Learning Representations with Label-Based Supervision,"  Advances in neural network based classifiers have transformed automatic feature learning from a pipe dream of stronger AI to a routine and expected property of practical systems. Since the emergence of AlexNet every winning submission of the ImageNet challenge has employed end-to-end representation learning, and due to the utility of good representations for transfer learning, representation learning has become as an important and distinct task from supervised learning. At present, this distinction is inconsequential, as supervised methods are state-of-the-art in learning transferable representations. But recent work has shown that generative models can also be powerful agents of representation learning. Will the representations learned from these generative methods ever rival the quality of those from their supervised competitors? In this work, we argue in the affirmative, that from an information theoretic perspective, generative models have greater potential for representation learning. Based on several experimentally validated assumptions, we show that supervised learning is upper bounded in its capacity for representation learning in ways that certain generative models, such as Generative Adversarial Networks (GANs) are not. We hope that our analysis will provide a rigorous motivation for further exploration of generative representation learning. "