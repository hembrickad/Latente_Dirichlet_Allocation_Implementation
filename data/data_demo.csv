Closed-form Marginal Likelihood in Gamma-Poisson Matrix Factorization,"  We present novel understandings of the Gamma-Poisson (GaP) model, a probabilistic matrix factorization model for count data. We show that GaP can be rewritten free of the score/activation matrix. This gives us new insights about the estimation of the topic/dictionary matrix by maximum marginal likelihood estimation. In particular, this explains the robustness of this estimator to over-specified values of the factorization rank, especially its ability to automatically prune irrelevant dictionary columns, as empirically observed in previous work. The marginalization of the activation matrix leads in turn to a new Monte Carlo Expectation-Maximization algorithm with favorable properties. "
Laboratory mid-IR spectra of equilibrated and igneous meteorites. Searching for observables of planetesimal debris,"  Meteorites contain minerals from Solar System asteroids with different properties (like size, presence of water, core formation). We provide new mid-IR transmission spectra of powdered meteorites to obtain templates of how mid-IR spectra of asteroidal debris would look like. This is essential for interpreting mid-IR spectra of past and future space observatories, like the James Webb Space Telescope. We show that the transmission spectra of wet and dry chondrites, carbonaceous and ordinary chondrites and achondrite and chondrite meteorites are distinctly different in a way one can distinguish in astronomical mid-IR spectra. The two observables that spectroscopically separate the different meteorites groups (and thus the different types of parent bodies) are the pyroxene-olivine feature strength ratio and the peak shift of the olivine spectral features due to an increase in the iron concentration of the olivine. "
Case For Static AMSDU Aggregation in WLANs,"  Frame aggregation is a mechanism by which multiple frames are combined into a single transmission unit over the air. Frames aggregated at the AMSDU level use a common CRC check to enforce integrity. For longer aggregated AMSDU frames, the packet error rate increases significantly for the same bit error rate. Hence, multiple studies have proposed doing AMSDU aggregation adaptively based on the error rate. This study evaluates if there is a \emph{practical} advantage in doing adaptive AMSDU aggregation based on the link bit error rate. Evaluations on a model show that instead of implementing a complex adaptive AMSDU frame aggregation mechanism which impact queuing and other implementation aspects, it is easier to influence packet error rate with traditional mechanisms while keeping the AMSDU aggregation logic simple. "
The $Gaia$-ESO Survey: the inner disk intermediate-age open cluster NGC 6802,"Milky Way open clusters are very diverse in terms of age, chemical composition, and kinematic properties. Intermediate-age and old open clusters are less common, and it is even harder to find them inside the solar Galactocentric radius, due to the high mortality rate and strong extinction inside this region. NGC 6802 is one of the inner disk open clusters (IOCs) observed by the $Gaia$-ESO survey (GES). This cluster is an important target for calibrating the abundances derived in the survey due to the kinematic and chemical homogeneity of the members in open clusters. Using the measurements from $Gaia$-ESO internal data release 4 (iDR4), we identify 95 main-sequence dwarfs as cluster members from the GIRAFFE target list, and eight giants as cluster members from the UVES target list. The dwarf cluster members have a median radial velocity of $13.6\pm1.9$ km s$^{-1}$, while the giant cluster members have a median radial velocity of $12.0\pm0.9$ km s$^{-1}$ and a median [Fe/H] of $0.10\pm0.02$ dex. The color-magnitude diagram of these cluster members suggests an age of $0.9\pm0.1$ Gyr, with $(m-M)_0=11.4$ and $E(B-V)=0.86$. We perform the first detailed chemical abundance analysis of NGC 6802, including 27 elemental species. To gain a more general picture about IOCs, the measurements of NGC 6802 are compared with those of other IOCs previously studied by GES, that is, NGC 4815, Trumpler 20, NGC 6705, and Berkeley 81. NGC 6802 shows similar C, N, Na, and Al abundances as other IOCs. These elements are compared with nucleosynthetic models as a function of cluster turn-off mass. The $\alpha$, iron-peak, and neutron-capture elements are also explored in a self-consistent way."
Witness-Functions versus Interpretation-Functions for Secrecy in Cryptographic Protocols: What to Choose?,"  Proving that a cryptographic protocol is correct for secrecy is a hard task. One of the strongest strategies to reach this goal is to show that it is increasing, which means that the security level of every single atomic message exchanged in the protocol, safely evaluated, never deceases. Recently, two families of functions have been proposed to measure the security level of atomic messages. The first one is the family of interpretation-functions. The second is the family of witness-functions. In this paper, we show that the witness-functions are more efficient than interpretation-functions. We give a detailed analysis of an ad-hoc protocol on which the witness-functions succeed in proving its correctness for secrecy while the interpretation-functions fail to do so. "
Pairwise Difference Estimation of High Dimensional Partially Linear Model,"This paper proposes a regularized pairwise difference approach for estimating the linear component coefficient in a partially linear model, with consistency and exact rates of convergence obtained in high dimensions under mild scaling requirements. Our analysis reveals interesting features such as (i) the bandwidth parameter automatically adapts to the model and is actually tuning-insensitive; and (ii) the procedure could even maintain fast rate of convergence for $\alpha$-Hölder class of $\alpha\leq1/2$. Simulation studies show the advantage of the proposed method, and application of our approach to a brain imaging data reveals some biological patterns which fail to be recovered using competing methods."
Dissecting the multivariate extremal index and tail dependence,"  A central issue in the theory of extreme values focuses on suitable conditions such that the well-known results for the limiting distributions of the maximum of i.i.d. sequences can be applied to stationary ones. In this context, the extremal index appears as a key parameter to capture the effect of temporal dependence on the limiting distribution of the maxima. The multivariate extremal index corresponds to a generalization of this concept to a multivariate context and affects the tail dependence structure within the marginal sequences and between them. As it is a function, the inference becomes more difficult, and it is therefore important to obtain characterizations, namely bounds based on the marginal dependence that are easier to estimate. In this work we present two decompositions that emphasize different types of information contained in the multivariate extremal index, an upper limit better than those found in the literature and we analyze its role in dependence on the limiting model of the componentwise maxima of a stationary sequence. We will illustrate the results with examples of recognized interest in applications. "
"Big Universe, Big Data: Machine Learning and Image Analysis for Astronomy","  Astrophysics and cosmology are rich with data. The advent of wide-area digital cameras on large aperture telescopes has led to ever more ambitious surveys of the sky. Data volumes of entire surveys a decade ago can now be acquired in a single night and real-time analysis is often desired. Thus, modern astronomy requires big data know-how, in particular it demands highly efficient machine learning and image analysis algorithms. But scalability is not the only challenge: Astronomy applications touch several current machine learning research questions, such as learning from biased data and dealing with label and measurement noise. We argue that this makes astronomy a great domain for computer science research, as it pushes the boundaries of data analysis. In the following, we will present this exciting application area for data scientists. We will focus on exemplary results, discuss main challenges, and highlight some recent methodological advancements in machine learning and image analysis triggered by astronomical applications. "
Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog,"  A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, all learned without any human supervision! In this paper, using a Task and Tell reference game between two agents as a testbed, we present a sequence of 'negative' results culminating in a 'positive' one -- showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge 'naturally', despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate. "
Properties and Origin of Galaxy Velocity Bias in the Illustris Simulation,"We use the hydrodynamical galaxy formation simulations from the Illustris suite to study the origin and properties of galaxy velocity bias, i.e., the difference between the velocity distributions of galaxies and dark matter inside halos. We find that galaxy velocity bias is a decreasing function of the ratio of galaxy stellar mass to host halo mass. In general, central galaxies are not at rest with respect to dark matter halos or the core of halos, with a velocity dispersion above 0.04 times that of the dark matter. The central galaxy velocity bias is found to be mostly caused by the close interactions between the central and satellite galaxies. For satellite galaxies, the velocity bias is related to their dynamical and tidal evolution history after being accreted onto the host halos. It depends on the time after the accretion and their distances from the halo centers, with massive satellites generally moving more slowly than the dark matter. The results are in broad agreements with those inferred from modeling small-scale redshift-space galaxy clustering data, and the study can help improve models of redshift-space galaxy clustering."
Computer Modeling of Halogen Bonds and Other $σ$-Hole Interactions,"In the field of noncovalent interactions a new paradigm has recently become popular. It stems from the analysis of molecular electrostatic potentials and introduces a label, which has recently attracted enormous attention. The label is {\sigma}-hole, and it was first used in connection with halogens. It initiated a renaissance of interest in halogenated compounds, and later on, when found also on other groups of atoms (chalcogens, pnicogens, tetrels and aerogens), it resulted in a new direction of research of intermolecular interactions. In this review, we summarize advances from about the last 10 years in understanding those interactions related to {\sigma}-hole. We pay particular attention to theoretical and computational techniques, which play a crucial role in the field."
Towards Universal End-to-End Affect Recognition from Multilingual Speech by ConvNets,"We propose an end-to-end affect recognition approach using a Convolutional Neural Network (CNN) that handles multiple languages, with applications to emotion and personality recognition from speech. We lay the foundation of a universal model that is trained on multiple languages at once. As affect is shared across all languages, we are able to leverage shared information between languages and improve the overall performance for each one. We obtained an average improvement of 12.8% on emotion and 10.1% on personality when compared with the same model trained on each language only. It is end-to-end because we directly take narrow-band raw waveforms as input. This allows us to accept as input audio recorded from any source and to avoid the overhead and information loss of feature extraction. It outperforms a similar CNN using spectrograms as input by 12.8% for emotion and 6.3% for personality, based on F-scores. Analysis of the network parameters and layers activation shows that the network learns and extracts significant features in the first layer, in particular pitch, energy and contour variations. Subsequent convolutional layers instead capture language-specific representations through the analysis of supra-segmental features. Our model represents an important step for the development of a fully universal affect recognizer, able to recognize additional descriptors, such as stress, and for the future implementation into affective interactive systems."
A Variational Characterization of Fluid Sloshing with Surface Tension,"  We consider the sloshing problem for an incompressible, inviscid, irrotational fluid in an open container, including effects due to surface tension on the free surface. We restrict ourselves to a constant contact angle and seek time-harmonic solutions of the linearized problem, which describes the time-evolution of the fluid due to a small initial disturbance of the surface at rest. As opposed to the zero surface tension case, where the problem reduces to a partial differential equation for the velocity potential, we obtain a coupled system for the velocity potential and the free surface displacement. We derive a new variational formulation of the coupled problem and establish the existence of solutions using the direct method from the calculus of variations. We prove a domain monotonicity result for the fundamental sloshing eigenvalue. In the limit of zero surface tension, we recover the variational formulation of the mixed Steklov-Neumann eigenvalue problem and give the first-order perturbation formula for a simple eigenvalue. "
diagnoseIT: Expertengestützte automatische Diagnose von Performance-Probleme in Enterprise-Anwendungen (Abschlussbericht),  This is the final report of the collaborative research project diagnoseIT on expert-guided automatic diagnosis of performance problems in enterprise applications. 
Statistical Inference in Political Networks Research,"  Researchers interested in statistically modeling network data have a well-established and quickly growing set of approaches from which to choose. Several of these methods have been regularly applied in research on political networks, while others have yet to permeate the field. Here, we review the most prominent methods of inferential network analysis---both for cross-sectionally and longitudinally observed networks including (temporal) exponential random graph models, latent space models, the quadratic assignment procedure, and stochastic actor oriented models. For each method, we summarize its analytic form, identify prominent published applications in political science and discuss computational considerations. We conclude with a set of guidelines for selecting a method for a given application. "
Controller-jammer game models of Denial of Service in control systems operating over packet-dropping links,"  The paper introduces a class of zero-sum games between the adversary and controller as a scenario for a `denial of service' in a networked control system. The communication link is modeled as a set of transmission regimes controlled by a strategic jammer whose intention is to wage an attack on the plant by choosing a most damaging regime-switching strategy. We demonstrate that even in the one-step case, the introduced games admit a saddle-point equilibrium, at which the jammer's optimal policy is to randomize in a region of the plant's state space, thus requiring the controller to undertake a nontrivial response which is different from what one would expect in a standard stochastic control problem over a packet dropping link. The paper derives conditions for the introduced games to have such a saddle-point equilibrium. Furthermore, we show that in more general multi-stage games, these conditions provide `greedy' jamming strategies for the adversary. "
Conservation of spin supercurrents in superconductors,"  We demonstrate that spin supercurrents are conserved upon transmission through a conventional superconductor, even in the presence of spin-dependent scattering by impurities with magnetic moments or spin-orbit coupling. This is fundamentally different from conventional spin currents, which decay in the presence of such scattering, and this has important implications for the usage of superconducting materials in spintronic hybrid structures. "
Driver Identification Using Automobile Sensor Data from a Single Turn,"As automotive electronics continue to advance, cars are becoming more and more reliant on sensors to perform everyday driving operations. These sensors are omnipresent and help the car navigate, reduce accidents, and provide comfortable rides. However, they can also be used to learn about the drivers themselves. In this paper, we propose a method to predict, from sensor data collected at a single turn, the identity of a driver out of a given set of individuals. We cast the problem in terms of time series classification, where our dataset contains sensor readings at one turn, repeated several times by multiple drivers. We build a classifier to find unique patterns in each individual's driving style, which are visible in the data even on such a short road segment. To test our approach, we analyze a new dataset collected by AUDI AG and Audi Electronics Venture, where a fleet of test vehicles was equipped with automotive data loggers storing all sensor readings on real roads. We show that turns are particularly well-suited for detecting variations across drivers, especially when compared to straightaways. We then focus on the 12 most frequently made turns in the dataset, which include rural, urban, highway on-ramps, and more, obtaining accurate identification results and learning useful insights about driver behavior in a variety of settings."
Same-different problems strain convolutional neural networks,"  The robust and efficient recognition of visual relations in images is a hallmark of biological vision. We argue that, despite recent progress in visual recognition, modern machine vision algorithms are severely limited in their ability to learn visual relations. Through controlled experiments, we demonstrate that visual-relation problems strain convolutional neural networks (CNNs). The networks eventually break altogether when rote memorization becomes impossible, as when intra-class variability exceeds network capacity. Motivated by the comparable success of biological vision, we argue that feedback mechanisms including attention and perceptual grouping may be the key computational components underlying abstract visual reasoning.\ "
Hyper-dimensional computing for a visual question-answering system that is trainable end-to-end,"  In this work we propose a system for visual question answering. Our architecture is composed of two parts, the first part creates the logical knowledge base given the image. The second part evaluates questions against the knowledge base. Differently from previous work, the knowledge base is represented using hyper-dimensional computing. This choice has the advantage that all the operations in the system, namely creating the knowledge base and evaluating the questions against it, are differentiable, thereby making the system easily trainable in an end-to-end fashion. "
The Inner Structure of Time-Dependent Signals,"This paper shows how a time series of measurements of an evolving system can be processed to create an inner time series that is unaffected by any instantaneous invertible, possibly nonlinear transformation of the measurements. An inner time series contains information that does not depend on the nature of the sensors, which the observer chose to monitor the system. Instead, it encodes information that is intrinsic to the evolution of the observed system. Because of its sensor-independence, an inner time series may produce fewer false negatives when it is used to detect events in the presence of sensor drift. Furthermore, if the observed physical system is comprised of non-interacting subsystems, its inner time series is separable; i.e., it consists of a collection of time series, each one being the inner time series of an isolated subsystem. Because of this property, an inner time series can be used to detect a specific behavior of one of the independent subsystems without using blind source separation to disentangle that subsystem from the others. The method is illustrated by applying it to: 1) an analytic example; 2) the audio waveform of one speaker; 3) video images from a moving camera; 4) mixtures of audio waveforms of two speakers."
Classical properties of the leading eigenstates of quantum dissipative systems,"  By analyzing a paradigmatic example of the theory of dissipative systems -- the classical and quantum dissipative standard map -- we are able to explain the main features of the decay to the quantum equilibrium state. The classical isoperiodic stable structures typically present in the parameter space of these kind of systems play a fundamental role. In fact, we have found that the period of stable structures that are near in this space determines the phase of the leading eigenstates of the corresponding quantum superoperator. Moreover, the eigenvectors show a strong localization on the corresponding periodic orbits (limit cycles). We show that this sort of scarring phenomenon (an established property of Hamiltonian and projectively open systems) is present in the dissipative case and it is of extreme simplicity. "
Towards Binary-Valued Gates for Robust LSTM Training,"Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. It aims to use gates to control information flow (e.g., whether to skip some information or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal. In this paper, we propose a new way for LSTM training, which pushes the output values of the gates towards 0 or 1. By doing so, we can better control the information flow: the gates are mostly open or closed, instead of in a middle state, which makes the results more interpretable. Empirical studies show that (1) Although it seems that we restrict the model capacity, there is no performance drop: we achieve better or comparable performances due to its better generalization ability; (2) The outputs of gates are not sensitive to their inputs: we can easily compress the LSTM unit in multiple ways, e.g., low-rank approximation and low-precision approximation. The compressed models are even better than the baseline models without compression."
GUB Covers and Power-Indexed formulations for Wireless Network Design,"We propose a pure 0-1 formulation for the wireless network design problem, i.e. the problem of configuring a set of transmitters to provide service coverage to a set of receivers. In contrast with classical mixed integer formulations, where power emissions are represented by continuous variables, we consider only a finite set of powers values. This has two major advantages: it better fits the usual practice and eliminates the sources of numerical problems which heavily affect continuous models. A crucial ingredient of our approach is an effective basic formulation for the single knapsack problem representing the coverage condition of a receiver. This formulation is based on the GUB cover inequalities introduced by Wolsey (1990) and its core is an extension of the exact formulation of the GUB knapsack polytope with two GUB constraints. This special case corresponds to the very common practical situation where only one major interferer is present. We assess the effectiveness of our formulation by comprehensive computational results over realistic instances of two typical technologies, namely WiMAX and DVB-T."
Orbital contributions to the electron g-factor in semiconductor nanowires,"Recent experiments on Majorana fermions in semiconductor nanowires [Albrecht et al., Nat. 531, 206 (2016)] revealed a surprisingly large electronic Landé g-factor, several times larger than the bulk value - contrary to the expectation that confinement reduces the g-factor. Here we assess the role of orbital contributions to the electron g-factor in nanowires and quantum dots. We show that an LS coupling in higher subbands leads to an enhancement of the g-factor of an order of magnitude or more for small effective mass semiconductors. We validate our theoretical finding with simulations of InAs and InSb, showing that the effect persists even if cylindrical symmetry is broken. A huge anisotropy of the enhanced g-factors under magnetic field rotation allows for a straightforward experimental test of this theory."
Comprehensive routing strategy on multilayer networks,"  Designing an efficient routing strategy is of great importance to alleviate traffic congestion in multilayer networks. In this work, we design an effective routing strategy for multilayer networks by comprehensively considering the roles of nodes' local structures in micro-level, as well as the macro-level differences in transmission speeds between different layers. Both numerical and analytical results indicate that our proposed routing strategy can reasonably redistribute the traffic load of the low speed layer to the high speed layer, and thus the traffic capacity of multilayer networks are significantly enhanced compared with the monolayer low speed networks. There is an optimal combination of macro- and micro-level control parameters at which can remarkably alleviate the congestion and thus maximize the traffic capacity for a given multilayer network. Moreover, we find that increasing the size and the average degree of the high speed layer can enhance the traffic capacity of multilayer networks more effectively. We finally verify that real-world network topology does not invalidate the results. The theoretical predictions agree well with the numerical simulations. "
Long short-term memory and learning-to-learn in networks of spiking neurons,"Recurrent networks of spiking neurons (RSNNs) underlie the astounding computing and learning capabilities of the brain. But computing and learning capabilities of RSNN models have remained poor, at least in comparison with artificial neural networks (ANNs). We address two possible reasons for that. One is that RSNNs in the brain are not randomly connected or designed according to simple rules, and they do not start learning as a tabula rasa network. Rather, RSNNs in the brain were optimized for their tasks through evolution, development, and prior experience. Details of these optimization processes are largely unknown. But their functional contribution can be approximated through powerful optimization methods, such as backpropagation through time (BPTT). A second major mismatch between RSNNs in the brain and models is that the latter only show a small fraction of the dynamics of neurons and synapses in the brain. We include neurons in our RSNN model that reproduce one prominent dynamical process of biological neurons that takes place at the behaviourally relevant time scale of seconds: neuronal adaptation. We denote these networks as LSNNs because of their Long short-term memory. The inclusion of adapting neurons drastically increases the computing and learning capability of RSNNs if they are trained and configured by deep learning (BPTT combined with a rewiring algorithm that optimizes the network architecture). In fact, the computational performance of these RSNNs approaches for the first time that of LSTM networks. In addition RSNNs with adapting neurons can acquire abstract knowledge from prior learning in a Learning-to-Learn (L2L) scheme, and transfer that knowledge in order to learn new but related tasks from very few examples. We demonstrate this for supervised learning and reinforcement learning."
Detecting Molecular Rotational Dynamics Complementing the Low-Frequency Terahertz Vibrations in a Zirconium-Based Metal-Organic Framework,"We show clear experimental evidence of co-operative terahertz (THz) dynamics observed below 3 THz (~100 cm-1), for a low-symmetry Zr-based metal-organic framework (MOF) structure, termed MIL-140A [ZrO(O2C-C6H4-CO2)]. Utilizing a combination of high-resolution inelastic neutron scattering and synchrotron radiation far-infrared spectroscopy, we measured low-energy vibrations originating from the hindered rotations of organic linkers, whose energy barriers and detailed dynamics have been elucidated via ab initio density functional theory (DFT) calculations. For completeness, we obtained Raman spectra and characterized the alterations to the complex pore architecture caused by the THz rotations. We discovered an array of soft modes with trampoline-like motions, which could potentially be the source of anomalous mechanical phenomena, such as negative linear compressibility and negative thermal expansion. Our results also demonstrate coordinated shear dynamics (~2.5 THz), a mechanism which we have shown to destabilize MOF crystals, in the exact crystallographic direction of the minimum shear modulus (Gmin)."
A southern-sky total intensity source catalogue at 2.3 GHz from S-band Polarisation All-Sky Survey data,"The S-band Polarisation All-Sky Survey (S-PASS) has observed the entire southern sky using the 64-metre Parkes radio telescope at 2.3GHz with an effective bandwidth of 184MHz. The surveyed sky area covers all declinations $\delta\leq 0^\circ$. To analyse compact sources the survey data have been re-processed to produce a set of 107 Stokes $I$ maps with 10.75arcmin resolution and the large scale emission contribution filtered out. In this paper we use these Stokes $I$ images to create a total intensity southern-sky extragalactic source catalogue at 2.3GHz. The source catalogue contains 23,389 sources and covers a sky area of 16,600deg$^2$, excluding the Galactic plane for latitudes $|b|<10^\circ$. Approximately 8% of catalogued sources are resolved. S-PASS source positions are typically accurate to within 35arcsec. At a flux density of 225mJy the S-PASS source catalogue is more than 95% complete, and $\sim$94% of S-PASS sources brighter than 500mJy beam$^{-1}$ have a counterpart at lower frequencies."
Mechanomyography based closed-loop Functional Electrical Stimulation cycling system,"Functional Electrical Stimulation (FES) systems are successful in restoring motor function and supporting paralyzed users. Commercially available FES products are open loop, meaning that the system is unable to adapt to changing conditions with the user and their muscles which results in muscle fatigue and poor stimulation protocols. This is because it is difficult to close the loop between stimulation and monitoring of muscle contraction using adaptive stimulation. FES causes electrical artefacts which make it challenging to monitor muscle contractions with traditional methods such as electromyography (EMG). We look to overcome this limitation by combining FES with novel mechanomyographic (MMG) sensors to be able to monitor muscle activity during stimulation in real time. To provide a meaningful task we built an FES cycling rig with a software interface that enabled us to perform adaptive recording and stimulation, and then combine this with sensors to record forces applied to the pedals using force sensitive resistors (FSRs), crank angle position using a magnetic incremental encoder and inputs from the user using switches and a potentiometer. We illustrated this with a closed-loop stimulation algorithm that used the inputs from the sensors to control the output of a programmable RehaStim 1 FES stimulator (Hasomed) in real-time. This recumbent bicycle rig was used as a testing platform for FES cycling. The algorithm was designed to respond to a change in requested speed (RPM) from the user and change the stimulation power (% of maximum current mA) until this speed was achieved and then maintain it."
QT2S: A System for Monitoring Road Traffic via Fine Grounding of Tweets,"Social media platforms provide continuous access to user generated content that enables real-time monitoring of user behavior and of events. The geographical dimension of such user behavior and events has recently caught a lot of attention in several domains: mobility, humanitarian, or infrastructural. While resolving the location of a user can be straightforward, depending on the affordances of their device and/or of the application they are using, in most cases, locating a user demands a larger effort, such as exploiting textual features. On Twitter for instance, only 2% of all tweets are geo-referenced. In this paper, we present a system for zoomed-in grounding (below city level) for short messages (e.g., tweets). The system combines different natural language processing and machine learning techniques to increase the number of geo-grounded tweets, which is essential to many applications such as disaster response and real-time traffic monitoring."
A Benchmark for Dose Finding Studies with Continuous Outcomes,"An important tool to evaluate the performance of any design is an optimal benchmark proposed by O'Quigley and others (2002, Biostatistics 3(1), 51-56) that provides an upper bound on the performance of a design under a given scenario. The original benchmark can be applied to dose finding studies with a binary endpoint only. However, there is a growing interest in dose finding studies involving continuous outcomes, but no benchmark for such studies has been developed. We show that the original benchmark and its extension by Cheung (2014, Biometrics 70(2), 389-397), when looked at from a different perspective, can be generalised to various settings with several discrete and continuous outcomes. We illustrate and compare the benchmark performance in the setting of a Phase I clinical trial with continuous toxicity endpoint and in the setting of a Phase I/II clinical trial with continuous efficacy outcome. We show that the proposed benchmark provides an accurate upper bound for model-based dose finding methods and serves as a powerful tool for evaluating designs."
Wavefront retrieval through random pupil plane phase probes: Gerchberg-Saxton approach,"A pupil plane wavefront reconstruction procedure is proposed based on analysis of a sequence of focal plane images corresponding to a sequence of random pupil plane phase probes. The developed method provides the unique nontrivial solution of wavefront retrieval problem and shows global convergence to this solution demonstrated using a Gerchberg-Saxton implementation. The method is general and can be used in any optical system that includes deformable mirrors for active/adaptive wavefront correction. The presented numerical simulation and lab experimental results show low noise sensitivity, high reliability and robustness of the proposed approach for high quality optical wavefront restoration. Laboratory experiments have shown $\lambda$/14 rms accuracy in retrieval of a poked DM actuator fiducial pattern with spatial resolution of 20-30$~\mu$m that is comparable with accuracy of direct high-resolution interferometric measurements."
Uplink Non-Orthogonal Multiple Access for 5G Wireless Networks,"  Orthogonal Frequency Division Multiple Access (OFDMA) as well as other orthogonal multiple access techniques fail to achieve the system capacity limit in the uplink due to the exclusivity in resource allocation. This issue is more prominent when fairness among the users is considered in the system. Current Non-Orthogonal Multiple Access (NOMA) techniques introduce redundancy by coding/spreading to facilitate the users' signals separation at the receiver, which degrade the system spectral efficiency. Hence, in order to achieve higher capacity, more efficient NOMA schemes need to be developed. In this paper, we propose a NOMA scheme for uplink that removes the resource allocation exclusivity and allows more than one user to share the same subcarrier without any coding/spreading redundancy. Joint processing is implemented at the receiver to detect the users' signals. However, to control the receiver complexity, an upper limit on the number of users per subcarrier needs to be imposed. In addition, a novel subcarrier and power allocation algorithm is proposed for the new NOMA scheme that maximizes the users' sum-rate. The link-level performance evaluation has shown that the proposed scheme achieves bit error rate close to the single-user case. Numerical results show that the proposed NOMA scheme can significantly improve the system performance in terms of spectral efficiency and fairness comparing to OFDMA. "
Epsilon-approximations and epsilon-nets,  The use of random samples to approximate properties of geometric configurations has been an influential idea for both combinatorial and algorithmic purposes. This chapter considers two related notions---$\epsilon$-approximations and $\epsilon$-nets---that capture the most important quantitative properties that one would expect from a random sample with respect to an underlying geometric configuration. 
A continuous framework for fairness,"  Increasingly, discrimination by algorithms is perceived as a societal and legal problem. As a response, a number of criteria for implementing algorithmic fairness in machine learning have been developed in the literature. This paper proposes the Continuous Fairness Algorithm (CFA$\theta$) which enables a continuous interpolation between different fairness definitions. More specifically, we make three main contributions to the existing literature. First, our approach allows the decision maker to continuously vary between concepts of individual and group fairness. As a consequence, the algorithm enables the decision maker to adopt intermediate ""worldviews"" on the degree of discrimination encoded in algorithmic processes, adding nuance to the extreme cases of ""we're all equal"" (WAE) and ""what you see is what you get"" (WYSIWYG) proposed so far in the literature. Second, we use optimal transport theory, and specifically the concept of the barycenter, to maximize decision maker utility under the chosen fairness constraints. Third, the algorithm is able to handle cases of intersectionality, i.e., of multi-dimensional discrimination of certain groups on grounds of several criteria. We discuss three main examples (college admissions; credit application; insurance contracts) and map out the policy implications of our approach. The explicit formalization of the trade-off between individual and group fairness allows this post-processing approach to be tailored to different situational contexts in which one or the other fairness criterion may take precedence. "
Counting Subwords Occurrences in Base-b Expansions,"We count the number of distinct (scattered) subwords occurring in the base-b expansion of the non-negative integers. More precisely, we consider the sequence $(S_b(n))_{n\ge 0}$ counting the number of positive entries on each row of a generalization of the Pascal triangle to binomial coefficients of base-$b$ expansions. By using a convenient tree structure, we provide recurrence relations for $(S_b(n))_{n\ge 0}$ leading to the $b$-regularity of the latter sequence. Then we deduce the asymptotics of the summatory function of the sequence $(S_b(n))_{n\ge 0}$."
Tensor Renormalization Group with Randomized Singular Value Decomposition,"  An algorithm of the tensor renormalization group is proposed based on a randomized algorithm for singular value decomposition. Our algorithm is applicable to a broad range of two-dimensional classical models. In the case of a square lattice, its computational complexity and memory usage are proportional to the fifth and the third power of the bond dimension, respectively, whereas those of the conventional implementation are of the sixth and the fourth power. The oversampling parameter larger than the bond dimension is sufficient to reproduce the same result as full singular value decomposition even at the critical point of the two-dimensional Ising model. "
Cohomology of symplectic groups and Meyer's signature theorem,"Meyer showed that the signature of a closed oriented surface bundle over a surface is a multiple of $4$, and can be computed using an element of $H^2(\mathsf{Sp}(2g, \mathbb{Z}),\mathbb{Z})$. Denoting by $1 \to \mathbb{Z} \to \widetilde{\mathsf{Sp}(2g,\mathbb{Z})} \to \mathsf{Sp}(2g,\mathbb{Z}) \to 1$ the pullback of the universal cover of $\mathsf{ Sp}(2g,\mathbb{R})$, Deligne proved that every finite index subgroup of $\widetilde{\mathsf {Sp}(2g, \mathbb{Z})}$ contains $2\mathbb{Z}$. As a consequence, a class in the second cohomology of any finite quotient of $\mathsf{Sp}(2g, \mathbb{Z})$ can at most enable us to compute the signature of a surface bundle modulo $8$. We show that this is in fact possible and investigate the smallest quotient of $\mathsf{Sp}(2g, \mathbb{Z})$ that contains this information. This quotient $\mathfrak{H}$ is a non-split extension of $\mathsf {Sp}(2g,2)$ by an elementary abelian group of order $2^{2g+1}$. There is a central extension $1\to \mathbb{Z}/2\to\tilde{\mathfrak{H}}\to\mathfrak{H}\to 1$, and $\tilde{\mathfrak{H}}$ appears as a quotient of the metaplectic double cover $\mathsf{Mp}(2g,\mathbb{Z})=\widetilde{\mathsf{Sp}(2g,\mathbb{Z})}/2\mathbb{Z}$. It is an extension of $\mathsf{Sp}(2g,2)$ by an almost extraspecial group of order $2^{2g+2}$, and has a faithful irreducible complex representation of dimension $2^g$. Provided $g\ge 4$, $\widetilde{\mathfrak{H}}$ is the universal central extension of $\mathfrak{H}$. Putting all this together, we provide a recipe for computing the signature modulo $8$, and indicate some consequences."
The Newman--Shapiro problem,"We give a negative answer to the Newman--Shapiro problem on weighted approximation for entire functions formulated in 1966 and motivated by the theory of operators on the Fock space. There exists a function in the Fock space such that its exponential multiples do not approximate some entire multiples in the space. Furthermore, we establish several positive results under different restrictions on the function in question."
randUTV: A blocked randomized algorithm for computing a rank-revealing UTV factorization,"  This manuscript describes the randomized algorithm randUTV for computing a so called UTV factorization efficiently. Given a matrix $A$, the algorithm computes a factorization $A = UTV^{*}$, where $U$ and $V$ have orthonormal columns, and $T$ is triangular (either upper or lower, whichever is preferred). The algorithm randUTV is developed primarily to be a fast and easily parallelized alternative to algorithms for computing the Singular Value Decomposition (SVD). randUTV provides accuracy very close to that of the SVD for problems such as low-rank approximation, solving ill-conditioned linear systems, determining bases for various subspaces associated with the matrix, etc. Moreover, randUTV produces highly accurate approximations to the singular values of $A$. Unlike the SVD, the randomized algorithm proposed builds a UTV factorization in an incremental, single-stage, and non-iterative way, making it possible to halt the factorization process once a specified tolerance has been met. Numerical experiments comparing the accuracy and speed of randUTV to the SVD are presented. These experiments demonstrate that in comparison to column pivoted QR, which is another factorization that is often used as a relatively economic alternative to the SVD, randUTV compares favorably in terms of speed while providing far higher accuracy. "
"Continuity of nonlinear eigenvalues in $CD(K,\infty)$ spaces with respect to measured Gromov-Hausdorff convergence","  In this note we prove in the nonlinear setting of $CD(K,\infty)$ spaces the stability of the Krasnoselskii spectrum of the Laplace operator $-\Delta$ under measured Gromov-Hausdorff convergence, under an additional compactness assumption satisfied, for instance, by sequences of $CD^*(K,N)$ metric measure spaces with uniformly bounded diameter. Additionally, we show that every element $\lambda$ in the Krasnoselskii spectrum is indeed an eigenvalue, namely there exists a nontrivial $u$ satisfying the eigenvalue equation $- \Delta u = \lambda u$. "
Modeling WiFi Traffic for White Space Prediction in Wireless Sensor Networks,"Cross Technology Interference (CTI) is a prevalent phenomenon in the 2.4 GHz unlicensed spectrum causing packet losses and increased channel contention. In particular, WiFi interference is a severe problem for low-power wireless networks as its presence causes a significant degradation of the overall performance. In this paper, we propose a proactive approach based on WiFi interference modeling for accurately predicting transmission opportunities for low-power wireless networks. We leverage statistical analysis of real-world WiFi traces to learn aggregated traffic characteristics in terms of Inter-Arrival Time (IAT) that, once captured into a specific 2nd order Markov Modulated Poisson Process (MMPP(2)) model, enable accurate estimation of interference. We further use a hidden Markov model (HMM) for channel occupancy prediction. We evaluated the performance of i) the MMPP(2) traffic model w.r.t. real-world traces and an existing Pareto model for accurately characterizing the WiFi traffic and, ii) compared the HMM based white space prediction to random channel access. We report encouraging results for using interference modeling for white space prediction."
Dyson models under renormalization and in weak fields,"We consider one-dimensional long-range spin models (usually called Dyson models), consisting of Ising ferromagnets with slowly decaying long-range pair potentials of the form $\frac{1}{|i-j|^{\alpha}}$ mainly focusing on the range of slow decays $1 < \alpha \leq 2$. We describe two recent results, one about renormalization and one about the effect of external fields at low temperature. The first result states that a decimated long-range Gibbs measure in one dimension becomes non-Gibbsian, in the same vein as comparable results in higher dimensions for short-range models. The second result addresses the behaviour of such models under inhomogeneous fields, in particular external fields which decay to zero polynomially as $(|i|+1)^{- \gamma}$. We study how the critical decay power of the field, $\gamma$, for which the phase transition persists and the decay power $\alpha$ of the Dyson model compare, extending recent results for short-range models on lattices and on trees. We also briefly point out some analogies between these results."
Coupling geometry on binary bipartite networks: hypotheses testing on pattern geometry and nestedness,"  Upon a matrix representation of a binary bipartite network, via the permutation invariance, a coupling geometry is computed to approximate the minimum energy macrostate of a network's system. Such a macrostate is supposed to constitute the intrinsic structures of the system, so that the coupling geometry should be taken as information contents, or even the nonparametric minimum sufficient statistics of the network data. Then pertinent null and alternative hypotheses, such as nestedness, are to be formulated according to the macrostate. That is, any efficient testing statistic needs to be a function of this coupling geometry. These conceptual architectures and mechanisms are by and large still missing in community ecology literature, and rendered misconceptions prevalent in this research area. Here the algorithmically computed coupling geometry is shown consisting of deterministic multiscale block patterns, which are framed by two marginal ultrametric trees on row and column axes, and stochastic uniform randomness within each block found on the finest scale. Functionally a series of increasingly larger ensembles of matrix mimicries is derived by conforming to the multiscale block configurations. Here matrix mimicking is meant to be subject to constraints of row and column sums sequences. Based on such a series of ensembles, a profile of distributions becomes a natural device for checking the validity of testing statistics or structural indexes. An energy based index is used for testing whether network data indeed contains structural geometry. A new version block-based nestedness index is also proposed. Its validity is checked and compared with the existing ones. A computing paradigm, called Data Mechanics, and its application on one real data network are illustrated throughout the developments and discussions in this paper. "
"MUSE-inspired view of the quasar Q2059-360, its Lyman alpha blob, and its neighborhood","The radio-quiet quasar Q2059-360 at redshift $z=3.08$ is known to be close to a small Lyman $\alpha$ blob (LAB) and to be absorbed by a proximate damped Ly$\alpha$ (PDLA) system. Here, we present the Multi Unit Spectroscopic Explorer (MUSE) integral field spectroscopy follow-up of this quasi-stellar object (QSO). Our primary goal is to characterize this LAB in detail by mapping it both spatially and spectrally using the Ly$\alpha$ line, and by looking for high-ionization lines to constrain the emission mechanism. Combining the high sensitivity of the MUSE integral field spectrograph mounted on the Yepun telescope at ESO-VLT with the natural coronagraph provided by the PDLA, we map the LAB down to the QSO position, after robust subtraction of QSO light in the spectral domain. In addition to confirming earlier results for the small bright component of the LAB, we unveil a faint filamentary emission protruding to the south over about 80 pkpc (physical kpc); this results in a total size of about 120 pkpc. We derive the velocity field of the LAB (assuming no transfer effects) and map the Ly$\alpha$ line width. Upper limits are set to the flux of the N V $\lambda 1238-1242$, C IV $\lambda 1548-1551$, He II $\lambda 1640$, and C III] $\lambda 1548-1551$ lines. We have discovered two probable Ly$\alpha$ emitters at the same redshift as the LAB and at projected distances of 265 kpc and 207 kpc from the QSO; their Ly$\alpha$ luminosities might well be enhanced by the QSO radiation. We also find an emission line galaxy at $z=0.33$ near the line of sight to the QSO. This LAB shares the same general characteristics as the 17 others surrounding radio-quiet QSOs presented previously. However, there are indications that it may be centered on the PDLA galaxy rather than on the QSO."
Community Detection on Euclidean Random Graphs,"  We study the problem of community detection (CD) on Euclidean random geometric graphs where each vertex has two latent variables: a binary community label and a $\mathbb{R}^d$ valued location label which forms the support of a Poisson point process of intensity $\lambda$. A random graph is then drawn with edge probabilities dependent on both the community and location labels. In contrast to the stochastic block model (SBM) that has no location labels, the resulting random graph contains many more short loops due to the geometric embedding. We consider the recovery of the community labels, partial and exact, using the random graph and the location labels. We establish phase transitions for both sparse and logarithmic degree regimes, and provide bounds on the location of the thresholds, conjectured to be tight in the case of exact recovery. We also show that the threshold of the distinguishability problem, i.e., the testing between our model and the null model without community labels exhibits no phase-transition and in particular, does not match the weak recovery threshold (in contrast to the SBM). "
Load balancing with heterogeneous schedulers,"Load balancing is a common approach in web server farms or inventory routing problems. An important issue in such systems is to determine the server to which an incoming request should be routed to optimize a given performance criteria. In this paper, we assume the server's scheduling disciplines to be heterogeneous. More precisely, a server implements a scheduling discipline which belongs to the class of limited processor sharing (LPS-$d$) scheduling disciplines. Under LPS-$d$, up to $d$ jobs can be served simultaneously, and hence, includes as special cases First Come First Served ($d=1$) and Processor Sharing ($d=\infty$). In order to obtain efficient heuristics, we model the above load-balancing framework as a multi-armed restless bandit problem. Using the relaxation technique, as first developed in the seminal work of Whittle, we derive Whittle's index policy for general cost functions and obtain a closed-form expression for Whittle's index in terms of the steady-state distribution. Through numerical computations, we investigate the performance of Whittle's index with two different performance criteria: linear cost criterion and a cost criterion that depends on the first and second moment of the throughput. Our results show that \emph{(i)} the structure of Whittle's index policy can strongly depend on the scheduling discipline implemented in the server, i.e., on $d$, and that \emph{(ii)} Whittle's index policy significantly outperforms standard dispatching rules such as Join the Shortest Queue (JSQ), Join the Shortest Expected Workload (JSEW), and Random Server allocation (RSA)."
An Information Theoretic Framework for Active De-anonymization in Social Networks Based on Group Memberships,"  In this paper, a new mathematical formulation for the problem of de-anonymizing social network users by actively querying their membership in social network groups is introduced. In this formulation, the attacker has access to a noisy observation of the group membership of each user in the social network. When an unidentified victim visits a malicious website, the attacker uses browser history sniffing to make queries regarding the victim's social media activity. Particularly, it can make polar queries regarding the victim's group memberships and the victim's identity. The attacker receives noisy responses to her queries. The goal is to de-anonymize the victim with the minimum number of queries. Starting with a rigorous mathematical model for this active de-anonymization problem, an upper bound on the attacker's expected query cost is derived, and new attack algorithms are proposed which achieve this bound. These algorithms vary in computational cost and performance. The results suggest that prior heuristic approaches to this problem provide sub-optimal solutions. "
Geobiodynamics and Roegenian Economic Systems,"  This mathematical essay brings together ideas from Economics, Geobiodynamics and Thermodynamics. Its purpose is to obtain real models of complex evolutionary systems. More specifically, the essay defines Roegenian Economy and links Geobiodynamics and Roegenian Economy. In this context, we discuss the isomorphism between the concepts and techniques of Thermodynamics and Economics. Then we describe a Roegenian economic system like a Carnot group. After we analyse the phase equilibrium for two heterogeneous economic systems. The European Union Economics appears like Cartesian product of Roegenian economic systems and its Balance is analysed in details. A Section at the end describes the ""economic black holes"" as small parts of a a global economic system in which national income is so great that it causes others poor enrichment. These ideas can be used to improve our knowledge and understanding of the nature of development and evolution of thermodynamic-economic systems. "